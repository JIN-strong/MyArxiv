{"2025-01-14T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2501.08207v1","updated":"2025-01-14T15:46:35Z","published":"2025-01-14T15:46:35Z","title":"Efficient Dataframe Systems: Lazy Fat Pandas on a Diet","summary":"  Pandas is widely used for data science applications, but users often run into\nproblems when datasets are larger than memory. There are several frameworks\nbased on lazy evaluation that handle large datasets, but the programs have to\nbe rewritten to suit the framework, and the presence of multiple frameworks\ncomplicates the life of a programmer. In this paper we present a framework that\nallows programmers to code in plain Pandas; with just two lines of code changed\nby the user, our system optimizes the program using a combination of\njust-in-time static analysis, and runtime optimization based on a lazy\ndataframe wrapper framework. Moreover, our system allows the programmer to\nchoose the backend. It works seamlessly with Pandas, Dask, and Modin, allowing\nthe choice of the best-suited backend for an application based on factors such\nas data size. Performance results on a variety of programs show the benefits of\nour framework.\n","authors":["Bhushan Pal Singh","Priyesh Kumar","Chiranmoy Bhattacharya","S. Sudarshan"],"pdf_url":"https://arxiv.org/pdf/2501.08207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14216v2","updated":"2025-01-14T12:43:25Z","published":"2024-08-26T12:19:29Z","title":"Multi-variable Quantification of BDDs in External Memory using Nested\n  Sweeping (Extended Paper)","summary":"  Previous research on the Adiar BDD package has been successful at designing\nalgorithms capable of handling large Binary Decision Diagrams (BDDs) stored in\nexternal memory. To do so, it uses consecutive sweeps through the BDDs to\nresolve computations. Yet, this approach has kept algorithms for multi-variable\nquantification, the relational product, and variable reordering out of its\nscope.\n  In this work, we address this by introducing the nested sweeping framework.\nHere, multiple concurrent sweeps pass information between eachother to compute\nthe result. We have implemented the framework in Adiar and used it to create a\nnew external memory multi-variable quantification algorithm. Compared to\nconventional depth-first implementations, Adiar with nested sweeping is able to\nsolve more instances of our benchmarks and/or solve them faster.\n","authors":["Steffan Christ Sølvsten","Jaco van de Pol"],"pdf_url":"https://arxiv.org/pdf/2408.14216v2.pdf","comment":"27 pages, 14 figures, 2 tables"},{"id":"http://arxiv.org/abs/2501.05262v2","updated":"2025-01-14T02:02:01Z","published":"2025-01-09T14:16:43Z","title":"QMDB: Quick Merkle Database","summary":"  Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.\n","authors":["Isaac Zhang","Ryan Zarick","Daniel Wong","Thomas Kim","Bryan Pellegrino","Mignon Li","Kelvin Wong"],"pdf_url":"https://arxiv.org/pdf/2501.05262v2.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.07771v1","updated":"2025-01-14T01:07:21Z","published":"2025-01-14T01:07:21Z","title":"An Empirical Evaluation of Serverless Cloud Infrastructure for\n  Large-Scale Data Processing","summary":"  Data processing systems are increasingly deployed in the cloud. While\nmonolithic systems run fully on virtual servers, recent systems embrace cloud\ninfrastructure and utilize the disaggregation of compute and storage to scale\nthem independently. The introduction of serverless compute services, such as\nAWS Lambda, enables finer-grained and elastic scalability within these systems.\nPrior work shows the viability of serverless infrastructure for scalable data\nprocessing yet also sees limitations due to variable performance and cost\noverhead, in particular for networking and storage.\n  In this paper, we perform a detailed analysis of the performance and cost\ncharacteristics of serverless infrastructure in the data processing context. We\nbase our analysis on a large series of micro-benchmarks across different\ncompute and storage services, as well as end-to-end workloads. To enable our\nanalysis, we propose the Skyrise serverless evaluation platform. For the widely\nused serverless infrastructure of AWS, our analysis reveals distinct boundaries\nfor performance variability in serverless networks and storage. We further\npresent cost break-even points for serverless compute and storage. These\ninsights provide guidance on when and how serverless infrastructure can be\nefficiently used for data processing.\n","authors":["Thomas Bodner","Theo Radig","David Justen","Daniel Ritter","Tilmann Rabl"],"pdf_url":"https://arxiv.org/pdf/2501.07771v1.pdf","comment":null}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2501.08293v1","updated":"2025-01-14T18:13:36Z","published":"2025-01-14T18:13:36Z","title":"A GPU-Accelerated Distributed Algorithm for Optimal Power Flow in\n  Distribution Systems","summary":"  We propose a GPU-accelerated distributed optimization algorithm for\ncontrolling multi-phase optimal power flow in active distribution systems with\ndynamically changing topologies. To handle varying network configurations and\nenable adaptable decomposition, we advocate a componentwise decomposition\nstrategy. However, this approach can lead to a prolonged computation time\nmainly due to the excessive iterations required for achieving consensus among a\nlarge number of fine-grained components. To overcome this, we introduce a\ntechnique that segregates equality constraints from inequality constraints,\nenabling GPU parallelism to reduce per-iteration time by orders of magnitude,\nthereby significantly accelerating the overall computation. Numerical\nexperiments on IEEE test systems ranging from 13 to 8500 buses demonstrate the\nsuperior scalability of the proposed approach compared to its CPU-based\ncounterparts.\n","authors":["Minseok Ryu","Geunyeong Byeon","Kibaek Kim"],"pdf_url":"https://arxiv.org/pdf/2501.08293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07435v2","updated":"2025-01-14T17:04:40Z","published":"2025-01-13T16:03:51Z","title":"Union: A Trust-minimized Bridge for Rootstock","summary":"  We present Union, a trust-minimized bridge protocol that enables secure\ntransfer of BTC between Bitcoin and a secondary blockchain. The growing\necosystem of blockchain systems built around Bitcoin has created a pressing\nneed for secure and efficient bridges to transfer BTC between networks while\npreserving Bitcoin's security guarantees. Union employs a multi-party variant\nof BitVMX, an optimistic proving system on Bitcoin, to create a bridge that\noperates securely under the assumption that at least one participant remains\nhonest. This 1-of-n honest approach is strikingly different from the\nconventional honest-majority assumption adopted by practically all federated\nsystems. The protocol introduces several innovations: a packet-based\narchitecture that allows security bonds to be reused for multiple bridge\noperations, improving capital efficiency; a system of enablers to manage\nfunctionaries participation and to enforce penalties; a flexible light client\nframework adaptable to various blockchain architectures; and an efficient stop\nwatch mechanism to optimize time-lock management. Union is a practical and\nscalable solution for Bitcoin interoperability that maintains strong security\nguarantees and minimizes trust assumptions.\n","authors":["Ramon Amela","Shreemoy Mishra","Sergio Demian Lerner","Javier Álvarez Cid-Fuentes"],"pdf_url":"https://arxiv.org/pdf/2501.07435v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12874v4","updated":"2025-01-14T16:03:42Z","published":"2023-07-24T15:13:39Z","title":"SoK: Design, Vulnerabilities, and Security Measures of Cryptocurrency\n  Wallets","summary":"  With the advent of decentralised digital currencies powered by blockchain\ntechnology, a new era of peer-to-peer transactions has commenced. The rapid\ngrowth of the cryptocurrency economy has led to increased use of\ntransaction-enabling wallets, making them a focal point for security risks. As\nthe frequency of wallet-related incidents rises, there is a critical need for a\nsystematic approach to measure and evaluate these attacks, drawing lessons from\npast incidents to enhance wallet security. In response, we introduce a\nmulti-dimensional design taxonomy for existing and novel wallets with various\ndesign decisions. We classify existing industry wallets based on this taxonomy,\nidentify previously occurring vulnerabilities and discuss the security\nimplications of design decisions. We also systematise threats to the wallet\nmechanism and analyse the adversary's goals, capabilities and required\nknowledge. We present a multi-layered attack framework and investigate 84\nincidents between 2012 and 2024, accounting for $5.4B. Following this, we\nclassify defence implementations for these attacks on the precautionary and\nremedial axes. We map the mechanism and design decisions to vulnerabilities,\nattacks, and possible defence methods to discuss various insights.\n","authors":["Yimika Erinle","Yathin Kethepalli","Yebo Feng","Jiahua Xu"],"pdf_url":"https://arxiv.org/pdf/2307.12874v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08192v1","updated":"2025-01-14T15:14:10Z","published":"2025-01-14T15:14:10Z","title":"PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving","summary":"  Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.\n","authors":["Ahmet Caner Yüzügüler","Jiawei Zhuang","Lukas Cavigelli"],"pdf_url":"https://arxiv.org/pdf/2501.08192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08090v1","updated":"2025-01-14T12:57:40Z","published":"2025-01-14T12:57:40Z","title":"Hierarchical Autoscaling for Large Language Model Serving with Chiron","summary":"  Large language model (LLM) serving is becoming an increasingly important\nworkload for cloud providers. Based on performance SLO requirements, LLM\ninference requests can be divided into (a) interactive requests that have tight\nSLOs in the order of seconds, and (b) batch requests that have relaxed SLO in\nthe order of minutes to hours. These SLOs can degrade based on the arrival\nrates, multiplexing, and configuration parameters, thus necessitating the use\nof resource autoscaling on serving instances and their batch sizes. However,\nprevious autoscalers for LLM serving do not consider request SLOs leading to\nunnecessary scaling and resource under-utilization. To address these\nlimitations, we introduce Chiron, an autoscaler that uses the idea of\nhierarchical backpressure estimated using queue size, utilization, and SLOs.\nOur experiments show that Chiron achieves up to 90% higher SLO attainment and\nimproves GPU efficiency by up to 70% compared to existing solutions.\n","authors":["Archit Patke","Dhemath Reddy","Saurabh Jha","Chandra Narayanaswami","Zbigniew Kalbarczyk","Ravishankar Iyer"],"pdf_url":"https://arxiv.org/pdf/2501.08090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07958v1","updated":"2025-01-14T09:21:49Z","published":"2025-01-14T09:21:49Z","title":"Technical Report: Exploring Automatic Model-Checking of the Ethereum\n  specification","summary":"  We investigate automated model-checking of the Ethereum specification,\nfocusing on the Accountable Safety property of the 3SF consensus protocol. We\nselect 3SF due to its relevance and the unique challenges it poses for formal\nverification. Our primary tools are TLA+ for specification and the Apalache\nmodel checker for verification.\n  Our formalization builds on the executable Python specification of 3SF. To\nbegin, we manually translate this specification into TLA+, revealing\nsignificant combinatorial complexity in the definition of Accountable Safety.\nTo address these challenges, we introduce several layers of manual abstraction:\n(1) replacing recursion with folds, (2) substituting abstract graphs with\nintegers, and (3) decomposing chain configurations. To cross-validate our\nresults, we develop alternative encodings in SMT (CVC5) and Alloy.\n  Despite the inherent complexity, our results demonstrate that exhaustive\nverification of Accountable Safety is feasible for small instances - supporting\nup to 7 checkpoints and 24 validator votes. Moreover, no violations of\nAccountable Safety are observed, even in slightly larger configurations. Beyond\nthese findings, our study highlights the importance of manual abstraction and\ndomain expertise in enhancing model-checking efficiency and showcases the\nflexibility of TLA+ for managing intricate specifications.\n","authors":["Igor Konnov","Jure Kukovec","Thomas Pani","Roberto Saltini","Thanh Hai Tran"],"pdf_url":"https://arxiv.org/pdf/2501.07958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01917v2","updated":"2025-01-14T05:02:24Z","published":"2024-07-02T03:32:09Z","title":"Securing Distributed Network Digital Twin Systems Against Model\n  Poisoning Attacks","summary":"  In the era of 5G and beyond, the increasing complexity of wireless networks\nnecessitates innovative frameworks for efficient management and deployment.\nDigital twins (DTs), embodying real-time monitoring, predictive configurations,\nand enhanced decision-making capabilities, stand out as a promising solution in\nthis context. Within a time-series data-driven framework that effectively maps\nwireless networks into digital counterparts, encapsulated by integrated\nvertical and horizontal twinning phases, this study investigates the security\nchallenges in distributed network DT systems, which potentially undermine the\nreliability of subsequent network applications such as wireless traffic\nforecasting. Specifically, we consider a minimal-knowledge scenario for all\nattackers, in that they do not have access to network data and other\nspecialized knowledge, yet can interact with previous iterations of\nserver-level models. In this context, we spotlight a novel fake traffic\ninjection attack designed to compromise a distributed network DT system for\nwireless traffic prediction. In response, we then propose a defense mechanism,\ntermed global-local inconsistency detection (GLID), to counteract various model\npoisoning threats. GLID strategically removes abnormal model parameters that\ndeviate beyond a particular percentile range, thereby fortifying the security\nof network twinning process. Through extensive experiments on real-world\nwireless traffic datasets, our experimental evaluations show that both our\nattack and defense strategies significantly outperform existing baselines,\nhighlighting the importance of security measures in the design and\nimplementation of DTs for 5G and beyond network systems.\n","authors":["Zifan Zhang","Minghong Fang","Mingzhe Chen","Gaolei Li","Xi Lin","Yuchen Liu"],"pdf_url":"https://arxiv.org/pdf/2407.01917v2.pdf","comment":"Accepted by Internet of Things Journal (IoT-J). arXiv admin note:\n  substantial text overlap with arXiv:2404.14389"},{"id":"http://arxiv.org/abs/2412.00090v2","updated":"2025-01-14T03:27:10Z","published":"2024-11-27T12:34:45Z","title":"Energy-Efficient Split Learning for Fine-Tuning Large Language Models in\n  Edge Networks","summary":"  In this letter, we propose an energy-efficient split learning (SL) framework\nfor fine-tuning large language models (LLMs) using geo-distributed personal\ndata at the network edge, where LLMs are split and alternately across massive\nmobile devices and an edge server. Considering the device heterogeneity and\nchannel dynamics in edge networks, a \\underline{C}ut l\\underline{A}yer and\ncomputing \\underline{R}esource \\underline{D}ecision (CARD) algorithm is\ndeveloped to minimize training delay and energy consumption. Simulation results\ndemonstrate that the proposed approach reduces the average training delay and\nserver's energy consumption by 70.8% and 53.1%, compared to the benchmarks,\nrespectively.\n","authors":["Zuguang Li","Shaohua Wu","Liang Li","Songge Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.00090v2.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.07767v1","updated":"2025-01-14T00:51:50Z","published":"2025-01-14T00:51:50Z","title":"HgPCN: A Heterogeneous Architecture for E2E Embedded Point Cloud\n  Inference","summary":"  Point cloud is an important type of geometric data structure for many\nembedded applications such as autonomous driving and augmented reality. Current\nPoint Cloud Networks (PCNs) have proven to achieve great success in using\ninference to perform point cloud analysis, including object part segmentation,\nshape classification, and so on. However, point cloud applications on the\ncomputing edge require more than just the inference step. They require an\nend-to-end (E2E) processing of the point cloud workloads: pre-processing of raw\ndata, input preparation, and inference to perform point cloud analysis. Current\nPCN approaches to support end-to-end processing of point cloud workload cannot\nmeet the real-time latency requirement on the edge, i.e., the ability of the AI\nservice to keep up with the speed of raw data generation by 3D sensors. Latency\nfor end-to-end processing of the point cloud workloads stems from two reasons:\nmemory-intensive down-sampling in the pre-processing phase and the data\nstructuring step for input preparation in the inference phase. In this paper,\nwe present HgPCN, an end-to-end heterogeneous architecture for real-time\nembedded point cloud applications. In HgPCN, we introduce two novel\nmethodologies based on spatial indexing to address the two identified\nbottlenecks. In the Pre-processing Engine of HgPCN, an Octree-Indexed-Sampling\nmethod is used to optimize the memory-intensive down-sampling bottleneck of the\npre-processing phase. In the Inference Engine, HgPCN extends a commercial DLA\nwith a customized Data Structuring Unit which is based on a Voxel-Expanded\nGathering method to fundamentally reduce the workload of the data structuring\nstep in the inference phase.\n","authors":["Yiming Gao","Chao Jiang","Wesley Piard","Xiangru Chen","Bhavesh Patel","Herman Lam"],"pdf_url":"https://arxiv.org/pdf/2501.07767v1.pdf","comment":"Accepted by MICRO2024"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2501.08330v1","updated":"2025-01-14T18:59:09Z","published":"2025-01-14T18:59:09Z","title":"Gradient Equilibrium in Online Learning: Theory and Applications","summary":"  We present a new perspective on online learning that we refer to as gradient\nequilibrium: a sequence of iterates achieves gradient equilibrium if the\naverage of gradients of losses along the sequence converges to zero. In\ngeneral, this condition is not implied by nor implies sublinear regret. It\nturns out that gradient equilibrium is achievable by standard online learning\nmethods such as gradient descent and mirror descent with constant step sizes\n(rather than decaying step sizes, as is usually required for no regret).\nFurther, as we show through examples, gradient equilibrium translates into an\ninterpretable and meaningful property in online prediction problems spanning\nregression, classification, quantile estimation, and others. Notably, we show\nthat the gradient equilibrium framework can be used to develop a debiasing\nscheme for black-box predictions under arbitrary distribution shift, based on\nsimple post hoc online descent updates. We also show that post hoc gradient\nupdates can be used to calibrate predicted quantiles under distribution shift,\nand that the framework leads to unbiased Elo scores for pairwise preference\nprediction.\n","authors":["Anastasios N. Angelopoulos","Michael I. Jordan","Ryan J. Tibshirani"],"pdf_url":"https://arxiv.org/pdf/2501.08330v1.pdf","comment":"Code available at\n  https://github.com/aangelopoulos/gradient-equilibrium/"},{"id":"http://arxiv.org/abs/2501.08317v1","updated":"2025-01-14T18:52:27Z","published":"2025-01-14T18:52:27Z","title":"A Similarity Measure Between Functions with Applications to Statistical\n  Learning and Optimization","summary":"  In this note, we present a novel measure of similarity between two functions.\nIt quantifies how the sub-optimality gaps of two functions convert to each\nother, and unifies several existing notions of functional similarity. We show\nthat it has convenient operation rules, and illustrate its use in empirical\nrisk minimization and non-stationary online optimization.\n","authors":["Chengpiao Huang","Kaizheng Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08317v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2501.08316v1","updated":"2025-01-14T18:51:48Z","published":"2025-01-14T18:51:48Z","title":"Diffusion Adversarial Post-Training for One-Step Video Generation","summary":"  The diffusion models are widely used for image and video generation, but\ntheir iterative generation process is slow and expansive. While existing\ndistillation approaches have demonstrated the potential for one-step generation\nin the image domain, they still suffer from significant quality degradation. In\nthis work, we propose Adversarial Post-Training (APT) against real data\nfollowing diffusion pre-training for one-step video generation. To improve the\ntraining stability and quality, we introduce several improvements to the model\narchitecture and training procedures, along with an approximated R1\nregularization objective. Empirically, our experiments show that our\nadversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,\n24fps videos in real time using a single forward evaluation step. Additionally,\nour model is capable of generating 1024px images in a single step, achieving\nquality comparable to state-of-the-art methods.\n","authors":["Shanchuan Lin","Xin Xia","Yuxi Ren","Ceyuan Yang","Xuefeng Xiao","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2501.08316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07169v3","updated":"2025-01-14T18:51:43Z","published":"2024-12-10T04:03:46Z","title":"Rate-In: Information-Driven Adaptive Dropout Rates for Improved\n  Inference-Time Uncertainty Estimation","summary":"  Accurate uncertainty estimation is crucial for deploying neural networks in\nrisk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a\nwidely used technique for approximating predictive uncertainty by performing\nstochastic forward passes with dropout during inference. However, using static\ndropout rates across all layers and inputs can lead to suboptimal uncertainty\nestimates, as it fails to adapt to the varying characteristics of individual\ninputs and network layers. Existing approaches optimize dropout rates during\ntraining using labeled data, resulting in fixed inference-time parameters that\ncannot adjust to new data distributions, compromising uncertainty estimates in\nMonte Carlo simulations.\n  In this paper, we propose Rate-In, an algorithm that dynamically adjusts\ndropout rates during inference by quantifying the information loss induced by\ndropout in each layer's feature maps. By treating dropout as controlled noise\ninjection and leveraging information-theoretic principles, Rate-In adapts\ndropout rates per layer and per input instance without requiring ground truth\nlabels. By quantifying the functional information loss in feature maps, we\nadaptively tune dropout rates to maintain perceptual quality across diverse\nmedical imaging tasks and architectural configurations. Our extensive empirical\nstudy on synthetic data and real-world medical imaging tasks demonstrates that\nRate-In improves calibration and sharpens uncertainty estimates compared to\nfixed or heuristic dropout rates without compromising predictive performance.\nRate-In offers a practical, unsupervised, inference-time approach to optimizing\ndropout for more reliable predictive uncertainty estimation in critical\napplications.\n","authors":["Tal Zeevi","Ravid Shwartz-Ziv","Yann LeCun","Lawrence H. Staib","John A. Onofrey"],"pdf_url":"https://arxiv.org/pdf/2412.07169v3.pdf","comment":"Updated author affiliation"},{"id":"http://arxiv.org/abs/2501.08306v1","updated":"2025-01-14T18:44:35Z","published":"2025-01-14T18:44:35Z","title":"Path Loss Prediction Using Machine Learning with Extended Features","summary":"  Wireless communications rely on path loss modeling, which is most effective\nwhen it includes the physical details of the propagation environment. Acquiring\nthis data has historically been challenging, but geographic information system\ndata is becoming increasingly available with higher resolution and accuracy.\nAccess to such details enables propagation models to more accurately predict\ncoverage and minimize interference in wireless deployments. Machine\nlearning-based modeling can significantly support this effort, with\nfeature-based approaches allowing for accurate, efficient, and scalable\npropagation modeling. Building on previous work, we introduce an extended set\nof features that improves prediction accuracy while, most importantly,\nmaintaining model generalization across a broad range of environments.\n","authors":["Jonathan Ethier","Mathieu Chateauvert","Ryan G. Dempsey","Alexis Bose"],"pdf_url":"https://arxiv.org/pdf/2501.08306v1.pdf","comment":"4 pages, 4 figures, conference paper"},{"id":"http://arxiv.org/abs/2501.08305v1","updated":"2025-01-14T18:41:15Z","published":"2025-01-14T18:41:15Z","title":"Benchmarking Graph Representations and Graph Neural Networks for\n  Multivariate Time Series Classification","summary":"  Multivariate Time Series Classification (MTSC) enables the analysis if\ncomplex temporal data, and thus serves as a cornerstone in various real-world\napplications, ranging from healthcare to finance. Since the relationship among\nvariables in MTS usually contain crucial cues, a large number of graph-based\nMTSC approaches have been proposed, as the graph topology and edges can\nexplicitly represent relationships among variables (channels), where not only\nvarious MTS graph representation learning strategies but also different Graph\nNeural Networks (GNNs) have been explored. Despite such progresses, there is no\ncomprehensive study that fairly benchmarks and investigates the performances of\nexisting widely-used graph representation learning strategies/GNN classifiers\nin the application of different MTSC tasks. In this paper, we present the first\nbenchmark which systematically investigates the effectiveness of the\nwidely-used three node feature definition strategies, four edge feature\nlearning strategies and five GNN architecture, resulting in 60 different\nvariants for graph-based MTSC. These variants are developed and evaluated with\na standardized data pipeline and training/validation/testing strategy on 26\nwidely-used suspensor MTSC datasets. Our experiments highlight that node\nfeatures significantly influence MTSC performance, while the visualization of\nedge features illustrates why adaptive edge learning outperforms other edge\nfeature learning methods. The code of the proposed benchmark is publicly\navailable at\n\\url{https://github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification}.\n","authors":["Wennuo Yang","Shiling Wu","Yuzhi Zhou","Weicheng Xie","Linlin Shen","Siyang Song"],"pdf_url":"https://arxiv.org/pdf/2501.08305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08297v1","updated":"2025-01-14T18:28:08Z","published":"2025-01-14T18:28:08Z","title":"Polynomial Threshold Functions of Bounded Tree-Width: Some\n  Explainability and Complexity Aspects","summary":"  The tree-width of a multivariate polynomial is the tree-width of the\nhypergraph with hyperedges corresponding to its terms. Multivariate polynomials\nof bounded tree-width have been studied by Makowsky and Meer as a new sparsity\ncondition that allows for polynomial solvability of problems which are\nintractable in general. We consider a variation on this theme for Boolean\nvariables. A representation of a Boolean function as the sign of a polynomial\nis called a polynomial threshold representation. We discuss Boolean functions\nrepresentable as polynomial threshold functions of bounded tree-width and\npresent two applications to Bayesian network classifiers, a probabilistic\ngraphical model. Both applications are in Explainable Artificial Intelligence\n(XAI), the research area dealing with the black-box nature of many recent\nmachine learning models. We also give a separation result between the\nrepresentational power of positive and general polynomial threshold functions.\n","authors":["Karine Chubarian","Johnny Joyce","Gyorgy Turan"],"pdf_url":"https://arxiv.org/pdf/2501.08297v1.pdf","comment":"22 pages, 3 figures. To be published in Festschrift in honor of\n  Johann A. Makowsky"},{"id":"http://arxiv.org/abs/2501.08288v1","updated":"2025-01-14T18:08:52Z","published":"2025-01-14T18:08:52Z","title":"Avoiding subtraction and division of stochastic signals using\n  normalizing flows: NFdeconvolve","summary":"  Across the scientific realm, we find ourselves subtracting or dividing\nstochastic signals. For instance, consider a stochastic realization, $x$,\ngenerated from the addition or multiplication of two stochastic signals $a$ and\n$b$, namely $x=a+b$ or $x = ab$. For the $x=a+b$ example, $a$ can be\nfluorescence background and $b$ the signal of interest whose statistics are to\nbe learned from the measured $x$. Similarly, when writing $x=ab$, $a$ can be\nthought of as the illumination intensity and $b$ the density of fluorescent\nmolecules of interest. Yet dividing or subtracting stochastic signals amplifies\nnoise, and we ask instead whether, using the statistics of $a$ and the\nmeasurement of $x$ as input, we can recover the statistics of $b$. Here, we\nshow how normalizing flows can generate an approximation of the probability\ndistribution over $b$, thereby avoiding subtraction or division altogether.\nThis method is implemented in our software package, NFdeconvolve, available on\nGitHub with a tutorial linked in the main text.\n","authors":["Pedro Pessoa","Max Schweiger","Lance W. Q. Xu","Tristan Manha","Ayush Saurabh","Julian Antolin Camarena","Steve Pressé"],"pdf_url":"https://arxiv.org/pdf/2501.08288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08285v1","updated":"2025-01-14T18:00:41Z","published":"2025-01-14T18:00:41Z","title":"Can Bayesian Neural Networks Explicitly Model Input Uncertainty?","summary":"  Inputs to machine learning models can have associated noise or uncertainties,\nbut they are often ignored and not modelled. It is unknown if Bayesian Neural\nNetworks and their approximations are able to consider uncertainty in their\ninputs. In this paper we build a two input Bayesian Neural Network (mean and\nstandard deviation) and evaluate its capabilities for input uncertainty\nestimation across different methods like Ensembles, MC-Dropout, and Flipout.\nOur results indicate that only some uncertainty estimation methods for\napproximate Bayesian NNs can model input uncertainty, in particular Ensembles\nand Flipout.\n","authors":["Matias Valdenegro-Toro","Marco Zullich"],"pdf_url":"https://arxiv.org/pdf/2501.08285v1.pdf","comment":"12 pages, 11 figures, VISAPP 2025 camera ready"},{"id":"http://arxiv.org/abs/2501.08281v1","updated":"2025-01-14T17:57:26Z","published":"2025-01-14T17:57:26Z","title":"Decoding Interpretable Logic Rules from Neural Networks","summary":"  As deep neural networks continue to excel across various domains, their\nblack-box nature has raised concerns about transparency and trust. In\nparticular, interpretability has become increasingly essential for applications\nthat demand high safety and knowledge rigor, such as drug discovery, autonomous\ndriving, and genomics. However, progress in understanding even the simplest\ndeep neural networks - such as fully connected networks - has been limited,\ndespite their role as foundational elements in state-of-the-art models like\nResNet and Transformer. In this paper, we address this challenge by introducing\nNeuroLogic, a novel approach for decoding interpretable logic rules from neural\nnetworks. NeuroLogic leverages neural activation patterns to capture the\nmodel's critical decision-making processes, translating them into logical rules\nrepresented by hidden predicates. Thanks to its flexible design in the\ngrounding phase, NeuroLogic can be adapted to a wide range of neural networks.\nFor simple fully connected neural networks, hidden predicates can be grounded\nin certain split patterns of original input features to derive\ndecision-tree-like rules. For large, complex vision neural networks, NeuroLogic\ngrounds hidden predicates into high-level visual concepts that are\nunderstandable to humans. Our empirical study demonstrates that NeuroLogic can\nextract global and interpretable rules from state-of-the-art models such as\nResNet, a task at which existing work struggles. We believe NeuroLogic can help\npave the way for understanding the black-box nature of neural networks.\n","authors":["Chuqin Geng","Xiaojie Xu","Zhaoyue Wang","Ziyu Zhao","Xujie Si"],"pdf_url":"https://arxiv.org/pdf/2501.08281v1.pdf","comment":"23 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.06993v2","updated":"2025-01-14T17:52:40Z","published":"2024-10-09T15:40:04Z","title":"Efficient Distribution Matching of Representations via Noise-Injected\n  Deep InfoMax","summary":"  Deep InfoMax (DIM) is a well-established method for self-supervised\nrepresentation learning (SSRL) based on maximization of the mutual information\nbetween the input and the output of a deep neural network encoder. Despite the\nDIM and contrastive SSRL in general being well-explored, the task of learning\nrepresentations conforming to a specific distribution (i.e., distribution\nmatching, DM) is still under-addressed. Motivated by the importance of DM to\nseveral downstream tasks (including generative modeling, disentanglement,\noutliers detection and other), we enhance DIM to enable automatic matching of\nlearned representations to a selected prior distribution. To achieve this, we\npropose injecting an independent noise into the normalized outputs of the\nencoder, while keeping the same InfoMax training objective. We show that such\nmodification allows for learning uniformly and normally distributed\nrepresentations, as well as representations of other absolutely continuous\ndistributions. Our approach is tested on various downstream tasks. The results\nindicate a moderate trade-off between the performance on the downstream tasks\nand quality of DM.\n","authors":["Ivan Butakov","Alexander Semenenko","Alexander Tolmachev","Andrey Gladkov","Marina Munkhoeva","Alexey Frolov"],"pdf_url":"https://arxiv.org/pdf/2410.06993v2.pdf","comment":"25 pages, 7 fugures"},{"id":"http://arxiv.org/abs/2501.08266v1","updated":"2025-01-14T17:26:02Z","published":"2025-01-14T17:26:02Z","title":"AI Driven Water Segmentation with deep learning models for Enhanced\n  Flood Monitoring","summary":"  Flooding is a major natural hazard causing significant fatalities and\neconomic losses annually, with increasing frequency due to climate change.\nRapid and accurate flood detection and monitoring are crucial for mitigating\nthese impacts. This study compares the performance of three deep learning\nmodels UNet, ResNet, and DeepLabv3 for pixelwise water segmentation to aid in\nflood detection, utilizing images from drones, in field observations, and\nsocial media. This study involves creating a new dataset that augments\nwellknown benchmark datasets with flood-specific images, enhancing the\nrobustness of the models. The UNet, ResNet, and DeepLab v3 architectures are\ntested to determine their effectiveness in various environmental conditions and\ngeographical locations, and the strengths and limitations of each model are\nalso discussed here, providing insights into their applicability in different\nscenarios by predicting image segmentation masks. This fully automated approach\nallows these models to isolate flooded areas in images, significantly reducing\nprocessing time compared to traditional semi-automated methods. The outcome of\nthis study is to predict segmented masks for each image effected by a flood\ndisaster and the validation accuracy of these models. This methodology\nfacilitates timely and continuous flood monitoring, providing vital data for\nemergency response teams to reduce loss of life and economic damages. It offers\na significant reduction in the time required to generate flood maps, cutting\ndown the manual processing time. Additionally, we present avenues for future\nresearch, including the integration of multimodal data sources and the\ndevelopment of robust deep learning architectures tailored specifically for\nflood detection tasks. Overall, our work contributes to the advancement of\nflood management strategies through innovative use of deep learning\ntechnologies.\n","authors":["Sanjida Afrin Mou","Tasfia Noor Chowdhury","Adib Ibn Mannan","Sadia Nourin Mim","Lubana Tarannum","Tasrin Noman","Jamal Uddin Ahamed"],"pdf_url":"https://arxiv.org/pdf/2501.08266v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.08263v1","updated":"2025-01-14T17:23:14Z","published":"2025-01-14T17:23:14Z","title":"Multiplayer Federated Learning: Reaching Equilibrium with Less\n  Communication","summary":"  Traditional Federated Learning (FL) approaches assume collaborative clients\nwith aligned objectives working towards a shared global model. However, in many\nreal-world scenarios, clients act as rational players with individual\nobjectives and strategic behaviors, a concept that existing FL frameworks are\nnot equipped to adequately address. To bridge this gap, we introduce\nMultiplayer Federated Learning (MpFL), a novel framework that models the\nclients in the FL environment as players in a game-theoretic context, aiming to\nreach an equilibrium. In this scenario, each player tries to optimize their own\nutility function, which may not align with the collective goal. Within MpFL, we\npropose Per-Player Local Stochastic Gradient Descent (PEARL-SGD), an algorithm\nin which each player/client performs local updates independently and\nperiodically communicates with other players. We theoretically analyze\nPEARL-SGD and prove that it reaches a neighborhood of equilibrium with less\ncommunication in the stochastic setup compared to its non-local counterpart.\nFinally, we verify our theoretical findings through numerical experiments.\n","authors":["TaeHo Yoon","Sayantan Choudhury","Nicolas Loizou"],"pdf_url":"https://arxiv.org/pdf/2501.08263v1.pdf","comment":"43 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.02748v3","updated":"2025-01-14T17:20:04Z","published":"2024-10-03T17:57:01Z","title":"CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation","summary":"  Existing automatic prompt engineering methods are typically designed for\ndiscriminative tasks, where new task prompts are iteratively refined with\nlimited feedback from a single metric reflecting a single aspect. However,\nthese approaches are suboptimal for generative tasks, which require more\nnuanced guidance beyond a single numeric metric to improve the prompt and\noptimize multiple aspects of the generated text. To address these challenges,\nwe propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt\nOptimization (CriSPO) approach. CriSPO introduces a critique-suggestion module\nas its core component. This module spontaneously discovers aspects, and\ncompares generated and reference texts across these aspects, providing specific\nsuggestions for prompt modification. These clear critiques and actionable\nsuggestions guide a receptive optimizer module to make more substantial\nchanges, exploring a broader and more effective search space. To further\nimprove CriSPO with multi-metric optimization, we introduce an Automatic Suffix\nTuning (AST) extension to enhance the performance of task prompts across\nmultiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4\nsummarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score\nimprovement on summarization and substantial improvement of various metrics on\nQA. Code available at https://github.com/amazon-science/crispo\n","authors":["Han He","Qianchu Liu","Lei Xu","Chaitanya Shivade","Yi Zhang","Sundararajan Srinivasan","Katrin Kirchhoff"],"pdf_url":"https://arxiv.org/pdf/2410.02748v3.pdf","comment":"Accepted to AAAI-2025"},{"id":"http://arxiv.org/abs/2501.08259v1","updated":"2025-01-14T17:15:27Z","published":"2025-01-14T17:15:27Z","title":"FDPP: Fine-tune Diffusion Policy with Human Preference","summary":"  Imitation learning from human demonstrations enables robots to perform\ncomplex manipulation tasks and has recently witnessed huge success. However,\nthese techniques often struggle to adapt behavior to new preferences or changes\nin the environment. To address these limitations, we propose Fine-tuning\nDiffusion Policy with Human Preference (FDPP). FDPP learns a reward function\nthrough preference-based learning. This reward is then used to fine-tune the\npre-trained policy with reinforcement learning (RL), resulting in alignment of\npre-trained policy with new human preferences while still solving the original\ntask. Our experiments across various robotic tasks and preferences demonstrate\nthat FDPP effectively customizes policy behavior without compromising\nperformance. Additionally, we show that incorporating Kullback-Leibler (KL)\nregularization during fine-tuning prevents over-fitting and helps maintain the\ncompetencies of the initial policy.\n","authors":["Yuxin Chen","Devesh K. Jha","Masayoshi Tomizuka","Diego Romeres"],"pdf_url":"https://arxiv.org/pdf/2501.08259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06457v2","updated":"2025-01-14T16:58:26Z","published":"2025-01-11T07:09:57Z","title":"Automated Detection and Analysis of Minor Deformations in Flat Walls Due\n  to Railway Vibrations Using LiDAR and Machine Learning","summary":"  This study introduces an advanced methodology for automatically identifying\nminor deformations in flat walls caused by vibrations from nearby railway\ntracks. It leverages high-density Terrestrial Laser Scanner (TLS) LiDAR surveys\nand AI/ML techniques to collect and analyze data. The scan data is processed\ninto a detailed point cloud, which is segmented to distinguish ground points,\ntrees, buildings, and other objects. The analysis focuses on identifying\nsections along flat walls and estimating their deformations relative to the\nground orientation.\n  Findings from the study, conducted at the RGIPT campus, reveal significant\ndeformations in walls close to the railway corridor, with the highest\ndeformations ranging from 7 to 8 cm and an average of 3 to 4 cm. In contrast,\nwalls further from the corridor show negligible deformations. The developed\nautomated process for feature extraction and deformation monitoring\ndemonstrates potential for structural health monitoring. By integrating LiDAR\ndata with machine learning, the methodology provides an efficient system for\nidentifying and analyzing structural deformations, highlighting the importance\nof continuous monitoring for ensuring structural integrity and public safety in\nurban infrastructure. This approach represents a substantial advancement in\nautomated feature extraction and deformation analysis, contributing to more\neffective management of urban infrastructure.\n","authors":["Surjo Dey","Ankit Sharma","Hritu Raj","Susham Biswas"],"pdf_url":"https://arxiv.org/pdf/2501.06457v2.pdf","comment":"I am requesting the withdrawal of my paper due to the need for\n  significant revisions to ensure the accuracy and integrity of the presented\n  findings"},{"id":"http://arxiv.org/abs/2410.18803v2","updated":"2025-01-14T16:54:54Z","published":"2024-10-24T14:52:21Z","title":"Language-Agnostic Modeling of Source Reliability on Wikipedia","summary":"  Over the last few years, content verification through reliable sources has\nbecome a fundamental need to combat disinformation. Here, we present a\nlanguage-agnostic model designed to assess the reliability of sources across\nmultiple language editions of Wikipedia. Utilizing editorial activity data, the\nmodel evaluates source reliability within different articles of varying\ncontroversiality such as Climate Change, COVID-19, History, Media, and Biology\ntopics. Crafting features that express domain usage across articles, the model\neffectively predicts source reliability, achieving an F1 Macro score of\napproximately 0.80 for English and other high-resource languages. For\nmid-resource languages, we achieve 0.65 while the performance of low-resource\nlanguages varies; in all cases, the time the domain remains present in the\narticles (which we dub as permanence) is one of the most predictive features.\nWe highlight the challenge of maintaining consistent model performance across\nlanguages of varying resource levels and demonstrate that adapting models from\nhigher-resource languages can improve performance. This work contributes not\nonly to Wikipedia's efforts in ensuring content verifiability but in ensuring\nreliability across diverse user-generated content in various language\ncommunities.\n","authors":["Jacopo D'Ignazi","Andreas Kaltenbrunner","Yelena Mejova","Michele Tizzani","Kyriaki Kalimeri","Mariano Beiró","Pablo Aragón"],"pdf_url":"https://arxiv.org/pdf/2410.18803v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08248v1","updated":"2025-01-14T16:38:33Z","published":"2025-01-14T16:38:33Z","title":"Eliciting In-context Retrieval and Reasoning for Long-context Large\n  Language Models","summary":"  Recent advancements in long-context language models (LCLMs) promise to\ntransform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With\ntheir expanded context windows, LCLMs can process entire knowledge bases and\nperform retrieval and reasoning directly -- a capability we define as\nIn-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like\nLOFT often overestimate LCLM performance by providing overly simplified\ncontexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs\nin more realistic scenarios by including confounding passages retrieved with\nstrong retrievers. We then propose three methods to enhance LCLM performance:\n(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which\nuses attention heads to filter and de-noise long contexts during decoding, and\n(3) joint retrieval head training alongside the generation head. Our evaluation\nof five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with\nour best approach applied to Mistral-7B: +17 and +15 points by Exact Match on\nLOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised\nfine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks\ndespite being a much smaller model.\n","authors":["Yifu Qiu","Varun Embar","Yizhe Zhang","Navdeep Jaitly","Shay B. Cohen","Benjamin Han"],"pdf_url":"https://arxiv.org/pdf/2501.08248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08246v1","updated":"2025-01-14T16:32:01Z","published":"2025-01-14T16:32:01Z","title":"Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful\n  Behaviors with Proximity Constraints","summary":"  Recent work has proposed automated red-teaming methods for testing the\nvulnerabilities of a given target large language model (LLM). These methods use\nred-teaming LLMs to uncover inputs that induce harmful behavior in a target\nLLM. In this paper, we study red-teaming strategies that enable a targeted\nsecurity assessment. We propose an optimization framework for red-teaming with\nproximity constraints, where the discovered prompts must be similar to\nreference prompts from a given dataset. This dataset serves as a template for\nthe discovered prompts, anchoring the search for test-cases to specific topics,\nwriting styles, or types of harmful behavior. We show that established\nauto-regressive model architectures do not perform well in this setting. We\ntherefore introduce a black-box red-teaming method inspired by text-diffusion\nmodels: Diffusion for Auditing and Red-Teaming (DART). DART modifies the\nreference prompt by perturbing it in the embedding space, directly controlling\nthe amount of change introduced. We systematically evaluate our method by\ncomparing its effectiveness with established methods based on model fine-tuning\nand zero- and few-shot prompting. Our results show that DART is significantly\nmore effective at discovering harmful inputs in close proximity to the\nreference prompt.\n","authors":["Jonathan Nöther","Adish Singla","Goran Radanović"],"pdf_url":"https://arxiv.org/pdf/2501.08246v1.pdf","comment":"This is an extended version of a paper published at AAAI 25"},{"id":"http://arxiv.org/abs/2501.08245v1","updated":"2025-01-14T16:31:01Z","published":"2025-01-14T16:31:01Z","title":"Continual Deep Active Learning for Medical Imaging: Replay-Base\n  Architecture for Context Adaptation","summary":"  Deep Learning for medical imaging faces challenges in adapting and\ngeneralizing to new contexts. Additionally, it often lacks sufficient labeled\ndata for specific tasks requiring significant annotation effort. Continual\nLearning (CL) tackles adaptability and generalizability by enabling lifelong\nlearning from a data stream while mitigating forgetting of previously learned\nknowledge. Active Learning (AL) reduces the number of required annotations for\neffective training. This work explores both approaches (CAL) to develop a novel\nframework for robust medical image analysis. Based on the automatic recognition\nof shifts in image characteristics, Replay-Base Architecture for Context\nAdaptation (RBACA) employs a CL rehearsal method to continually learn from\ndiverse contexts, and an AL component to select the most informative instances\nfor annotation. A novel approach to evaluate CAL methods is established using a\ndefined metric denominated IL-Score, which allows for the simultaneous\nassessment of transfer learning, forgetting, and final model performance. We\nshow that RBACA works in domain and class-incremental learning scenarios, by\nassessing its IL-Score on the segmentation and diagnosis of cardiac images. The\nresults show that RBACA outperforms a baseline framework without CAL, and a\nstate-of-the-art CAL method across various memory sizes and annotation budgets.\nOur code is available in https://github.com/RuiDaniel/RBACA .\n","authors":["Rui Daniel","M. Rita Verdelho","Catarina Barata","Carlos Santiago"],"pdf_url":"https://arxiv.org/pdf/2501.08245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08243v1","updated":"2025-01-14T16:30:10Z","published":"2025-01-14T16:30:10Z","title":"Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps","summary":"  Cloud Operations (CloudOps) is a rapidly growing field focused on the\nautomated management and optimization of cloud infrastructure which is\nessential for organizations navigating increasingly complex cloud environments.\nMontyCloud Inc. is one of the major companies in the CloudOps domain that\nleverages autonomous bots to manage cloud compliance, security, and continuous\noperations. To make the platform more accessible and effective to the\ncustomers, we leveraged the use of GenAI.\n  Developing a GenAI-based solution for autonomous CloudOps for the existing\nMontyCloud system presented us with various challenges such as i) diverse data\nsources; ii) orchestration of multiple processes; and iii) handling complex\nworkflows to automate routine tasks. To this end, we developed MOYA, a\nmulti-agent framework that leverages GenAI and balances autonomy with the\nnecessary human control. This framework integrates various internal and\nexternal systems and is optimized for factors like task orchestration,\nsecurity, and error mitigation while producing accurate, reliable, and relevant\ninsights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our\nmulti-agent system with the help of practitioners as well as using automated\nchecks demonstrate enhanced accuracy, responsiveness, and effectiveness over\nnon-agentic approaches across complex workflows.\n","authors":["Kannan Parthasarathy","Karthik Vaidhyanathan","Rudra Dhar","Venkat Krishnamachari","Basil Muhammed","Adyansh Kakran","Sreemaee Akshathala","Shrikara Arun","Sumant Dubey","Mohan Veerubhotla","Amey Karan"],"pdf_url":"https://arxiv.org/pdf/2501.08243v1.pdf","comment":"The paper has been accepted as full paper to CAIN 2025\n  (https://conf.researchr.org/home/cain-2025), co-located with ICSE 2025\n  (https://conf.researchr.org/home/icse-2025). The paper was submitted to CAIN\n  for review on 9 November 2024"},{"id":"http://arxiv.org/abs/2501.08241v1","updated":"2025-01-14T16:28:02Z","published":"2025-01-14T16:28:02Z","title":"A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images\n  using Choquet Integral and Differential Evolution Optimization","summary":"  The COVID-19 pandemic has profoundly impacted billions globally. It\nchallenges public health and healthcare systems due to its rapid spread and\nsevere respiratory effects. An effective strategy to mitigate the COVID-19\npandemic involves integrating testing to identify infected individuals. While\nRT-PCR is considered the gold standard for diagnosing COVID-19, it has some\nlimitations such as the risk of false negatives. To address this problem, this\npaper introduces a novel Deep Learning Diagnosis System that integrates\npre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemble\nlearning framework to achieve precise identification of COVID-19 cases from\nChest X-ray (CXR) images. We combine feature vectors from the final hidden\nlayers of pre-trained DCNNs using the Choquet integral to capture interactions\nbetween different DCNNs that a linear approach cannot. We employed\nSugeno-$\\lambda$ measure theory to derive fuzzy measures for subsets of\nnetworks to enable aggregation. We utilized Differential Evolution to estimate\nfuzzy densities. We developed a TensorFlow-based layer for Choquet operation to\nfacilitate efficient aggregation, due to the intricacies involved in\naggregating feature vectors. Experimental results on the COVIDx dataset show\nthat our ensemble model achieved 98\\% accuracy in three-class classification\nand 99.50\\% in binary classification, outperforming its components-DenseNet-201\n(97\\% for three-class, 98.75\\% for binary), Inception-v3 (96.25\\% for\nthree-class, 98.50\\% for binary), and Xception (94.50\\% for three-class, 98\\%\nfor binary)-and surpassing many previous methods.\n","authors":["Amir Reza Takhsha","Maryam Rastgarpour","Mozhgan Naderi"],"pdf_url":"https://arxiv.org/pdf/2501.08241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08236v1","updated":"2025-01-14T16:21:54Z","published":"2025-01-14T16:21:54Z","title":"Privacy-Preserving Model and Preprocessing Verification for Machine\n  Learning","summary":"  This paper presents a framework for privacy-preserving verification of\nmachine learning models, focusing on models trained on sensitive data.\nIntegrating Local Differential Privacy (LDP) with model explanations from LIME\nand SHAP, our framework enables robust verification without compromising\nindividual privacy. It addresses two key tasks: binary classification, to\nverify if a target model was trained correctly by applying the appropriate\npreprocessing steps, and multi-class classification, to identify specific\npreprocessing errors. Evaluations on three real-world datasets-Diabetes, Adult,\nand Student Record-demonstrate that while the ML-based approach is particularly\neffective in binary tasks, the threshold-based method performs comparably in\nmulti-class tasks. Results indicate that although verification accuracy varies\nacross datasets and noise levels, the framework provides effective detection of\npreprocessing errors, strong privacy guarantees, and practical applicability\nfor safeguarding sensitive data.\n","authors":["Wenbiao Li","Anisa Halimi","Xiaoqian Jiang","Jaideep Vaidya","Erman Ayday"],"pdf_url":"https://arxiv.org/pdf/2501.08236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08234v1","updated":"2025-01-14T16:19:25Z","published":"2025-01-14T16:19:25Z","title":"Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement\n  Learning","summary":"  This paper addresses a critical challenge in the high-speed passenger railway\nindustry: designing effective dynamic pricing strategies in the context of\ncompeting and cooperating operators. To address this, a multi-agent\nreinforcement learning (MARL) framework based on a non-zero-sum Markov game is\nproposed, incorporating random utility models to capture passenger decision\nmaking. Unlike prior studies in areas such as energy, airlines, and mobile\nnetworks, dynamic pricing for railway systems using deep reinforcement learning\nhas received limited attention. A key contribution of this paper is a\nparametrisable and versatile reinforcement learning simulator designed to model\na variety of railway network configurations and demand patterns while enabling\nrealistic, microscopic modelling of user behaviour, called RailPricing-RL. This\nenvironment supports the proposed MARL framework, which models heterogeneous\nagents competing to maximise individual profits while fostering cooperative\nbehaviour to synchronise connecting services. Experimental results validate the\nframework, demonstrating how user preferences affect MARL performance and how\npricing policies influence passenger choices, utility, and overall system\ndynamics. This study provides a foundation for advancing dynamic pricing\nstrategies in railway systems, aligning profitability with system-wide\nefficiency, and supporting future research on optimising pricing policies.\n","authors":["Enrique Adrian Villarrubia-Martin","Luis Rodriguez-Benitez","David Muñoz-Valero","Giovanni Montana","Luis Jimenez-Linares"],"pdf_url":"https://arxiv.org/pdf/2501.08234v1.pdf","comment":"37 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.10729v2","updated":"2025-01-14T16:17:00Z","published":"2024-06-15T20:04:06Z","title":"A Comprehensive Survey of Foundation Models in Medicine","summary":"  Foundation models (FMs) are large-scale deep learning models that are\ndeveloped using large datasets and self-supervised learning methods. These\nmodels serve as a base for different downstream tasks, including healthcare.\nFMs have been adopted with great success across various domains within\nhealthcare. Existing healthcare-based surveys have not yet included all of\nthese domains. Therefore, we provide a detailed survey of FMs in healthcare. We\nfocus on the history, learning strategies, flagship models, applications, and\nchallenges of FMs. We explore how FMs such as the BERT and GPT families are\nreshaping various healthcare domains, including clinical large language models,\nmedical image analysis, and omics. Furthermore, we provide a detailed taxonomy\nof healthcare applications facilitated by FMs, such as clinical NLP, medical\ncomputer vision, graph learning, and other biology-related tasks. Despite the\npromising opportunities FMs provide, they also have several associated\nchallenges, which are explained in detail. We also outline open research issues\nand potential lessons learned to provide researchers and practitioners with\ninsights into the capabilities of FMs in healthcare to advance their deployment\nand mitigate associated risks.\n","authors":["Wasif Khan","Seowung Leem","Kyle B. See","Joshua K. Wong","Shaoting Zhang","Ruogu Fang"],"pdf_url":"https://arxiv.org/pdf/2406.10729v2.pdf","comment":"Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING"},{"id":"http://arxiv.org/abs/2501.08226v1","updated":"2025-01-14T16:10:25Z","published":"2025-01-14T16:10:25Z","title":"Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth\n  Models","summary":"  Glioblastoma, a highly aggressive brain tumor, poses major challenges due to\nits poor prognosis and high morbidity rates. Partial differential\nequation-based models offer promising potential to enhance therapeutic outcomes\nby simulating patient-specific tumor behavior for improved radiotherapy\nplanning. However, model calibration remains a bottleneck due to the high\ncomputational demands of optimization methods like Monte Carlo sampling and\nevolutionary algorithms. To address this, we recently introduced an approach\nleveraging a neural forward solver with gradient-based optimization to\nsignificantly reduce calibration time. This approach requires a highly accurate\nand fully differentiable forward model. We investigate multiple architectures,\nincluding (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a\n3D Vision Transformer (ViT). The optimized TumorSurrogate achieved the best\noverall results, excelling in both tumor outline matching and voxel-level\nprediction of tumor cell concentration. It halved the MSE relative to the\nbaseline model and achieved the highest Dice score across all tumor cell\nconcentration thresholds. Our study demonstrates significant enhancement in\nforward solver performance and outlines important future research directions.\n","authors":["Zeineb Haouari","Jonas Weidner","Ivan Ezhov","Aswathi Varma","Daniel Rueckert","Bjoern Menze","Benedikt Wiestler"],"pdf_url":"https://arxiv.org/pdf/2501.08226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06773v2","updated":"2025-01-14T16:08:28Z","published":"2025-01-12T10:43:05Z","title":"Pareto Set Learning for Multi-Objective Reinforcement Learning","summary":"  Multi-objective decision-making problems have emerged in numerous real-world\nscenarios, such as video games, navigation and robotics. Considering the clear\nadvantages of Reinforcement Learning (RL) in optimizing decision-making\nprocesses, researchers have delved into the development of Multi-Objective RL\n(MORL) methods for solving multi-objective decision problems. However, previous\nmethods either cannot obtain the entire Pareto front, or employ only a single\npolicy network for all the preferences over multiple objectives, which may not\nproduce personalized solutions for each preference. To address these\nlimitations, we propose a novel decomposition-based framework for MORL, Pareto\nSet Learning for MORL (PSL-MORL), that harnesses the generation capability of\nhypernetwork to produce the parameters of the policy network for each\ndecomposition weight, generating relatively distinct policies for various\nscalarized subproblems with high efficiency. PSL-MORL is a general framework,\nwhich is compatible for any RL algorithm. The theoretical result guarantees the\nsuperiority of the model capacity of PSL-MORL and the optimality of the\nobtained policy network. Through extensive experiments on diverse benchmarks,\nwe demonstrate the effectiveness of PSL-MORL in achieving dense coverage of the\nPareto front, significantly outperforming state-of-the-art MORL methods in the\nhypervolume and sparsity indicators.\n","authors":["Erlong Liu","Yu-Chang Wu","Xiaobin Huang","Chengrui Gao","Ren-Jian Wang","Ke Xue","Chao Qian"],"pdf_url":"https://arxiv.org/pdf/2501.06773v2.pdf","comment":"AAAI 2025 Accept"},{"id":"http://arxiv.org/abs/2501.08223v1","updated":"2025-01-14T16:06:54Z","published":"2025-01-14T16:06:54Z","title":"Big Batch Bayesian Active Learning by Considering Predictive\n  Probabilities","summary":"  We observe that BatchBALD, a popular acquisition function for batch Bayesian\nactive learning for classification, can conflate epistemic and aleatoric\nuncertainty, leading to suboptimal performance. Motivated by this observation,\nwe propose to focus on the predictive probabilities, which only exhibit\nepistemic uncertainty. The result is an acquisition function that not only\nperforms better, but is also faster to evaluate, allowing for larger batches\nthan before.\n","authors":["Sebastian W. Ober","Samuel Power","Tom Diethe","Henry B. Moss"],"pdf_url":"https://arxiv.org/pdf/2501.08223v1.pdf","comment":"7 pages, 2 figures; presented as a lightning talk at the NeurIPS\n  Workshop on Bayesian Decision-making and Uncertainty (BDU; 2024)"},{"id":"http://arxiv.org/abs/2501.08219v1","updated":"2025-01-14T16:02:33Z","published":"2025-01-14T16:02:33Z","title":"Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings","summary":"  Large language models (LLMs) have shown significant improvements in many\nnatural language processing (NLP) tasks, accelerating their rapid adoption\nacross many industries. These models are resource-intensive, requiring\nextensive computational resources both during training and inference, leading\nto increased energy consumption and negative environmental impact. As their\nadoption accelerates, the sustainability of LLMs has become a critical issue,\nnecessitating strategies to optimize their runtime efficiency without\ncompromising performance. Hence, it is imperative to identify the parameters\nthat significantly influence the performance and energy efficiency of LLMs. To\nthat end, in this work, we investigate the effect of important parameters on\nthe performance and energy efficiency of LLMs during inference and examine\ntheir trade-offs.\n  First, we analyze how different types of models with varying numbers of\nparameters and architectures perform on tasks like text generation, question\nanswering, and summarization by benchmarking LLMs such as Falcon-7B,\nMistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study\ninput and output sequence characteristics such as sequence length concerning\nenergy consumption, performance, and throughput. Finally, we explore the impact\nof hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency\nScaling (DVFS), on the models' latency and energy efficiency. Our extensive\nbenchmarking and statistical analysis reveal many interesting findings,\nuncovering how specific optimizations can reduce energy consumption while\nmaintaining throughput and accuracy. This study provides actionable insights\nfor researchers and practitioners to design energy-efficient LLM inference\nsystems.\n","authors":["Paul Joe Maliakel","Shashikant Ilager","Ivona Brandic"],"pdf_url":"https://arxiv.org/pdf/2501.08219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08205v1","updated":"2025-01-14T15:45:27Z","published":"2025-01-14T15:45:27Z","title":"Modeling Feature Maps for Quantum Machine Learning","summary":"  Quantum Machine Learning (QML) offers significant potential for complex tasks\nlike genome sequence classification, but quantum noise on Noisy\nIntermediate-Scale Quantum (NISQ) devices poses practical challenges. This\nstudy systematically evaluates how various quantum noise models including\ndephasing, amplitude damping, depolarizing, thermal noise, bit-flip, and\nphase-flip affect key QML algorithms (QSVC, Peg-QSVC, QNN, VQC) and feature\nmapping techniques (ZFeatureMap, ZZFeatureMap, and PauliFeatureMap). Results\nindicate that QSVC is notably robust under noise, whereas Peg-QSVC and QNN are\nmore sensitive, particularly to depolarizing and amplitude-damping noise. The\nPauliFeatureMap is especially vulnerable, highlighting difficulties in\nmaintaining accurate classification under noisy conditions. These findings\nunderscore the critical importance of feature map selection and noise\nmitigation strategies in optimizing QML for genomic classification, with\npromising implications for personalized medicine.\n","authors":["Navneet Singh","Shiva Raj Pokhrel"],"pdf_url":"https://arxiv.org/pdf/2501.08205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08202v1","updated":"2025-01-14T15:37:03Z","published":"2025-01-14T15:37:03Z","title":"Data-driven system identification using quadratic embeddings of\n  nonlinear dynamics","summary":"  We propose a novel data-driven method called QENDy (Quadratic Embedding of\nNonlinear Dynamics) that not only allows us to learn quadratic representations\nof highly nonlinear dynamical systems, but also to identify the governing\nequations. The approach is based on an embedding of the system into a\nhigher-dimensional feature space in which the dynamics become quadratic. Just\nlike SINDy (Sparse Identification of Nonlinear Dynamics), our method requires\ntrajectory data, time derivatives for the training data points, which can also\nbe estimated using finite difference approximations, and a set of preselected\nbasis functions, called dictionary. We illustrate the efficacy and accuracy of\nQENDy with the aid of various benchmark problems and compare its performance\nwith SINDy and a deep learning method for identifying quadratic embeddings.\nFurthermore, we analyze the convergence of QENDy and SINDy in the infinite data\nlimit, highlight their similarities and main differences, and compare the\nquadratic embedding with linearization techniques based on the Koopman\noperator.\n","authors":["Stefan Klus","Joel-Pascal N'Konzi"],"pdf_url":"https://arxiv.org/pdf/2501.08202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08201v1","updated":"2025-01-14T15:36:32Z","published":"2025-01-14T15:36:32Z","title":"Globally Convergent Variational Inference","summary":"  In variational inference (VI), an approximation of the posterior distribution\nis selected from a family of distributions through numerical optimization. With\nthe most common variational objective function, known as the evidence lower\nbound (ELBO), only convergence to a local optimum can be guaranteed. In this\nwork, we instead establish the global convergence of a particular VI method.\nThis VI method, which may be considered an instance of neural posterior\nestimation (NPE), minimizes an expectation of the inclusive (forward) KL\ndivergence to fit a variational distribution that is parameterized by a neural\nnetwork. Our convergence result relies on the neural tangent kernel (NTK) to\ncharacterize the gradient dynamics that arise from considering the variational\nobjective in function space. In the asymptotic regime of a fixed,\npositive-definite neural tangent kernel, we establish conditions under which\nthe variational objective admits a unique solution in a reproducing kernel\nHilbert space (RKHS). Then, we show that the gradient descent dynamics in\nfunction space converge to this unique function. In ablation studies and\npractical problems, we demonstrate that our results explain the behavior of NPE\nin non-asymptotic finite-neuron settings, and show that NPE outperforms\nELBO-based optimization, which often converges to shallow local optima.\n","authors":["Declan McNamara","Jackson Loper","Jeffrey Regier"],"pdf_url":"https://arxiv.org/pdf/2501.08201v1.pdf","comment":"Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2501.08200v1","updated":"2025-01-14T15:27:01Z","published":"2025-01-14T15:27:01Z","title":"CWEval: Outcome-driven Evaluation on Functionality and Security of LLM\n  Code Generation","summary":"  Large Language Models (LLMs) have significantly aided developers by\ngenerating or assisting in code writing, enhancing productivity across various\ntasks. While identifying incorrect code is often straightforward, detecting\nvulnerabilities in functionally correct code is more challenging, especially\nfor developers with limited security knowledge, which poses considerable\nsecurity risks of using LLM-generated code and underscores the need for robust\nevaluation benchmarks that assess both functional correctness and security.\nCurrent benchmarks like CyberSecEval and SecurityEval attempt to solve it but\nare hindered by unclear and impractical specifications, failing to assess both\nfunctionality and security accurately. To tackle these deficiencies, we\nintroduce CWEval, a novel outcome-driven evaluation framework designed to\nenhance the evaluation of secure code generation by LLMs. This framework not\nonly assesses code functionality but also its security simultaneously with\nhigh-quality task specifications and outcome-driven test oracles which provides\nhigh accuracy. Coupled with CWEval-bench, a multilingual, security-critical\ncoding benchmark, CWEval provides a rigorous empirical security evaluation on\nLLM-generated code, overcoming previous benchmarks' shortcomings. Through our\nevaluations, CWEval reveals a notable portion of functional but insecure code\nproduced by LLMs, and shows a serious inaccuracy of previous evaluations,\nultimately contributing significantly to the field of secure code generation.\nWe open-source our artifact at: https://github.com/Co1lin/CWEval .\n","authors":["Jinjun Peng","Leyi Cui","Kele Huang","Junfeng Yang","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2501.08200v1.pdf","comment":"to be published in LLM4Code 2025"},{"id":"http://arxiv.org/abs/2501.08195v1","updated":"2025-01-14T15:18:28Z","published":"2025-01-14T15:18:28Z","title":"Self-supervised Deep Hyperspectral Inpainting with the Plug and Play and\n  Deep Image Prior Models","summary":"  Hyperspectral images are typically composed of hundreds of narrow and\ncontiguous spectral bands, each containing information regarding the material\ncomposition of the imaged scene. However, these images can be affected by\nvarious sources of noise, distortions, or data loss, which can significantly\ndegrade their quality and usefulness. This paper introduces a convergent\nguaranteed algorithm, LRS-PnP-DIP(1-Lip), which successfully addresses the\ninstability issue of DHP that has been reported before. The proposed algorithm\nextends the successful joint low-rank and sparse model to further exploit the\nunderlying data structures beyond the conventional and sometimes restrictive\nunions of subspace models. A stability analysis guarantees the convergence of\nthe proposed algorithm under mild assumptions , which is crucial for its\napplication in real-world scenarios. Extensive experiments demonstrate that the\nproposed solution consistently delivers visually and quantitatively superior\ninpainting results, establishing state-of-the-art performance.\n","authors":["Shuo Li","Mehrdad Yaghoobi"],"pdf_url":"https://arxiv.org/pdf/2501.08195v1.pdf","comment":"31 pages, 9 Figures, 7 Tables. arXiv admin note: text overlap with\n  arXiv:2306.08128"},{"id":"http://arxiv.org/abs/2501.08193v1","updated":"2025-01-14T15:14:26Z","published":"2025-01-14T15:14:26Z","title":"Modeling Quantum Machine Learning for Genomic Data Analysis","summary":"  Quantum Machine Learning (QML) continues to evolve, unlocking new\nopportunities for diverse applications. In this study, we investigate and\nevaluate the applicability of QML models for binary classification of genome\nsequence data by employing various feature mapping techniques. We present an\nopen-source, independent Qiskit-based implementation to conduct experiments on\na benchmark genomic dataset. Our simulations reveal that the interplay between\nfeature mapping techniques and QML algorithms significantly influences\nperformance. Notably, the Pegasos Quantum Support Vector Classifier\n(Pegasos-QSVC) exhibits high sensitivity, particularly excelling in recall\nmetrics, while Quantum Neural Networks (QNN) achieve the highest training\naccuracy across all feature maps. However, the pronounced variability in\nclassifier performance, dependent on feature mapping, highlights the risk of\noverfitting to localized output distributions in certain scenarios. This work\nunderscores the transformative potential of QML for genomic data classification\nwhile emphasizing the need for continued advancements to enhance the robustness\nand accuracy of these methodologies.\n","authors":["Navneet Singh","Shiva Raj Pokhrel"],"pdf_url":"https://arxiv.org/pdf/2501.08193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08188v1","updated":"2025-01-14T15:13:00Z","published":"2025-01-14T15:13:00Z","title":"A Critical Synthesis of Uncertainty Quantification and Foundation Models\n  in Monocular Depth Estimation","summary":"  While recent foundation models have enabled significant breakthroughs in\nmonocular depth estimation, a clear path towards safe and reliable deployment\nin the real-world remains elusive. Metric depth estimation, which involves\npredicting absolute distances, poses particular challenges, as even the most\nadvanced foundation models remain prone to critical errors. Since quantifying\nthe uncertainty has emerged as a promising endeavor to address these\nlimitations and enable trustworthy deployment, we fuse five different\nuncertainty quantification methods with the current state-of-the-art\nDepthAnythingV2 foundation model. To cover a wide range of metric depth\ndomains, we evaluate their performance on four diverse datasets. Our findings\nidentify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a\nparticularly promising approach, offering reliable uncertainty estimates while\nmaintaining predictive performance and computational efficiency on par with the\nbaseline, encompassing both training and inference time. By fusing uncertainty\nquantification and foundation models within the context of monocular depth\nestimation, this paper lays a critical foundation for future research aimed at\nimproving not only model performance but also its explainability. Extending\nthis critical synthesis of uncertainty quantification and foundation models\ninto other crucial tasks, such as semantic segmentation and pose estimation,\npresents exciting opportunities for safer and more reliable machine vision\nsystems.\n","authors":["Steven Landgraf","Rongjun Qin","Markus Ulrich"],"pdf_url":"https://arxiv.org/pdf/2501.08188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08187v1","updated":"2025-01-14T15:12:19Z","published":"2025-01-14T15:12:19Z","title":"A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction\n  Following","summary":"  Large language models excel at interpreting complex natural language\ninstructions, enabling them to perform a wide range of tasks. In the life\nsciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language\nof cellular biology\", capturing intricate gene expression patterns at the\nsingle-cell level. However, interacting with this \"language\" through\nconventional tools is often inefficient and unintuitive, posing challenges for\nresearchers. To address these limitations, we present InstructCell, a\nmulti-modal AI copilot that leverages natural language as a medium for more\ndirect and flexible single-cell analysis. We construct a comprehensive\nmulti-modal instruction dataset that pairs text-based instructions with\nscRNA-seq profiles from diverse tissues and species. Building on this, we\ndevelop a multi-modal cell language architecture capable of simultaneously\ninterpreting and processing both modalities. InstructCell empowers researchers\nto accomplish critical tasks-such as cell type annotation, conditional\npseudo-cell generation, and drug sensitivity prediction-using straightforward\nnatural language commands. Extensive evaluations demonstrate that InstructCell\nconsistently meets or exceeds the performance of existing single-cell\nfoundation models, while adapting to diverse experimental conditions. More\nimportantly, InstructCell provides an accessible and intuitive tool for\nexploring complex single-cell data, lowering technical barriers and enabling\ndeeper biological insights.\n","authors":["Yin Fang","Xinle Deng","Kangwei Liu","Ningyu Zhang","Jingyang Qian","Penghui Yang","Xiaohui Fan","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2501.08187v1.pdf","comment":"37 pages; 13 figures; Code: https://github.com/zjunlp/Instructcell;\n  Models: https://huggingface.co/zjunlp/Instructcell-chat,\n  https://huggingface.co/zjunlp/InstructCell-instruct"},{"id":"http://arxiv.org/abs/2501.08180v1","updated":"2025-01-14T15:03:53Z","published":"2025-01-14T15:03:53Z","title":"D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models","summary":"  Diffusion models have achieved cutting-edge performance in image generation.\nHowever, their lengthy denoising process and computationally intensive score\nestimation network impede their scalability in low-latency and\nresource-constrained scenarios. Post-training quantization (PTQ) compresses and\naccelerates diffusion models without retraining, but it inevitably introduces\nadditional quantization noise, resulting in mean and variance deviations. In\nthis work, we propose D2-DPM, a dual denoising mechanism aimed at precisely\nmitigating the adverse effects of quantization noise on the noise estimation\nnetwork. Specifically, we first unravel the impact of quantization noise on the\nsampling equation into two components: the mean deviation and the variance\ndeviation. The mean deviation alters the drift coefficient of the sampling\nequation, influencing the trajectory trend, while the variance deviation\nmagnifies the diffusion coefficient, impacting the convergence of the sampling\ntrajectory. The proposed D2-DPM is thus devised to denoise the quantization\nnoise at each time step, and then denoise the noisy sample through the inverse\ndiffusion iterations. Experimental results demonstrate that D2-DPM achieves\nsuperior generation quality, yielding a 1.42 lower FID than the full-precision\nmodel while achieving 3.99x compression and 11.67x bit-operation acceleration.\n","authors":["Qian Zeng","Jie Song","Han Zheng","Hao Jiang","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2501.08180v1.pdf","comment":"9 pages, 4 figures, acceptted by AAAI2025"},{"id":"http://arxiv.org/abs/2411.19835v2","updated":"2025-01-14T14:53:10Z","published":"2024-11-29T16:45:25Z","title":"Feedback-driven object detection and iterative model improvement","summary":"  Automated object detection has become increasingly valuable across diverse\napplications, yet efficient, high-quality annotation remains a persistent\nchallenge. In this paper, we present the development and evaluation of a\nplatform designed to interactively improve object detection models. The\nplatform allows uploading and annotating images as well as fine-tuning object\ndetection models. Users can then manually review and refine annotations,\nfurther creating improved snapshots that are used for automatic object\ndetection on subsequent image uploads - a process we refer to as semi-automatic\nannotation resulting in a significant gain in annotation efficiency.\n  Whereas iterative refinement of model results to speed up annotation has\nbecome common practice, we are the first to quantitatively evaluate its\nbenefits with respect to time, effort, and interaction savings. Our\nexperimental results show clear evidence for a significant time reduction of up\nto 53% for semi-automatic compared to manual annotation. Importantly, these\nefficiency gains did not compromise annotation quality, while matching or\noccasionally even exceeding the accuracy of manual annotations. These findings\ndemonstrate the potential of our lightweight annotation platform for creating\nhigh-quality object detection datasets and provide best practices to guide\nfuture development of annotation platforms.\n  The platform is open-source, with the frontend and backend repositories\navailable on GitHub (https://github.com/ml-lab-htw/iterative-annotate). To\nsupport the understanding of our labeling process, we have created an\nexplanatory video demonstrating the methodology using microscopy images of E.\ncoli bacteria as an example. The video is available on YouTube\n(https://www.youtube.com/watch?v=CM9uhE8NN5E).\n","authors":["Sönke Tenckhoff","Mario Koddenbrock","Erik Rodner"],"pdf_url":"https://arxiv.org/pdf/2411.19835v2.pdf","comment":"AI4EA24"},{"id":"http://arxiv.org/abs/2501.08169v1","updated":"2025-01-14T14:49:49Z","published":"2025-01-14T14:49:49Z","title":"Revolutionizing Communication with Deep Learning and XAI for Enhanced\n  Arabic Sign Language Recognition","summary":"  This study introduces an integrated approach to recognizing Arabic Sign\nLanguage (ArSL) using state-of-the-art deep learning models such as\nMobileNetV3, ResNet50, and EfficientNet-B2. These models are further enhanced\nby explainable AI (XAI) techniques to boost interpretability. The ArSL2018 and\nRGB Arabic Alphabets Sign Language (AASL) datasets are employed, with\nEfficientNet-B2 achieving peak accuracies of 99.48\\% and 98.99\\%, respectively.\nKey innovations include sophisticated data augmentation methods to mitigate\nclass imbalance, implementation of stratified 5-fold cross-validation for\nbetter generalization, and the use of Grad-CAM for clear model decision\ntransparency. The proposed system not only sets new benchmarks in recognition\naccuracy but also emphasizes interpretability, making it suitable for\napplications in healthcare, education, and inclusive communication\ntechnologies.\n","authors":["Mazen Balat","Rewaa Awaad","Ahmed B. Zaky","Salah A. Aly"],"pdf_url":"https://arxiv.org/pdf/2501.08169v1.pdf","comment":"13 pages, 25 figures, 16 tables"},{"id":"http://arxiv.org/abs/2412.13174v2","updated":"2025-01-14T14:48:32Z","published":"2024-12-17T18:53:43Z","title":"ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark\n  Detection","summary":"  Although facial landmark detection (FLD) has gained significant progress,\nexisting FLD methods still suffer from performance drops on partially\nnon-visible faces, such as faces with occlusions or under extreme lighting\nconditions or poses. To address this issue, we introduce ORFormer, a novel\ntransformer-based method that can detect non-visible regions and recover their\nmissing features from visible parts. Specifically, ORFormer associates each\nimage patch token with one additional learnable token called the messenger\ntoken. The messenger token aggregates features from all but its patch. This\nway, the consensus between a patch and other patches can be assessed by\nreferring to the similarity between its regular and messenger embeddings,\nenabling non-visible region identification. Our method then recovers occluded\npatches with features aggregated by the messenger tokens. Leveraging the\nrecovered features, ORFormer compiles high-quality heatmaps for the downstream\nFLD task. Extensive experiments show that our method generates heatmaps\nresilient to partial occlusions. By integrating the resultant heatmaps into\nexisting FLD methods, our method performs favorably against the state of the\narts on challenging datasets such as WFLW and COFW.\n","authors":["Jui-Che Chiang","Hou-Ning Hu","Bo-Syuan Hou","Chia-Yu Tseng","Yu-Lun Liu","Min-Hung Chen","Yen-Yu Lin"],"pdf_url":"https://arxiv.org/pdf/2412.13174v2.pdf","comment":"WACV 2025 Project Link: https://ben0919.github.io/ORFormer/"},{"id":"http://arxiv.org/abs/2501.08156v1","updated":"2025-01-14T14:31:45Z","published":"2025-01-14T14:31:45Z","title":"Inference-Time-Compute: More Faithful? A Research Note","summary":"  Models trained specifically to generate long Chains of Thought (CoTs) have\nrecently achieved impressive results. We refer to these models as\nInference-Time-Compute (ITC) models. Are the CoTs of ITC models more faithful\ncompared to traditional non-ITC models? We evaluate two ITC models (based on\nQwen-2.5 and Gemini-2) on an existing test of faithful CoT To measure\nfaithfulness, we test if models articulate cues in their prompt that influence\ntheir answers to MMLU questions. For example, when the cue \"A Stanford\nProfessor thinks the answer is D'\" is added to the prompt, models sometimes\nswitch their answer to D. In such cases, the Gemini ITC model articulates the\ncue 54% of the time, compared to 14% for the non-ITC Gemini.\n  We evaluate 7 types of cue, such as misleading few-shot examples and\nanchoring on past responses. ITC models articulate cues that influence them\nmuch more reliably than all the 6 non-ITC models tested, such as\nClaude-3.5-Sonnet and GPT-4o, which often articulate close to 0% of the time.\n  However, our study has important limitations. We evaluate only two ITC models\n-- we cannot evaluate OpenAI's SOTA o1 model. We also lack details about the\ntraining of these ITC models, making it hard to attribute our findings to\nspecific processes.\n  We think faithfulness of CoT is an important property for AI Safety. The ITC\nmodels we tested show a large improvement in faithfulness, which is worth\ninvestigating further. To speed up this investigation, we release these early\nresults as a research note.\n","authors":["James Chua","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2501.08156v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.08155v1","updated":"2025-01-14T14:29:36Z","published":"2025-01-14T14:29:36Z","title":"FairTTTS: A Tree Test Time Simulation Method for Fairness-Aware\n  Classification","summary":"  Algorithmic decision-making has become deeply ingrained in many domains, yet\nbiases in machine learning models can still produce discriminatory outcomes,\noften harming unprivileged groups. Achieving fair classification is inherently\nchallenging, requiring a careful balance between predictive performance and\nethical considerations. We present FairTTTS, a novel post-processing bias\nmitigation method inspired by the Tree Test Time Simulation (TTTS) method.\nOriginally developed to enhance accuracy and robustness against adversarial\ninputs through probabilistic decision-path adjustments, TTTS serves as the\nfoundation for FairTTTS. By building on this accuracy-enhancing technique,\nFairTTTS mitigates bias and improves predictive performance. FairTTTS uses a\ndistance-based heuristic to adjust decisions at protected attribute nodes,\nensuring fairness for unprivileged samples. This fairness-oriented adjustment\noccurs as a post-processing step, allowing FairTTTS to be applied to\npre-trained models, diverse datasets, and various fairness metrics without\nretraining. Extensive evaluation on seven benchmark datasets shows that\nFairTTTS outperforms traditional methods in fairness improvement, achieving a\n20.96% average increase over the baseline compared to 18.78% for related work,\nand further enhances accuracy by 0.55%. In contrast, competing methods\ntypically reduce accuracy by 0.42%. These results confirm that FairTTTS\neffectively promotes more equitable decision-making while simultaneously\nimproving predictive performance.\n","authors":["Nurit Cohen-Inger","Lior Rokach","Bracha Shapira","Seffi Cohen"],"pdf_url":"https://arxiv.org/pdf/2501.08155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08149v1","updated":"2025-01-14T14:25:10Z","published":"2025-01-14T14:25:10Z","title":"Multiple-Input Variational Auto-Encoder for Anomaly Detection in\n  Heterogeneous Data","summary":"  Anomaly detection (AD) plays a pivotal role in AI applications, e.g., in\nclassification, and intrusion/threat detection in cybersecurity. However, most\nexisting methods face challenges of heterogeneity amongst feature subsets posed\nby non-independent and identically distributed (non-IID) data. We propose a\nnovel neural network model called Multiple-Input Auto-Encoder for AD (MIAEAD)\nto address this. MIAEAD assigns an anomaly score to each feature subset of a\ndata sample to indicate its likelihood of being an anomaly. This is done by\nusing the reconstruction error of its sub-encoder as the anomaly score. All\nsub-encoders are then simultaneously trained using unsupervised learning to\ndetermine the anomaly scores of feature subsets. The final AUC of MIAEAD is\ncalculated for each sub-dataset, and the maximum AUC obtained among the\nsub-datasets is selected. To leverage the modelling of the distribution of\nnormal data to identify anomalies of the generative models, we develop a novel\nneural network architecture/model called Multiple-Input Variational\nAuto-Encoder (MIVAE). MIVAE can process feature subsets through its\nsub-encoders before learning distribution of normal data in the latent space.\nThis allows MIVAE to identify anomalies that deviate from the learned\ndistribution. We theoretically prove that the difference in the average anomaly\nscore between normal samples and anomalies obtained by the proposed MIVAE is\ngreater than that of the Variational Auto-Encoder (VAEAD), resulting in a\nhigher AUC for MIVAE. Extensive experiments on eight real-world anomaly\ndatasets demonstrate the superior performance of MIAEAD and MIVAE over\nconventional methods and the state-of-the-art unsupervised models, by up to 6%\nin terms of AUC score. Alternatively, MIAEAD and MIVAE have a high AUC when\napplied to feature subsets with low heterogeneity based on the coefficient of\nvariation (CV) score.\n","authors":["Phai Vu Dinh","Diep N. Nguyen","Dinh Thai Hoang","Quang Uy Nguyen","Eryk Dutkiewicz"],"pdf_url":"https://arxiv.org/pdf/2501.08149v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2210.09655v2","updated":"2025-01-14T14:22:05Z","published":"2022-10-18T07:48:59Z","title":"WINE: Wavelet-Guided GAN Inversion and Editing for High-Fidelity\n  Refinement","summary":"  Recent advanced GAN inversion models aim to convey high-fidelity information\nfrom original images to generators through methods using generator tuning or\nhigh-dimensional feature learning. Despite these efforts, accurately\nreconstructing image-specific details remains as a challenge due to the\ninherent limitations both in terms of training and structural aspects, leading\nto a bias towards low-frequency information. In this paper, we look into the\nwidely used pixel loss in GAN inversion, revealing its predominant focus on the\nreconstruction of low-frequency features. We then propose WINE, a\nWavelet-guided GAN Inversion aNd Editing model, which transfers the\nhigh-frequency information through wavelet coefficients via newly proposed\nwavelet loss and wavelet fusion scheme. Notably, WINE is the first attempt to\ninterpret GAN inversion in the frequency domain. Our experimental results\nshowcase the precision of WINE in preserving high-frequency details and\nenhancing image quality. Even in editing scenarios, WINE outperforms existing\nstate-of-the-art GAN inversion models with a fine balance between editability\nand reconstruction quality.\n","authors":["Chaewon Kim","Seung-Jun Moon","Gyeong-Moon Park"],"pdf_url":"https://arxiv.org/pdf/2210.09655v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08142v1","updated":"2025-01-14T14:21:48Z","published":"2025-01-14T14:21:48Z","title":"Bootstrapping Corner Cases: High-Resolution Inpainting for Safety\n  Critical Detect and Avoid for Automated Flying","summary":"  Modern machine learning techniques have shown tremendous potential,\nespecially for object detection on camera images. For this reason, they are\nalso used to enable safety-critical automated processes such as autonomous\ndrone flights. We present a study on object detection for Detect and Avoid, a\nsafety critical function for drones that detects air traffic during automated\nflights for safety reasons. An ill-posed problem is the generation of good and\nespecially large data sets, since detection itself is the corner case. Most\nmodels suffer from limited ground truth in raw data, \\eg recorded air traffic\nor frontal flight with a small aircraft. It often leads to poor and critical\ndetection rates. We overcome this problem by using inpainting methods to\nbootstrap the dataset such that it explicitly contains the corner cases of the\nraw data. We provide an overview of inpainting methods and generative models\nand present an example pipeline given a small annotated dataset. We validate\nour method by generating a high-resolution dataset, which we make publicly\navailable and present it to an independent object detector that was fully\ntrained on real data.\n","authors":["Jonathan Lyhs","Lars Hinneburg","Michael Fischer","Florian Ölsner","Stefan Milz","Jeremy Tschirner","Patrick Mäder"],"pdf_url":"https://arxiv.org/pdf/2501.08142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08139v1","updated":"2025-01-14T14:19:40Z","published":"2025-01-14T14:19:40Z","title":"EEG-ReMinD: Enhancing Neurodegenerative EEG Decoding through\n  Self-Supervised State Reconstruction-Primed Riemannian Dynamics","summary":"  The development of EEG decoding algorithms confronts challenges such as data\nsparsity, subject variability, and the need for precise annotations, all of\nwhich are vital for advancing brain-computer interfaces and enhancing the\ndiagnosis of diseases. To address these issues, we propose a novel two-stage\napproach named Self-Supervised State Reconstruction-Primed Riemannian Dynamics\n(EEG-ReMinD) , which mitigates reliance on supervised learning and integrates\ninherent geometric features. This approach efficiently handles EEG data\ncorruptions and reduces the dependency on labels. EEG-ReMinD utilizes\nself-supervised and geometric learning techniques, along with an attention\nmechanism, to analyze the temporal dynamics of EEG features within the\nframework of Riemannian geometry, referred to as Riemannian dynamics.\nComparative analyses on both intact and corrupted datasets from two different\nneurodegenerative disorders underscore the enhanced performance of EEG-ReMinD.\n","authors":["Zirui Wang","Zhenxi Song","Yi Guo","Yuxin Liu","Guoyang Xu","Min Zhang","Zhiguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.08139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08134v1","updated":"2025-01-14T14:14:22Z","published":"2025-01-14T14:14:22Z","title":"An Empirical Wall-Pressure Spectrum Model for Aeroacoustic Predictions\n  Based on Symbolic Regression","summary":"  Fast-turn around methods to predict airfoil trailing-edge noise are crucial\nfor incorporating noise limitations into design optimization loops of several\napplications. Among these aeroacoustic predictive models, Amiet's theory offers\nthe best balance between accuracy and simplicity. The accuracy of the model\nrelies heavily on precise wall-pressure spectrum predictions, which are often\nbased on single-equation formulations with adjustable parameters. These\nparameters are calibrated for particular airfoils and flow conditions and\nconsequently tend to fail when applied outside their calibration range. This\npaper introduces a new wall-pressure spectrum empirical model designed to\nenhance the robustness and accuracy of current state-of-the-art predictions\nwhile widening the range of applicability of the model to different airfoils\nand flow conditions. The model is developed using AI-based symbolic regression\nvia a genetic-algorithm-based approach, and applied to a dataset of\nwall-pressure fluctuations measured on NACA 0008 and NACA 63018 airfoils at\nmultiple angles of attack and inflow velocities, covering turbulent boundary\nlayers with both adverse and favorable pressure gradients. Validation against\nexperimental data (outside the training dataset) demonstrates the robustness of\nthe model compared to well-accepted semi-empirical models. Finally, the model\nis integrated with Amiet's theory to predict the aeroacoustic noise of a\nfull-scale wind turbine, showing good agreement with experimental measurements.\n","authors":["Laura Botero Bolívar","David Huergo","Fernanda L. dos Santos","Cornelis H. Venner","Leandro D. de Santana","Esteban Ferrer"],"pdf_url":"https://arxiv.org/pdf/2501.08134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15102v2","updated":"2025-01-14T14:07:55Z","published":"2024-11-22T18:06:14Z","title":"AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution","summary":"  The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.\n","authors":["Fengyuan Liu","Nikhil Kandpal","Colin Raffel"],"pdf_url":"https://arxiv.org/pdf/2411.15102v2.pdf","comment":"29 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.00123v3","updated":"2025-01-14T14:01:36Z","published":"2024-11-28T10:32:50Z","title":"Electricity Price Prediction Using Multi-Kernel Gaussian Process\n  Regression Combined with Kernel-Based Support Vector Regression","summary":"  This paper presents a new hybrid model for predicting German electricity\nprices. The algorithm is based on combining Gaussian Process Regression (GPR)\nand Support Vector Regression (SVR). While GPR is a competent model for\nlearning the stochastic pattern within the data and interpolation, its\nperformance for out-of-sample data is not very promising. By choosing a\nsuitable data-dependent covariance function, we can enhance the performance of\nGPR for the tested German hourly power prices. However, since the out-of-sample\nprediction depends on the training data, the prediction is vulnerable to noise\nand outliers. To overcome this issue, a separate prediction is made using SVR,\nwhich applies margin-based optimization, having an advantage in dealing with\nnon-linear processes and outliers, since only certain necessary points (support\nvectors) in the training data are responsible for regression. Both individual\npredictions are later combined using the performance-based weight assignment\nmethod. A test on historic German power prices shows that this approach\noutperforms its chosen benchmarks such as the autoregressive exogenous model,\nthe naive approach, as well as the long short-term memory approach of\nprediction.\n","authors":["Abhinav Das","Stephan Schlüter","Lorenz Schneider"],"pdf_url":"https://arxiv.org/pdf/2412.00123v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16625v3","updated":"2025-01-14T13:48:49Z","published":"2023-05-26T04:34:28Z","title":"Set-based Neural Network Encoding Without Weight Tying","summary":"  We propose a neural network weight encoding method for network property\nprediction that utilizes set-to-set and set-to-vector functions to efficiently\nencode neural network parameters. Our approach is capable of encoding neural\nnetworks in a model zoo of mixed architecture and different parameter sizes as\nopposed to previous approaches that require custom encoding models for\ndifferent architectures. Furthermore, our \\textbf{S}et-based \\textbf{N}eural\nnetwork \\textbf{E}ncoder (SNE) takes into consideration the hierarchical\ncomputational structure of neural networks. To respect symmetries inherent in\nnetwork weight space, we utilize Logit Invariance to learn the required minimal\ninvariance properties. Additionally, we introduce a \\textit{pad-chunk-encode}\npipeline to efficiently encode neural network layers that is adjustable to\ncomputational and memory constraints. We also introduce two new tasks for\nneural network property prediction: cross-dataset and cross-architecture. In\ncross-dataset property prediction, we evaluate how well property predictors\ngeneralize across model zoos trained on different datasets but of the same\narchitecture. In cross-architecture property prediction, we evaluate how well\nproperty predictors transfer to model zoos of different architecture not seen\nduring training. We show that SNE outperforms the relevant baselines on\nstandard benchmarks.\n","authors":["Bruno Andreis","Soro Bedionita","Philip H. S. Torr","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2305.16625v3.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2501.08115v1","updated":"2025-01-14T13:46:07Z","published":"2025-01-14T13:46:07Z","title":"RoHan: Robust Hand Detection in Operation Room","summary":"  Hand-specific localization has garnered significant interest within the\ncomputer vision community. Although there are numerous datasets with hand\nannotations from various angles and settings, domain transfer techniques\nfrequently struggle in surgical environments. This is mainly due to the limited\navailability of gloved hand instances and the unique challenges of operating\nrooms (ORs). Thus, hand-detection models tailored to OR settings require\nextensive training and expensive annotation processes. To overcome these\nchallenges, we present \"RoHan\" - a novel approach for robust hand detection in\nthe OR, leveraging advanced semi-supervised domain adaptation techniques to\ntackle the challenges of varying recording conditions, diverse glove colors,\nand occlusions common in surgical settings. Our methodology encompasses two\nmain stages: (1) data augmentation strategy that utilizes \"Artificial Gloves,\"\na method for augmenting publicly available hand datasets with synthetic images\nof hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that\nimproves detection performance in real-world OR settings through iterative\nprediction refinement and efficient frame filtering. We evaluate our method\nusing two datasets: simulated enterotomy repair and saphenous vein graft\nharvesting. \"RoHan\" substantially reduces the need for extensive labeling and\nmodel training, paving the way for the practical implementation of hand\ndetection technologies in medical settings.\n","authors":["Roi Papo","Sapir Gershov","Tom Friedman","Itay Or","Gil Bolotin","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2501.08115v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2501.04023v2","updated":"2025-01-14T13:40:35Z","published":"2024-12-27T20:16:04Z","title":"Approximation Rates in Fréchet Metrics: Barron Spaces, Paley-Wiener\n  Spaces, and Fourier Multipliers","summary":"  Operator learning is a recent development in the simulation of Partial\nDifferential Equations (PDEs) by means of neural networks. The idea behind this\napproach is to learn the behavior of an operator, such that the resulting\nneural network is an (approximate) mapping in infinite-dimensional spaces that\nis capable of (approximately) simulating the solution operator governed by the\nPDE. In our work, we study some general approximation capabilities for linear\ndifferential operators by approximating the corresponding symbol in the Fourier\ndomain. Analogous to the structure of the class of H\\\"ormander-Symbols, we\nconsider the approximation with respect to a topology that is induced by a\nsequence of semi-norms. In that sense, we measure the approximation error in\nterms of a Fr\\'echet metric, and our main result identifies sufficient\nconditions for achieving a predefined approximation error. Secondly, we then\nfocus on a natural extension of our main theorem, in which we manage to reduce\nthe assumptions on the sequence of semi-norms. Based on existing approximation\nresults for the exponential spectral Barron space, we then present a concrete\nexample of symbols that can be approximated well.\n","authors":["Ahmed Abdeljawad","Thomas Dittrich"],"pdf_url":"https://arxiv.org/pdf/2501.04023v2.pdf","comment":"Minor revision"},{"id":"http://arxiv.org/abs/2501.08109v1","updated":"2025-01-14T13:40:08Z","published":"2025-01-14T13:40:08Z","title":"Data-driven inventory management for new products: A warm-start and\n  adjusted Dyna-$Q$ approach","summary":"  In this paper, we propose a novel reinforcement learning algorithm for\ninventory management of newly launched products with no or limited historical\ndemand information. The algorithm follows the classic Dyna-$Q$ structure,\nbalancing the model-based and model-free approaches, while accelerating the\ntraining process of Dyna-$Q$ and mitigating the model discrepancy generated by\nthe model-based feedback. Warm-start information from the demand data of\nexisting similar products can be incorporated into the algorithm to further\nstabilize the early-stage training and reduce the variance of the estimated\noptimal policy. Our approach is validated through a case study of bakery\ninventory management with real data. The adjusted Dyna-$Q$ shows up to a 23.7\\%\nreduction in average daily cost compared with $Q$-learning, and up to a 77.5\\%\nreduction in training time within the same horizon compared with classic\nDyna-$Q$. By incorporating the warm-start information, it can be found that the\nadjusted Dyna-$Q$ has the lowest total cost, lowest variance in total cost, and\nrelatively low shortage percentages among all the algorithms under a 30-day\ntesting.\n","authors":["Xinyu Qu","Longxiao Liu","Wenjie Huang"],"pdf_url":"https://arxiv.org/pdf/2501.08109v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.11304v4","updated":"2025-01-14T13:36:51Z","published":"2024-11-18T05:59:29Z","title":"Towards Federated Graph Learning in One-shot Communication","summary":"  Federated Graph Learning (FGL) has emerged as a promising paradigm for\nbreaking data silos among distributed private graphs. In practical scenarios\ninvolving heterogeneous distributed graph data, personalized Federated Graph\nLearning (pFGL) aims to enhance model utility by training personalized models\ntailored to client needs. However, existing pFGL methods often require numerous\ncommunication rounds under heterogeneous graphs, leading to significant\ncommunication overhead and security concerns. While One-shot Federated Learning\n(OFL) enables collaboration in a single round, existing OFL methods are\ndesigned for image-centric tasks and ineffective for graph data, leaving a\ncritical gap in the field. Additionally, personalized models derived from\nexisting methods suffer from bias, failing to effectively generalize to the\nminority. To address these challenges, we propose the first $\\textbf{O}$ne-shot\n$\\textbf{p}$ersonalized $\\textbf{F}$ederated $\\textbf{G}$raph\n$\\textbf{L}$earning method ($\\textbf{O-pFGL}$) for node classification,\ncompatible with Secure Aggregation protocols for privacy preservation.\nSpecifically, for effective graph learning in one communication round, our\nmethod estimates and aggregates class-wise feature distribution statistics to\nconstruct a global pseudo-graph on the server, facilitating the training of a\nglobal graph model. To mitigate bias, we introduce a two-stage personalized\ntraining approach that adaptively balances local personal information and\nglobal insights from the pseudo-graph, improving both personalization and\ngeneralization. Extensive experiments on 12 multi-scale graph datasets\ndemonstrate that our method significantly outperforms state-of-the-art\nbaselines across various settings.\n","authors":["Guochen Yan","Xunkai Li","Luyuan Xie","Wentao Zhang","Qingni Shen","Yuejian Fang","Zhonghai Wu"],"pdf_url":"https://arxiv.org/pdf/2411.11304v4.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2501.08099v1","updated":"2025-01-14T13:16:33Z","published":"2025-01-14T13:16:33Z","title":"Smooth Handovers via Smoothed Online Learning","summary":"  With users demanding seamless connectivity, handovers (HOs) have become a\nfundamental element of cellular networks. However, optimizing HOs is a\nchallenging problem, further exacerbated by the growing complexity of mobile\nnetworks. This paper presents the first countrywide study of HO optimization,\nthrough the prism of Smoothed Online Learning (SOL). We first analyze an\nextensive dataset from a commercial mobile network operator (MNO) in Europe\nwith more than 40M users, to understand and reveal important features and\nperformance impacts on HOs. Our findings highlight a correlation between HO\nfailures/delays, and the characteristics of radio cells and end-user devices,\nshowcasing the impact of heterogeneity in mobile networks nowadays. We\nsubsequently model UE-cell associations as dynamic decisions and propose a\nrealistic system model for smooth and accurate HOs that extends existing\napproaches by (i) incorporating device and cell features on HO optimization,\nand (ii) eliminating (prior) strong assumptions about requiring future signal\nmeasurements and knowledge of end-user mobility. Our algorithm, aligned with\nthe O-RAN paradigm, provides robust dynamic regret guarantees, even in\nchallenging environments, and shows superior performance in multiple scenarios\nwith real-world and synthetic data.\n","authors":["Michail Kalntis","Andra Lutu","Jesús Omaña Iglesias","Fernando A. Kuipers","George Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2501.08099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16409v2","updated":"2025-01-14T13:14:00Z","published":"2023-12-27T04:40:12Z","title":"Dynamic Sub-graph Distillation for Robust Semi-supervised Continual\n  Learning","summary":"  Continual learning (CL) has shown promising results and comparable\nperformance to learning at once in a fully supervised manner. However, CL\nstrategies typically require a large number of labeled samples, making their\nreal-life deployment challenging. In this work, we focus on semi-supervised\ncontinual learning (SSCL), where the model progressively learns from partially\nlabeled data with unknown categories. We provide a comprehensive analysis of\nSSCL and demonstrate that unreliable distributions of unlabeled data lead to\nunstable training and refinement of the progressing stages. This problem\nseverely impacts the performance of SSCL. To address the limitations, we\npropose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for\nsemi-supervised continual learning, which leverages both semantic and\nstructural information to achieve more stable knowledge distillation on\nunlabeled data and exhibit robustness against distribution bias. Firstly, we\nformalize a general model of structural distillation and design a dynamic graph\nconstruction for the continual learning progress. Next, we define a structure\ndistillation vector and design a dynamic sub-graph distillation algorithm,\nwhich enables end-to-end training and adaptability to scale up tasks. The\nentire proposed method is adaptable to various CL methods and supervision\nsettings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100,\nand ImageNet-100, with varying supervision ratios, demonstrate the\neffectiveness of our proposed approach in mitigating the catastrophic\nforgetting problem in semi-supervised continual learning scenarios.\n","authors":["Yan Fan","Yu Wang","Pengfei Zhu","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2312.16409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10174v3","updated":"2025-01-14T13:11:05Z","published":"2024-10-14T05:45:52Z","title":"Balanced Neural ODEs: nonlinear model order reduction and Koopman\n  operator approximations","summary":"  Variational Autoencoders (VAEs) are a powerful framework for learning latent\nrepresentations of reduced dimensionality, while Neural ODEs excel in learning\ntransient system dynamics. This work combines the strengths of both to generate\nfast surrogate models with adjustable complexity reacting on time-varying\ninputs signals. By leveraging the VAE's dimensionality reduction using a\nnonhierarchical prior, our method adaptively assigns stochastic noise,\nnaturally complementing known NeuralODE training enhancements and enabling\nprobabilistic time series modeling. We show that standard Latent ODEs struggle\nwith dimensionality reduction in systems with time-varying inputs. Our approach\nmitigates this by continuously propagating variational parameters through time,\nestablishing fixed information channels in latent space. This results in a\nflexible and robust method that can learn different system complexities, e.g.\ndeep neural networks or linear matrices. Hereby, it enables efficient\napproximation of the Koopman operator without the need for predefining its\ndimensionality. As our method balances dimensionality reduction and\nreconstruction accuracy, we call it Balanced Neural ODE (B-NODE). We\ndemonstrate the effectiveness of this methods on several academic and\nreal-world test cases, e.g. a power plant or MuJoCo data.\n","authors":["Julius Aka","Johannes Brunnemann","Jörg Eiden","Arne Speerforck","Lars Mikelsons"],"pdf_url":"https://arxiv.org/pdf/2410.10174v3.pdf","comment":"Conference paper under review, after revision"},{"id":"http://arxiv.org/abs/2501.08096v1","updated":"2025-01-14T13:10:13Z","published":"2025-01-14T13:10:13Z","title":"Hybrid Action Based Reinforcement Learning for Multi-Objective\n  Compatible Autonomous Driving","summary":"  Reinforcement Learning (RL) has shown excellent performance in solving\ndecision-making and control problems of autonomous driving, which is\nincreasingly applied in diverse driving scenarios. However, driving is a\nmulti-attribute problem, leading to challenges in achieving multi-objective\ncompatibility for current RL methods, especially in both policy execution and\npolicy iteration. On the one hand, the common action space structure with\nsingle action type limits driving flexibility or results in large behavior\nfluctuations during policy execution. On the other hand, the multi-attribute\nweighted single reward function result in the agent's disproportionate\nattention to certain objectives during policy iterations. To this end, we\npropose a Multi-objective Ensemble-Critic reinforcement learning method with\nHybrid Parametrized Action for multi-objective compatible autonomous driving.\nSpecifically, a parameterized action space is constructed to generate hybrid\ndriving actions, combining both abstract guidance and concrete control\ncommands. A multi-objective critics architecture is constructed considering\nmultiple attribute rewards, to ensure simultaneously focusing on different\ndriving objectives. Additionally, uncertainty-based exploration strategy is\nintroduced to help the agent faster approach viable driving policy. The\nexperimental results in both the simulated traffic environment and the HighD\ndataset demonstrate that our method can achieve multi-objective compatible\nautonomous driving in terms of driving efficiency, action consistency, and\nsafety. It enhances the general performance of the driving while significantly\nincreasing training efficiency.\n","authors":["Guizhe Jin","Zhuoren Li","Bo Leng","Wei Han","Lu Xiong","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2501.08096v1.pdf","comment":"12 pages, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2501.08085v1","updated":"2025-01-14T12:54:19Z","published":"2025-01-14T12:54:19Z","title":"Dynamic Multimodal Sentiment Analysis: Leveraging Cross-Modal Attention\n  for Enabled Classification","summary":"  This paper explores the development of a multimodal sentiment analysis model\nthat integrates text, audio, and visual data to enhance sentiment\nclassification. The goal is to improve emotion detection by capturing the\ncomplex interactions between these modalities, thereby enabling more accurate\nand nuanced sentiment interpretation. The study evaluates three feature fusion\nstrategies -- late stage fusion, early stage fusion, and multi-headed attention\n-- within a transformer-based architecture. Experiments were conducted using\nthe CMU-MOSEI dataset, which includes synchronized text, audio, and visual\ninputs labeled with sentiment scores. Results show that early stage fusion\nsignificantly outperforms late stage fusion, achieving an accuracy of 71.87\\%,\nwhile the multi-headed attention approach offers marginal improvement, reaching\n72.39\\%. The findings suggest that integrating modalities early in the process\nenhances sentiment classification, while attention mechanisms may have limited\nimpact within the current framework. Future work will focus on refining feature\nfusion techniques, incorporating temporal data, and exploring dynamic feature\nweighting to further improve model performance.\n","authors":["Hui Lee","Singh Suniljit","Yong Siang Ong"],"pdf_url":"https://arxiv.org/pdf/2501.08085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00376v3","updated":"2025-01-14T12:37:26Z","published":"2024-03-01T09:01:53Z","title":"Spurious Feature Eraser: Stabilizing Test-Time Adaptation for\n  Vision-Language Foundation Model","summary":"  Vision-language foundation models have exhibited remarkable success across a\nmultitude of downstream tasks due to their scalability on extensive image-text\npaired data. However, these models also display significant limitations when\napplied to downstream tasks, such as fine-grained image classification, as a\nresult of ``decision shortcuts'' that hinder their generalization capabilities.\nIn this work, we find that the CLIP model possesses a rich set of features,\nencompassing both \\textit{desired invariant causal features} and\n\\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP\non downstream tasks originates from its inability to effectively utilize\npre-trained features in accordance with specific task requirements. To address\nthis challenge, we propose a simple yet effective method, Spurious Feature\nEraser (SEraser), to alleviate the decision shortcuts by erasing the spurious\nfeatures. Specifically, we introduce a test-time prompt tuning paradigm that\noptimizes a learnable prompt, thereby compelling the model to exploit invariant\nfeatures while disregarding decision shortcuts during the inference phase. The\nproposed method effectively alleviates excessive dependence on potentially\nmisleading spurious information. We conduct comparative analysis of the\nproposed method against various approaches which validates the significant\nsuperiority.\n","authors":["Huan Ma","Yan Zhu","Changqing Zhang","Peilin Zhao","Baoyuan Wu","Long-Kai Huang","Qinghua Hu","Bingzhe Wu"],"pdf_url":"https://arxiv.org/pdf/2403.00376v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08071v1","updated":"2025-01-14T12:36:18Z","published":"2025-01-14T12:36:18Z","title":"CuAsmRL: Optimizing GPU SASS Schedules via Deep Reinforcement Learning","summary":"  Large language models (LLMs) are remarked by their substantial computational\nrequirements. To mitigate the cost, researchers develop specialized CUDA\nkernels, which often fuse several tensor operations to maximize the utilization\nof GPUs as much as possible. However, those specialized kernels may still leave\nperformance on the table as CUDA assembly experts show that manual optimization\nof GPU SASS schedules can lead to better performance, and trial-and-error is\nlargely employed to manually find the best GPU SASS schedules.\n  In this work, we employ an automatic approach to optimize GPU SASS schedules,\nwhich thus can be integrated into existing compiler frameworks. The key to\nautomatic optimization is training an RL agent to mimic how human experts\nperform manual scheduling. To this end, we formulate an assembly game, where RL\nagents can play to find the best GPU SASS schedules. The assembly game starts\nfrom a \\textit{-O3} optimized SASS schedule, and the RL agents can iteratively\napply actions to mutate the current schedules. Positive rewards are generated\nif the mutated schedules get higher throughput by executing on GPUs.\nExperiments show that CuAsmRL can further improve the performance of existing\nspecialized CUDA kernels transparently by up to $26\\%$, and on average $9\\%$.\nMoreover, it is used as a tool to reveal potential optimization moves learned\nautomatically.\n","authors":["Guoliang He","Eiko Yoneki"],"pdf_url":"https://arxiv.org/pdf/2501.08071v1.pdf","comment":"cgo 2025"},{"id":"http://arxiv.org/abs/2501.08067v1","updated":"2025-01-14T12:33:02Z","published":"2025-01-14T12:33:02Z","title":"Optimal Policy Adaptation under Covariate Shift","summary":"  Transfer learning of prediction models has been extensively studied, while\nthe corresponding policy learning approaches are rarely discussed. In this\npaper, we propose principled approaches for learning the optimal policy in the\ntarget domain by leveraging two datasets: one with full information from the\nsource domain and the other from the target domain with only covariates. First,\nunder the setting of covariate shift, we formulate the problem from a\nperspective of causality and present the identifiability assumptions for the\nreward induced by a given policy. Then, we derive the efficient influence\nfunction and the semiparametric efficiency bound for the reward. Based on this,\nwe construct a doubly robust and semiparametric efficient estimator for the\nreward and then learn the optimal policy by optimizing the estimated reward.\nMoreover, we theoretically analyze the bias and the generalization error bound\nfor the learned policy. Furthermore, in the presence of both covariate and\nconcept shifts, we propose a novel sensitivity analysis method to evaluate the\nrobustness of the proposed policy learning approach. Extensive experiments\ndemonstrate that the approach not only estimates the reward more accurately but\nalso yields a policy that closely approximates the theoretically optimal\npolicy.\n","authors":["Xueqing Liu","Qinwei Yang","Zhaoqing Tian","Ruocheng Guo","Peng Wu"],"pdf_url":"https://arxiv.org/pdf/2501.08067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20020v3","updated":"2025-01-14T12:31:48Z","published":"2024-07-29T13:57:24Z","title":"ImagiNet: A Multi-Content Benchmark for Synthetic Image Detection","summary":"  Recent generative models produce images with a level of authenticity that\nmakes them nearly indistinguishable from real photos and artwork. Potential\nharmful use cases of these models, necessitate the creation of robust synthetic\nimage detectors. However, current datasets in the field contain generated\nimages with questionable quality or have examples from one predominant content\ntype which leads to poor generalizability of the underlying detectors. We find\nthat the curation of a balanced amount of high-resolution generated images\nacross various content types is crucial for the generalizability of detectors,\nand introduce ImagiNet, a dataset of 200K examples, spanning four categories:\nphotos, paintings, faces, and miscellaneous. Synthetic images in ImagiNet are\nproduced with both open-source and proprietary generators, whereas real\ncounterparts for each content type are collected from public datasets. The\nstructure of ImagiNet allows for a two-track evaluation system: i)\nclassification as real or synthetic and ii) identification of the generative\nmodel. To establish a strong baseline, we train a ResNet-50 model using a\nself-supervised contrastive objective (SelfCon) for each track which achieves\nevaluation AUC of up to 0.99 and balanced accuracy ranging from 86% to 95%,\neven under conditions that involve compression and resizing. The provided model\nis generalizable enough to achieve zero-shot state-of-the-art performance on\nprevious synthetic detection benchmarks. We provide ablations to demonstrate\nthe importance of content types and publish code and data.\n","authors":["Delyan Boychev","Radostin Cholakov"],"pdf_url":"https://arxiv.org/pdf/2407.20020v3.pdf","comment":"Workshop on Datasets and Evaluators of AI Safety, AAAI 2025"},{"id":"http://arxiv.org/abs/2410.03335v2","updated":"2025-01-14T11:59:03Z","published":"2024-10-04T11:40:53Z","title":"Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition","summary":"  We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions and calls the agent for audio generation. In doing so,\nAudio-Agent can generate high-quality audio that is closely aligned with the\nprovided text or video exhibiting complex and multiple events, while supporting\nvariable-length and variable-volume generation. For video-to-audio (VTA) tasks,\nmost existing methods require training a timestamp detector to synchronize\nvideo events with the generated audio, a process that can be tedious and\ntime-consuming. Instead, we propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions that bridge the video and audio modality.\nConsequently, our framework contributes a comprehensive solution for both TTA\nand VTA tasks without substantial computational overhead in training.\n","authors":["Zixuan Wang","Chi-Keung Tang","Yu-Wing Tai"],"pdf_url":"https://arxiv.org/pdf/2410.03335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08050v1","updated":"2025-01-14T11:56:05Z","published":"2025-01-14T11:56:05Z","title":"On the use of Statistical Learning Theory for model selection in\n  Structural Health Monitoring","summary":"  Whenever data-based systems are employed in engineering applications,\ndefining an optimal statistical representation is subject to the problem of\nmodel selection. This paper focusses on how well models can generalise in\nStructural Health Monitoring (SHM). Although statistical model validation in\nthis field is often performed heuristically, it is possible to estimate\ngeneralisation more rigorously using the bounds provided by Statistical\nLearning Theory (SLT). Therefore, this paper explores the selection process of\na kernel smoother for modelling the impulse response of a linear oscillator\nfrom the perspective of SLT. It is demonstrated that incorporating domain\nknowledge into the regression problem yields a lower guaranteed risk, thereby\nenhancing generalisation.\n","authors":["C. A. Lindley","N. Dervilis","K. Worden"],"pdf_url":"https://arxiv.org/pdf/2501.08050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08049v1","updated":"2025-01-14T11:56:00Z","published":"2025-01-14T11:56:00Z","title":"Self-Attentive Spatio-Temporal Calibration for Precise Intermediate\n  Layer Matching in ANN-to-SNN Distillation","summary":"  Spiking Neural Networks (SNNs) are promising for low-power computation due to\ntheir event-driven mechanism but often suffer from lower accuracy compared to\nArtificial Neural Networks (ANNs). ANN-to-SNN knowledge distillation can\nimprove SNN performance, but previous methods either focus solely on label\ninformation, missing valuable intermediate layer features, or use a layer-wise\napproach that neglects spatial and temporal semantic inconsistencies, leading\nto performance degradation.To address these limitations, we propose a novel\nmethod called self-attentive spatio-temporal calibration (SASTC). SASTC uses\nself-attention to identify semantically aligned layer pairs between ANN and\nSNN, both spatially and temporally. This enables the autonomous transfer of\nrelevant semantic information. Extensive experiments show that SASTC\noutperforms existing methods, effectively solving the mismatching problem.\nSuperior accuracy results include 95.12% on CIFAR-10, 79.40% on CIFAR-100 with\n2 time steps, and 68.69% on ImageNet with 4 time steps for static datasets, and\n97.92% on DVS-Gesture and 83.60% on DVS-CIFAR10 for neuromorphic datasets. This\nmarks the first time SNNs have outperformed ANNs on both CIFAR-10 and\nCIFAR-100, shedding the new light on the potential applications of SNNs.\n","authors":["Di Hong","Yueming Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08047v1","updated":"2025-01-14T11:54:45Z","published":"2025-01-14T11:54:45Z","title":"Gen-A: Generalizing Ambisonics Neural Encoding to Unseen Microphone\n  Arrays","summary":"  Using deep neural networks (DNNs) for encoding of microphone array (MA)\nsignals to the Ambisonics spatial audio format can surpass certain limitations\nof established conventional methods, but existing DNN-based methods need to be\ntrained separately for each MA. This paper proposes a DNN-based method for\nAmbisonics encoding that can generalize to arbitrary MA geometries unseen\nduring training. The method takes as inputs the MA geometry and MA signals and\nuses a multi-level encoder consisting of separate paths for geometry and signal\ndata, where geometry features inform the signal encoder at each level. The\nmethod is validated in simulated anechoic and reverberant conditions with one\nand two sources. The results indicate improvement over conventional encoding\nacross the whole frequency range for dry scenes, while for reverberant scenes\nthe improvement is frequency-dependent.\n","authors":["Mikko Heikkinen","Archontis Politis","Konstantinos Drossos","Tuomas Virtanen"],"pdf_url":"https://arxiv.org/pdf/2501.08047v1.pdf","comment":"Accepted for publication in Proceedings of the 2025 IEEE\n  International Conference on Acoustics, Speech and Signal Processing"},{"id":"http://arxiv.org/abs/2501.08044v1","updated":"2025-01-14T11:52:16Z","published":"2025-01-14T11:52:16Z","title":"UFGraphFR: An attempt at a federated recommendation system based on user\n  text characteristics","summary":"  Federated learning has become an important research area in 'private\ncomputing' due to the 'useable invisibility' of data during training. Inspired\nby Federated learning, the federated recommendation system has gradually become\na new recommendation service architecture that can protect users' privacy. The\nuse of user diagrams to enhance federated recommendations is a promising topic.\nHow to use user diagrams to enhance federated recommendations is a promising\nresearch topic. However, it's a great challenge to construct a user diagram\nwithout compromising privacy in a federated learning scenario. Inspired by the\nsimple idea that similar users often have the same attribute characteristics,\nwe propose a personalized federated recommendation algorithm based on the user\nrelationship graph constructed by the user text characteristics(Graph\nFederation Recommendation System based on User Text description Features,\nUFGraphFR). The method uses the embedding layer weight of the user's text\nfeature description to construct the user relationship graph. It introduces the\nTransformer mechanism to capture the sequence modeling of the user's historical\ninteraction sequence. Without access to user history interactions and specific\nuser attributes, the federal learning privacy protection of data 'useable\ninvisibility' is embodied. Preliminary experiments on some benchmark datasets\ndemonstrate the superior performance of UFGraphFR. Our experiments show that\nthis model can protect user privacy to some extent without affecting the\nperformance of the recommendation system. The code will be easily available on\nhttps://github.com/trueWangSyutung/UFGraphFR.\n","authors":["Xudong Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08043v1","updated":"2025-01-14T11:51:57Z","published":"2025-01-14T11:51:57Z","title":"PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware\n  Structured Pruning","summary":"  Standard deep neural network inference involves the computation of\ninterleaved linear maps and nonlinear activation functions. Prior work for\nultra-low latency implementations has hardcoded these operations inside FPGA\nlookup tables (LUTs). However, FPGA LUTs can implement a much greater variety\nof functions. In this paper, we propose a novel approach to training DNNs for\nFPGA deployment using multivariate polynomials as the basic building block. Our\nmethod takes advantage of the flexibility offered by the soft logic, hiding the\npolynomial evaluation inside the LUTs with minimal overhead. By using\npolynomial building blocks, we achieve the same accuracy using considerably\nfewer layers of soft logic than by using linear functions, leading to\nsignificant latency and area improvements. LUT-based implementations also face\na significant challenge: the LUT size grows exponentially with the number of\ninputs. Prior work relies on a priori fixed sparsity, with results heavily\ndependent on seed selection. To address this, we propose a structured pruning\nstrategy using a bespoke hardware-aware group regularizer that encourages a\nparticular sparsity pattern that leads to a small number of inputs per neuron.\nWe demonstrate the effectiveness of PolyLUT on three tasks: network intrusion\ndetection, jet identification at the CERN Large Hadron Collider, and MNIST.\n","authors":["Marta Andronic","Jiawen Li","George A. Constantinides"],"pdf_url":"https://arxiv.org/pdf/2501.08043v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2309.02334"},{"id":"http://arxiv.org/abs/2501.08040v1","updated":"2025-01-14T11:46:36Z","published":"2025-01-14T11:46:36Z","title":"Convergence Analysis of Real-time Recurrent Learning (RTRL) for a class\n  of Recurrent Neural Networks","summary":"  Recurrent neural networks (RNNs) are commonly trained with the truncated\nbackpropagation-through-time (TBPTT) algorithm. For the purposes of\ncomputational tractability, the TBPTT algorithm truncates the chain rule and\ncalculates the gradient on a finite block of the overall data sequence. Such\napproximation could lead to significant inaccuracies, as the block length for\nthe truncated backpropagation is typically limited to be much smaller than the\noverall sequence length. In contrast, Real-time recurrent learning (RTRL) is an\nonline optimization algorithm which asymptotically follows the true gradient of\nthe loss on the data sequence as the number of sequence time steps $t\n\\rightarrow \\infty$. RTRL forward propagates the derivatives of the RNN\nhidden/memory units with respect to the parameters and, using the forward\nderivatives, performs online updates of the parameters at each time step in the\ndata sequence. RTRL's online forward propagation allows for exact optimization\nover extremely long data sequences, although it can be computationally costly\nfor models with large numbers of parameters. We prove convergence of the RTRL\nalgorithm for a class of RNNs. The convergence analysis establishes a fixed\npoint for the joint distribution of the data sequence, RNN hidden layer, and\nthe RNN hidden layer forward derivatives as the number of data samples from the\nsequence and the number of training steps tend to infinity. We prove\nconvergence of the RTRL algorithm to a stationary point of the loss. Numerical\nstudies illustrate our theoretical results. One potential application area for\nRTRL is the analysis of financial data, which typically involve long time\nseries and models with small to medium numbers of parameters. This makes RTRL\ncomputationally tractable and a potentially appealing optimization method for\ntraining models. Thus, we include an example of RTRL applied to limit order\nbook data.\n","authors":["Samuel Chun-Hei Lam","Justin Sirignano","Konstantinos Spiliopoulos"],"pdf_url":"https://arxiv.org/pdf/2501.08040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08037v1","updated":"2025-01-14T11:42:51Z","published":"2025-01-14T11:42:51Z","title":"Enhanced SPS Velocity-adaptive Scheme: Access Fariness in 5G NR V2I\n  Networks","summary":"  Vehicle-to-Infrastructure (V2I) technology enables information exchange\nbetween vehicles and road infrastructure. Specifically, when a vehicle\napproaches a roadside unit (RSU), it can exchange information with the RSU to\nobtain accurate data that assists in driving. With the release of the 3rd\nGeneration Partnership Project (3GPP) Release 16, which includes the 5G New\nRadio (NR) Vehicle-to-Everything (V2X) standards, vehicles typically adopt\nmode-2 communication using sensing-based semi-persistent scheduling (SPS) for\nresource allocation. In this approach, vehicles identify candidate resources\nwithin a selection window and exclude ineligible resources based on information\nfrom a sensing window. However, vehicles often drive at different speeds,\nresulting in varying amounts of data transmission with RSUs as they pass by,\nwhich leads to unfair access. Therefore, it is essential to design an access\nscheme that accounts for different vehicle speeds to achieve fair access across\nthe network. This paper formulates an optimization problem for vehicular\nnetworks and proposes a multi-objective optimization scheme to address it by\nadjusting the selection window in the SPS mechanism of 5G NR V2I mode-2.\nSimulation results demonstrate the effectiveness of the proposed scheme\n","authors":["Xiao Xu","Qiong Wu","Pingyi Fan","Kezhi Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08037v1.pdf","comment":"This paper has been submitted to IEEE Journal. The source code has\n  been released at:\n  https://github.com/qiongwu86/Enhanced-SPS-Velocity-adaptiveScheme-Access-Fariness-in-5G-NR-V2I-Networks"},{"id":"http://arxiv.org/abs/2411.17350v2","updated":"2025-01-14T11:34:26Z","published":"2024-11-26T11:52:47Z","title":"Correlation-Aware Graph Convolutional Networks for Multi-Label Node\n  Classification","summary":"  Multi-label node classification is an important yet under-explored domain in\ngraph mining as many real-world nodes belong to multiple categories rather than\njust a single one. Although a few efforts have been made by utilizing Graph\nConvolution Networks (GCNs) to learn node representations and model\ncorrelations between multiple labels in the embedding space, they still suffer\nfrom the ambiguous feature and ambiguous topology induced by multiple labels,\nwhich reduces the credibility of the messages delivered in graphs and overlooks\nthe label correlations on graph data. Therefore, it is crucial to reduce the\nambiguity and empower the GCNs for accurate classification. However, this is\nquite challenging due to the requirement of retaining the distinctiveness of\neach label while fully harnessing the correlation between labels\nsimultaneously. To address these issues, in this paper, we propose a\nCorrelation-aware Graph Convolutional Network (CorGCN) for multi-label node\nclassification. By introducing a novel Correlation-Aware Graph Decomposition\nmodule, CorGCN can learn a graph that contains rich label-correlated\ninformation for each label. It then employs a Correlation-Enhanced Graph\nConvolution to model the relationships between labels during message passing to\nfurther bolster the classification process. Extensive experiments on five\ndatasets demonstrate the effectiveness of our proposed CorGCN.\n","authors":["Yuanchen Bei","Weizhi Chen","Hao Chen","Sheng Zhou","Carl Yang","Jiapei Fan","Longtao Huang","Jiajun Bu"],"pdf_url":"https://arxiv.org/pdf/2411.17350v2.pdf","comment":"12 pages, accepted by KDD2025"},{"id":"http://arxiv.org/abs/2402.03169v3","updated":"2025-01-14T11:32:56Z","published":"2024-02-05T16:38:30Z","title":"A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation","summary":"  This work presents a comprehensive understanding of the estimation of a\nplanted low-rank signal from a general spiked tensor model near the\ncomputational threshold. Relying on standard tools from the theory of large\nrandom matrices, we characterize the large-dimensional spectral behavior of the\nunfoldings of the data tensor and exhibit relevant signal-to-noise ratios\ngoverning the detectability of the principal directions of the signal. These\nresults allow to accurately predict the reconstruction performance of truncated\nmultilinear SVD (MLSVD) in the non-trivial regime. This is particularly\nimportant since it serves as an initialization of the higher-order orthogonal\niteration (HOOI) scheme, whose convergence to the best low-multilinear-rank\napproximation depends entirely on its initialization. We give a sufficient\ncondition for the convergence of HOOI and show that the number of iterations\nbefore convergence tends to $1$ in the large-dimensional limit.\n","authors":["Hugo Lebeau","Florent Chatelain","Romain Couillet"],"pdf_url":"https://arxiv.org/pdf/2402.03169v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08019v1","updated":"2025-01-14T11:19:52Z","published":"2025-01-14T11:19:52Z","title":"An AI-driven framework for rapid and localized optimizations of urban\n  open spaces","summary":"  As urbanization accelerates, open spaces are increasingly recognized for\ntheir role in enhancing sustainability and well-being, yet they remain\nunderexplored compared to built spaces. This study introduces an AI-driven\nframework that integrates machine learning models (MLMs) and explainable AI\ntechniques to optimize Sky View Factor (SVF) and visibility, key spatial\nmetrics influencing thermal comfort and perceived safety in urban spaces.\nUnlike global optimization methods, which are computationally intensive and\nimpractical for localized adjustments, this framework supports incremental\ndesign improvements with lower computational costs and greater flexibility. The\nframework employs SHapley Adaptive Explanations (SHAP) to analyze feature\nimportance and Counterfactual Explanations (CFXs) to propose minimal design\nchanges. Simulations tested five MLMs, identifying XGBoost as the most\naccurate, with building width, park area, and heights of surrounding buildings\nas critical for SVF, and distances from southern buildings as key for\nvisibility. Compared to Genetic Algorithms, which required approximately 15/30\nminutes across 3/4 generations to converge, the tested CFX approach achieved\noptimized results in 1 minute with a 5% RMSE error, demonstrating significantly\nfaster performance and suitability for scalable retrofitting strategies. This\ninterpretable and computationally efficient framework advances urban\nperformance optimization, providing data-driven insights and practical\nretrofitting solutions for enhancing usability and environmental quality across\ndiverse urban contexts.\n","authors":["Pegah Eshraghi","Arman Nikkhah Dehnavi","Maedeh Mirdamadi","Riccardo Talami","Zahra-Sadat Zomorodian"],"pdf_url":"https://arxiv.org/pdf/2501.08019v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2403.02774v3","updated":"2025-01-14T11:14:57Z","published":"2024-03-05T08:41:41Z","title":"Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System\n  Model Fields with Generative Machine Learning","summary":"  Accurate and high-resolution Earth system model (ESM) simulations are\nessential to assess the ecological and socio-economic impacts of anthropogenic\nclimate change, but are computationally too expensive to be run at sufficiently\nhigh spatial resolution. Recent machine learning approaches have shown\npromising results in downscaling ESM simulations, outperforming\nstate-of-the-art statistical approaches. However, existing methods require\ncomputationally costly retraining for each ESM and extrapolate poorly to\nclimates unseen during training. We address these shortcomings by learning a\nconsistency model (CM) that efficiently and accurately downscales arbitrary ESM\nsimulations without retraining in a zero-shot manner. Our approach yields\nprobabilistic downscaled fields at a resolution only limited by the\nobservational reference data. We show that the CM outperforms state-of-the-art\ndiffusion models at a fraction of computational cost while maintaining high\ncontrollability on the downscaling task. Further, our method generalizes to\nclimate states unseen during training without explicitly formulated physical\nconstraints.\n","authors":["Philipp Hess","Michael Aich","Baoxiang Pan","Niklas Boers"],"pdf_url":"https://arxiv.org/pdf/2403.02774v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04594v2","updated":"2025-01-14T11:03:05Z","published":"2024-12-05T20:15:34Z","title":"Learning Symmetries via Weight-Sharing with Doubly Stochastic Tensors","summary":"  Group equivariance has emerged as a valuable inductive bias in deep learning,\nenhancing generalization, data efficiency, and robustness. Classically, group\nequivariant methods require the groups of interest to be known beforehand,\nwhich may not be realistic for real-world data. Additionally, baking in fixed\ngroup equivariance may impose overly restrictive constraints on model\narchitecture. This highlights the need for methods that can dynamically\ndiscover and apply symmetries as soft constraints. For neural network\narchitectures, equivariance is commonly achieved through group transformations\nof a canonical weight tensor, resulting in weight sharing over a given group\n$G$. In this work, we propose to learn such a weight-sharing scheme by defining\na collection of learnable doubly stochastic matrices that act as soft\npermutation matrices on canonical weight tensors, which can take regular group\nrepresentations as a special case. This yields learnable kernel transformations\nthat are jointly optimized with downstream tasks. We show that when the dataset\nexhibits strong symmetries, the permutation matrices will converge to regular\ngroup representations and our weight-sharing networks effectively become\nregular group convolutions. Additionally, the flexibility of the method enables\nit to effectively pick up on partial symmetries.\n","authors":["Putri A. van der Linden","Alejandro García-Castellanos","Sharvaree Vadgama","Thijs P. Kuipers","Erik J. Bekkers"],"pdf_url":"https://arxiv.org/pdf/2412.04594v2.pdf","comment":"19 pages, 14 figures, 4 tables"},{"id":"http://arxiv.org/abs/2501.08002v1","updated":"2025-01-14T10:46:41Z","published":"2025-01-14T10:46:41Z","title":"Maximizing Uncertainty for Federated learning via Bayesian\n  Optimisation-based Model Poisoning","summary":"  As we transition from Narrow Artificial Intelligence towards Artificial Super\nIntelligence, users are increasingly concerned about their privacy and the\ntrustworthiness of machine learning (ML) technology. A common denominator for\nthe metrics of trustworthiness is the quantification of uncertainty inherent in\nDL algorithms, and specifically in the model parameters, input data, and model\npredictions. One of the common approaches to address privacy-related issues in\nDL is to adopt distributed learning such as federated learning (FL), where\nprivate raw data is not shared among users. Despite the privacy-preserving\nmechanisms in FL, it still faces challenges in trustworthiness. Specifically,\nthe malicious users, during training, can systematically create malicious model\nparameters to compromise the models predictive and generative capabilities,\nresulting in high uncertainty about their reliability. To demonstrate malicious\nbehaviour, we propose a novel model poisoning attack method named Delphi which\naims to maximise the uncertainty of the global model output. We achieve this by\ntaking advantage of the relationship between the uncertainty and the model\nparameters of the first hidden layer of the local model. Delphi employs two\ntypes of optimisation , Bayesian Optimisation and Least Squares Trust Region,\nto search for the optimal poisoned model parameters, named as Delphi-BO and\nDelphi-LSTR. We quantify the uncertainty using the KL Divergence to minimise\nthe distance of the predictive probability distribution towards an uncertain\ndistribution of model output. Furthermore, we establish a mathematical proof\nfor the attack effectiveness demonstrated in FL. Numerical results demonstrate\nthat Delphi-BO induces a higher amount of uncertainty than Delphi-LSTR\nhighlighting vulnerability of FL systems to model poisoning attacks.\n","authors":["Marios Aristodemou","Xiaolan Liu","Yuan Wang","Konstantinos G. Kyriakopoulos","Sangarapillai Lambotharan","Qingsong Wei"],"pdf_url":"https://arxiv.org/pdf/2501.08002v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2501.07999v1","updated":"2025-01-14T10:41:46Z","published":"2025-01-14T10:41:46Z","title":"Unsupervised Feature Construction for Anomaly Detection in Time Series\n  -- An Evaluation","summary":"  To detect anomalies with precision and without prior knowledge in time\nseries, is it better to build a detector from the initial temporal\nrepresentation, or to compute a new (tabular) representation using an existing\nautomatic variable construction library? In this article, we address this\nquestion by conducting an in-depth experimental study for two popular detectors\n(Isolation Forest and Local Outlier Factor). The obtained results, for 5\ndifferent datasets, show that the new representation, computed using the\ntsfresh library, allows Isolation Forest to significantly improve its\nperformance.\n","authors":["Marine Hamon","Vincent Lemaire","Nour Eddine Yassine Nair-Benrekia","Samuel Berlemont","Julien Cumin"],"pdf_url":"https://arxiv.org/pdf/2501.07999v1.pdf","comment":"7"},{"id":"http://arxiv.org/abs/2410.07662v3","updated":"2025-01-14T10:41:34Z","published":"2024-10-10T07:12:32Z","title":"Scalable and Resource-Efficient Second-Order Federated Learning via\n  Over-the-Air Aggregation","summary":"  Second-order federated learning (FL) algorithms offer faster convergence than\ntheir first-order counterparts by leveraging curvature information. However,\nthey are hindered by high computational and storage costs, particularly for\nlarge-scale models. Furthermore, the communication overhead associated with\nlarge models and digital transmission exacerbates these challenges, causing\ncommunication bottlenecks. In this work, we propose a scalable second-order FL\nalgorithm using a sparse Hessian estimate and leveraging over-the-air\naggregation, making it feasible for larger models. Our simulation results\ndemonstrate more than $67\\%$ of communication resources and energy savings\ncompared to other first and second-order baselines.\n","authors":["Abdulmomen Ghalkha","Chaouki Ben Issaid","Mehdi Bennis"],"pdf_url":"https://arxiv.org/pdf/2410.07662v3.pdf","comment":"6 pages, 1 figure, 4 subfigures, letter"},{"id":"http://arxiv.org/abs/2501.07996v1","updated":"2025-01-14T10:39:04Z","published":"2025-01-14T10:39:04Z","title":"Reward Compatibility: A Framework for Inverse RL","summary":"  We provide an original theoretical study of Inverse Reinforcement Learning\n(IRL) through the lens of reward compatibility, a novel framework to quantify\nthe compatibility of a reward with the given expert's demonstrations.\nIntuitively, a reward is more compatible with the demonstrations the closer the\nperformance of the expert's policy computed with that reward is to the optimal\nperformance for that reward. This generalizes the notion of feasible reward\nset, the most common framework in the theoretical IRL literature, for which a\nreward is either compatible or not compatible. The grayscale introduced by the\nreward compatibility is the key to extend the realm of provably efficient IRL\nfar beyond what is attainable with the feasible reward set: from tabular to\nlarge-scale MDPs. We analyze the IRL problem across various settings, including\noptimal and suboptimal expert's demonstrations and both online and offline data\ncollection. For all of these dimensions, we provide a tractable algorithm and\ncorresponding sample complexity analysis, as well as various insights on reward\ncompatibility and how the framework can pave the way to yet more general\nproblem settings.\n","authors":["Filippo Lazzati","Mirco Mutti","Alberto Metelli"],"pdf_url":"https://arxiv.org/pdf/2501.07996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07994v1","updated":"2025-01-14T10:38:18Z","published":"2025-01-14T10:38:18Z","title":"Combining imaging and shape features for prediction tasks of Alzheimer's\n  disease classification and brain age regression","summary":"  We investigate combining imaging and shape features extracted from MRI for\nthe clinically relevant tasks of brain age prediction and Alzheimer's disease\nclassification. Our proposed model fuses ResNet-extracted image embeddings with\nshape embeddings from a bespoke graph neural network. The shape embeddings are\nderived from surface meshes of 15 brain structures, capturing detailed\ngeometric information. Combined with the appearance features from T1-weighted\nimages, we observe improvements in the prediction performance on both tasks,\nwith substantial gains for classification. We evaluate the model using public\ndatasets, including CamCAN, IXI, and OASIS3, demonstrating the effectiveness of\nfusing imaging and shape features for brain analysis.\n","authors":["Nairouz Shehata","Carolina Piçarra","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2501.07994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03033v3","updated":"2025-01-14T10:34:00Z","published":"2024-11-05T12:10:02Z","title":"Rethinking Decoders for Transformer-based Semantic Segmentation: A\n  Compression Perspective","summary":"  State-of-the-art methods for Transformer-based semantic segmentation\ntypically adopt Transformer decoders that are used to extract additional\nembeddings from image embeddings via cross-attention, refine either or both\ntypes of embeddings via self-attention, and project image embeddings onto the\nadditional embeddings via dot-product. Despite their remarkable success, these\nempirical designs still lack theoretical justifications or interpretations,\nthus hindering potentially principled improvements. In this paper, we argue\nthat there are fundamental connections between semantic segmentation and\ncompression, especially between the Transformer decoders and Principal\nComponent Analysis (PCA). From such a perspective, we derive a white-box, fully\nattentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the\ninterpretations as follows: 1) the self-attention operator refines image\nembeddings to construct an ideal principal subspace that aligns with the\nsupervision and retains most information; 2) the cross-attention operator seeks\nto find a low-rank approximation of the refined image embeddings, which is\nexpected to be a set of orthonormal bases of the principal subspace and\ncorresponds to the predefined classes; 3) the dot-product operation yields\ncompact representation for image embeddings as segmentation masks. Experiments\nconducted on dataset ADE20K find that DEPICT consistently outperforms its\nblack-box counterpart, Segmenter, and it is light weight and more robust.\n","authors":["Qishuai Wen","Chun-Guang Li"],"pdf_url":"https://arxiv.org/pdf/2411.03033v3.pdf","comment":"NeurIPS2024. Code:https://github.com/QishuaiWen/DEPICT/"},{"id":"http://arxiv.org/abs/2406.03912v2","updated":"2025-01-14T10:32:32Z","published":"2024-06-06T09:51:30Z","title":"GenSafe: A Generalizable Safety Enhancer for Safe Reinforcement Learning\n  Algorithms Based on Reduced Order Markov Decision Process Model","summary":"  Safe Reinforcement Learning (SRL) aims to realize a safe learning process for\nDeep Reinforcement Learning (DRL) algorithms by incorporating safety\nconstraints. However, the efficacy of SRL approaches often relies on accurate\nfunction approximations, which are notably challenging to achieve in the early\nlearning stages due to data insufficiency. To address this issue, we introduce\nin this work a novel Generalizable Safety enhancer (GenSafe) that is able to\novercome the challenge of data insufficiency and enhance the performance of SRL\napproaches. Leveraging model order reduction techniques, we first propose an\ninnovative method to construct a Reduced Order Markov Decision Process (ROMDP)\nas a low-dimensional approximator of the original safety constraints. Then, by\nsolving the reformulated ROMDP-based constraints, GenSafe refines the actions\nof the agent to increase the possibility of constraint satisfaction.\nEssentially, GenSafe acts as an additional safety layer for SRL algorithms. We\nevaluate GenSafe on multiple SRL approaches and benchmark problems. The results\ndemonstrate its capability to improve safety performance, especially in the\nearly learning phases, while maintaining satisfactory task performance. Our\nproposed GenSafe not only offers a novel measure to augment existing SRL\nmethods but also shows broad compatibility with various SRL algorithms, making\nit applicable to a wide range of systems and SRL problems.\n","authors":["Zhehua Zhou","Xuan Xie","Jiayang Song","Zhan Shu","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2406.03912v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07985v1","updated":"2025-01-14T10:13:41Z","published":"2025-01-14T10:13:41Z","title":"CHEQ-ing the Box: Safe Variable Impedance Learning for Robotic Polishing","summary":"  Robotic systems are increasingly employed for industrial automation, with\ncontact-rich tasks like polishing requiring dexterity and compliant behaviour.\nThese tasks are difficult to model, making classical control challenging. Deep\nreinforcement learning (RL) offers a promising solution by enabling the\nlearning of models and control policies directly from data. However, its\napplication to real-world problems is limited by data inefficiency and unsafe\nexploration. Adaptive hybrid RL methods blend classical control and RL\nadaptively, combining the strengths of both: structure from control and\nlearning from RL. This has led to improvements in data efficiency and\nexploration safety. However, their potential for hardware applications remains\nunderexplored, with no evaluations on physical systems to date. Such\nevaluations are critical to fully assess the practicality and effectiveness of\nthese methods in real-world settings. This work presents an experimental\ndemonstration of the hybrid RL algorithm CHEQ for robotic polishing with\nvariable impedance, a task requiring precise force and velocity tracking. In\nsimulation, we show that variable impedance enhances polishing performance. We\ncompare standalone RL with adaptive hybrid RL, demonstrating that CHEQ achieves\neffective learning while adhering to safety constraints. On hardware, CHEQ\nachieves effective polishing behaviour, requiring only eight hours of training\nand incurring just five failures. These results highlight the potential of\nadaptive hybrid RL for real-world, contact-rich tasks trained directly on\nhardware.\n","authors":["Emma Cramer","Lukas Jäschke","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2501.07985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08558v2","updated":"2025-01-14T10:02:39Z","published":"2024-09-13T06:24:18Z","title":"Fair CoVariance Neural Networks","summary":"  Covariance-based data processing is widespread across signal processing and\nmachine learning applications due to its ability to model data\ninterconnectivities and dependencies. However, harmful biases in the data may\nbecome encoded in the sample covariance matrix and cause data-driven methods to\ntreat different subpopulations unfairly. Existing works such as fair principal\ncomponent analysis (PCA) mitigate these effects, but remain unstable in low\nsample regimes, which in turn may jeopardize the fairness goal. To address both\nbiases and instability, we propose Fair coVariance Neural Networks (FVNNs),\nwhich perform graph convolutions on the covariance matrix for both fair and\naccurate predictions. Our FVNNs provide a flexible model compatible with\nseveral existing bias mitigation techniques. In particular, FVNNs allow for\nmitigating the bias in two ways: first, they operate on fair covariance\nestimates that remove biases from their principal components; second, they are\ntrained in an end-to-end fashion via a fairness regularizer in the loss\nfunction so that the model parameters are tailored to solve the task directly\nin a fair manner. We prove that FVNNs are intrinsically fairer than analogous\nPCA approaches thanks to their stability in low sample regimes. We validate the\nrobustness and fairness of our model on synthetic and real-world data,\nshowcasing the flexibility of FVNNs along with the tradeoff between fair and\naccurate performance.\n","authors":["Andrea Cavallo","Madeline Navarro","Santiago Segarra","Elvin Isufi"],"pdf_url":"https://arxiv.org/pdf/2409.08558v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02931v2","updated":"2025-01-14T10:01:41Z","published":"2025-01-06T11:14:18Z","title":"Self-Attention as a Parametric Endofunctor: A Categorical Framework for\n  Transformer Architectures","summary":"  Self-attention mechanisms have revolutionised deep learning architectures,\nyet their core mathematical structures remain incompletely understood. In this\nwork, we develop a category-theoretic framework focusing on the linear\ncomponents of self-attention. Specifically, we show that the query, key, and\nvalue maps naturally define a parametric 1-morphism in the 2-category\n$\\mathbf{Para(Vect)}$. On the underlying 1-category $\\mathbf{Vect}$, these maps\ninduce an endofunctor whose iterated composition precisely models multi-layer\nattention. We further prove that stacking multiple self-attention layers\ncorresponds to constructing the free monad on this endofunctor. For positional\nencodings, we demonstrate that strictly additive embeddings correspond to\nmonoid actions in an affine sense, while standard sinusoidal encodings, though\nnot additive, retain a universal property among injective (faithful)\nposition-preserving maps. We also establish that the linear portions of\nself-attention exhibit natural equivariance to permutations of input tokens,\nand show how the \"circuits\" identified in mechanistic interpretability can be\ninterpreted as compositions of parametric 1-morphisms. This categorical\nperspective unifies geometric, algebraic, and interpretability-based approaches\nto transformer analysis, making explicit the underlying structures of\nattention. We restrict to linear maps throughout, deferring the treatment of\nnonlinearities such as softmax and layer normalisation, which require more\nadvanced categorical constructions. Our results build on and extend recent work\non category-theoretic foundations for deep learning, offering deeper insights\ninto the algebraic structure of attention mechanisms.\n","authors":["Charles O'Neill"],"pdf_url":"https://arxiv.org/pdf/2501.02931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21151v2","updated":"2025-01-14T09:58:24Z","published":"2024-07-30T19:28:28Z","title":"Private Collaborative Edge Inference via Over-the-Air Computation","summary":"  We consider collaborative inference at the wireless edge, where each client's\nmodel is trained independently on its local dataset. Clients are queried in\nparallel to make an accurate decision collaboratively. In addition to\nmaximizing the inference accuracy, we also want to ensure the privacy of local\nmodels. To this end, we leverage the superposition property of the multiple\naccess channel to implement bandwidth-efficient multi-user inference methods.\nWe propose different methods for ensemble and multi-view classification that\nexploit over-the-air computation (OAC). We show that these schemes perform\nbetter than their orthogonal counterparts with statistically significant\ndifferences while using fewer resources and providing privacy guarantees. We\nalso provide experimental results verifying the benefits of the proposed OAC\napproach to multi-user inference, and perform an ablation study to demonstrate\nthe effectiveness of our design choices. We share the source code of the\nframework publicly on Github to facilitate further research and\nreproducibility.\n","authors":["Selim F. Yilmaz","Burak Hasircioglu","Li Qiao","Deniz Gunduz"],"pdf_url":"https://arxiv.org/pdf/2407.21151v2.pdf","comment":"17 pages, 8 figures. This work extends from our preliminary study\n  presented at the 2022 IEEE International Symposium on Information Theory [1].\n  arXiv admin note: text overlap with arXiv:2202.03129"},{"id":"http://arxiv.org/abs/2410.11005v2","updated":"2025-01-14T09:52:50Z","published":"2024-10-14T18:44:23Z","title":"One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of\n  Large Language Models in Reasoning Tasks","summary":"  Language is not monolithic. While benchmarks, including those designed for\nmultiple languages, are often used as proxies to evaluate the performance of\nLarge Language Models (LLMs), they tend to overlook the nuances of\nwithin-language variation, and thus fail to model the experience of speakers of\nnon-standard dialects. Focusing on African American Vernacular English (AAVE),\nwe present the first study aimed at objectively assessing the fairness and\nrobustness of LLMs in handling dialects in canonical reasoning tasks, including\nalgorithm, math, logic, and integrated reasoning. We introduce \\textbf{ReDial}\n(\\textbf{Re}asoning with \\textbf{Dial}ect Queries), a benchmark containing\n1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE\nspeakers, including experts with computer science backgrounds, to rewrite seven\npopular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate\nwidely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model\nfamilies. Our findings reveal that \\textbf{almost all of these widely used\nmodels show significant brittleness and unfairness to queries in AAVE}. Our\nwork establishes a systematic and objective framework for analyzing LLM bias in\ndialectal queries. Moreover, it highlights how mainstream LLMs provide unfair\nservice to dialect speakers in reasoning tasks, laying a critical foundation\nfor relevant future research. Code and data can be accessed at\nhttps://github.com/fangru-lin/redial_dialect_robustness_fairness.\n","authors":["Fangru Lin","Shaoguang Mao","Emanuele La Malfa","Valentin Hofmann","Adrian de Wynter","Xun Wang","Si-Qing Chen","Michael Wooldridge","Janet B. Pierrehumbert","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.11005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07964v1","updated":"2025-01-14T09:35:49Z","published":"2025-01-14T09:35:49Z","title":"Derivation of Output Correlation Inferences for Multi-Output (aka\n  Multi-Task) Gaussian Process","summary":"  Gaussian process (GP) is arguably one of the most widely used machine\nlearning algorithms in practice. One of its prominent applications is Bayesian\noptimization (BO). Although the vanilla GP itself is already a powerful tool\nfor BO, it is often beneficial to be able to consider the dependencies of\nmultiple outputs. To do so, Multi-task GP (MTGP) is formulated, but it is not\ntrivial to fully understand the derivations of its formulations and their\ngradients from the previous literature. This paper serves friendly derivations\nof the MTGP formulations and their gradients.\n","authors":["Shuhei Watanabe"],"pdf_url":"https://arxiv.org/pdf/2501.07964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07957v1","updated":"2025-01-14T09:21:17Z","published":"2025-01-14T09:21:17Z","title":"AI Guide Dog: Egocentric Path Prediction on Smartphone","summary":"  This paper introduces AI Guide Dog (AIGD), a lightweight egocentric\nnavigation assistance system for visually impaired individuals, designed for\nreal-time deployment on smartphones. AIGD addresses key challenges in blind\nnavigation by employing a vision-only, multi-label classification approach to\npredict directional commands, ensuring safe traversal across diverse\nenvironments. We propose a novel technique to enable goal-based outdoor\nnavigation by integrating GPS signals and high-level directions, while also\naddressing uncertain multi-path predictions for destination-free indoor\nnavigation. Our generalized model is the first navigation assistance system to\nhandle both goal-oriented and exploratory navigation scenarios across indoor\nand outdoor settings, establishing a new state-of-the-art in blind navigation.\nWe present methods, datasets, evaluations, and deployment insights to encourage\nfurther innovations in assistive navigation systems.\n","authors":["Aishwarya Jadhav","Jeffery Cao","Abhishree Shetty","Urvashi Priyam Kumar","Aditi Sharma","Ben Sukboontip","Jayant Sravan Tamarapalli","Jingyi Zhang","Anirudh Koul"],"pdf_url":"https://arxiv.org/pdf/2501.07957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07446v2","updated":"2025-01-14T09:17:26Z","published":"2025-01-13T16:16:53Z","title":"Synthesis and Analysis of Data as Probability Measures with\n  Entropy-Regularized Optimal Transport","summary":"  We consider synthesis and analysis of probability measures using the\nentropy-regularized Wasserstein-2 cost and its unbiased version, the Sinkhorn\ndivergence. The synthesis problem consists of computing the barycenter, with\nrespect to these costs, of $m$ reference measures given a set of coefficients\nbelonging to the $m$-dimensional simplex. The analysis problem consists of\nfinding the coefficients for the closest barycenter in the Wasserstein-2\ndistance to a given measure $\\mu$. Under the weakest assumptions on the\nmeasures thus far in the literature, we compute the derivative of the\nentropy-regularized Wasserstein-2 cost. We leverage this to establish a\ncharacterization of regularized barycenters as solutions to a fixed-point\nequation for the average of the entropic maps from the barycenter to the\nreference measures. This characterization yields a finite-dimensional, convex,\nquadratic program for solving the analysis problem when $\\mu$ is a barycenter.\nIt is shown that these coordinates, as well as the value of the barycenter\nfunctional, can be estimated from samples with dimension-independent rates of\nconvergence, a hallmark of entropy-regularized optimal transport, and we verify\nthese rates experimentally. We also establish that barycentric coordinates are\nstable with respect to perturbations in the Wasserstein-2 metric, suggesting a\nrobustness of these coefficients to corruptions. We employ the barycentric\ncoefficients as features for classification of corrupted point cloud data, and\nshow that compared to neural network baselines, our approach is more efficient\nin small training data regimes.\n","authors":["Brendan Mallery","James M. Murphy","Shuchin Aeron"],"pdf_url":"https://arxiv.org/pdf/2501.07446v2.pdf","comment":"58 pages. Code to reproduce experiments:\n  https://github.com/brendanmallery9/Entropic-Barycenters"},{"id":"http://arxiv.org/abs/2501.00709v2","updated":"2025-01-14T09:05:54Z","published":"2025-01-01T03:12:18Z","title":"KAN KAN Buff Signed Graph Neural Networks?","summary":"  Graph Representation Learning focuses on creating embeddings for nodes and\nedges that capture their features and connections. Graph Neural Networks (GNNs)\nuse neural networks to model complex graph relationships. The Kolmogorov-Arnold\nNeural Network (KAN) has recently emerged as an alternative to the Multi-Layer\nPerceptron (MLP), offering better accuracy and interpretability with fewer\nparameters. KANs have been applied to GNN tasks. This paper introduces the\nintegration of KANs into Signed Graph Convolutional Networks (SGCNs). We\nevaluate KAN-enhanced SGCNs (KASGCN) on signed community detection and link\nsign prediction tasks to improve embedding quality in signed networks. While\nthe results show some variability, KASGCN performs competitively with or\nsimilarly to the standard SGCN in the functions tested. Its effectiveness\ndepends on the specific context, such as the signed graph and parameter\nsettings.\n","authors":["Muhieddine Shebaro","Jelena Tešić"],"pdf_url":"https://arxiv.org/pdf/2501.00709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07317v2","updated":"2025-01-14T09:00:27Z","published":"2025-01-13T13:28:03Z","title":"Evaluation of Artificial Intelligence Methods for Lead Time Prediction\n  in Non-Cycled Areas of Automotive Production","summary":"  The present study examines the effectiveness of applying Artificial\nIntelligence methods in an automotive production environment to predict unknown\nlead times in a non-cycle-controlled production area. Data structures are\nanalyzed to identify contextual features and then preprocessed using one-hot\nencoding. Methods selection focuses on supervised machine learning techniques.\nIn supervised learning methods, regression and classification methods are\nevaluated. Continuous regression based on target size distribution is not\nfeasible. Classification methods analysis shows that Ensemble Learning and\nSupport Vector Machines are the most suitable. Preliminary study results\nindicate that gradient boosting algorithms LightGBM, XGBoost, and CatBoost\nyield the best results. After further testing and extensive hyperparameter\noptimization, the final method choice is the LightGBM algorithm. Depending on\nfeature availability and prediction interval granularity, relative prediction\naccuracies of up to 90% can be achieved. Further tests highlight the importance\nof periodic retraining of AI models to accurately represent complex production\nprocesses using the database. The research demonstrates that AI methods can be\neffectively applied to highly variable production data, adding business value\nby providing an additional metric for various control tasks while outperforming\ncurrent non AI-based systems.\n","authors":["Cornelius Hake","Jonas Weigele","Frederik Reichert","Christian Friedrich"],"pdf_url":"https://arxiv.org/pdf/2501.07317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14961v3","updated":"2025-01-14T08:56:48Z","published":"2024-01-26T15:52:41Z","title":"Set-Based Training for Neural Network Verification","summary":"  Neural networks are vulnerable to adversarial attacks, i.e., small input\nperturbations can significantly affect the outputs of a neural network.\nTherefore, to ensure safety of safety-critical environments, the robustness of\na neural network must be formally verified against input perturbations, e.g.,\nfrom noisy sensors. To improve the robustness of neural networks and thus\nsimplify the formal verification, we present a novel set-based training\nprocedure in which we compute the set of possible outputs given the set of\npossible inputs and compute for the first time a gradient set, i.e., each\npossible output has a different gradient. Therefore, we can directly reduce the\nsize of the output enclosure by choosing gradients toward its center. Small\noutput enclosures increase the robustness of a neural network and, at the same\ntime, simplify its formal verification. The latter benefit is due to the fact\nthat a larger size of propagated sets increases the conservatism of most\nverification methods. Our extensive evaluation demonstrates that set-based\ntraining produces robust neural networks with competitive performance, which\ncan be verified using fast (polynomial-time) verification algorithms due to the\nreduced output set.\n","authors":["Lukas Koller","Tobias Ladner","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2401.14961v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13874v4","updated":"2025-01-14T08:42:23Z","published":"2024-10-02T13:02:17Z","title":"COOL: Efficient and Reliable Chain-Oriented Objective Logic with Neural\n  Networks Feedback Control for Program Synthesis","summary":"  Program synthesis methods, whether formal or neural-based, lack fine-grained\ncontrol and flexible modularity, which limits their adaptation to complex\nsoftware development. These limitations stem from rigid Domain-Specific\nLanguage (DSL) frameworks and neural network incorrect predictions. To this\nend, we propose the Chain of Logic (CoL), which organizes the synthesis process\ninto an activity flow and provides heuristic control to guide the process.\nFurthermore, by integrating neural networks with libraries and introducing a\nNeural Network Feedback Control (NNFC) mechanism, our approach modularizes\nsynthesis and mitigates the impact of neural network mispredictions.\nExperiments on relational and symbolic synthesis tasks show that CoL\nsignificantly enhances the efficiency and reliability of DSL program synthesis\nacross multiple metrics. Specifically, CoL improves accuracy by 70% while\nreducing tree operations by 91% and time by 95%. Additionally, NNFC further\nboosts accuracy by 6%, with a 64% reduction in tree operations under\nchallenging conditions such as insufficient training data, increased\ndifficulty, and multidomain synthesis. These improvements confirm COOL as a\nhighly efficient and reliable program synthesis framework.\n","authors":["Jipeng Han"],"pdf_url":"https://arxiv.org/pdf/2410.13874v4.pdf","comment":"31 pages, 11 figures"},{"id":"http://arxiv.org/abs/2501.07927v1","updated":"2025-01-14T08:30:49Z","published":"2025-01-14T08:30:49Z","title":"Gandalf the Red: Adaptive Security for LLMs","summary":"  Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and rigorously expresses the\nsecurity-utility in an optimizable form. We further address the shortcomings in\nexisting evaluations by introducing Gandalf, a crowd-sourced, gamified\nred-teaming platform designed to generate realistic, adaptive attack datasets.\nUsing Gandalf, we collect and release a dataset of 279k prompt attacks.\nComplemented by benign user data, our analysis reveals the interplay between\nsecurity and utility, showing that defenses integrated in the LLM (e.g., system\nprompts) can degrade usability even without blocking requests. We demonstrate\nthat restricted application domains, defense-in-depth, and adaptive defenses\nare effective strategies for building secure and useful LLM applications. Code\nis available at\n\\href{https://github.com/lakeraai/dsec-gandalf}{\\texttt{https://github.com/lakeraai/dsec-gandalf}}.\n","authors":["Niklas Pfister","Václav Volhejn","Manuel Knott","Santiago Arias","Julia Bazińska","Mykhailo Bichurin","Alan Commike","Janet Darling","Peter Dienes","Matthew Fiedler","David Haber","Matthias Kraft","Marco Lancini","Max Mathys","Damián Pascual-Ortiz","Jakub Podolak","Adrià Romero-López","Kyriacos Shiarlis","Andreas Signer","Zsolt Terek","Athanasios Theocharis","Daniel Timbrell","Samuel Trautwein","Samuel Watts","Natalie Wu","Mateo Rojas-Carulla"],"pdf_url":"https://arxiv.org/pdf/2501.07927v1.pdf","comment":"Niklas Pfister, V\\'aclav Volhejn and Manuel Knott contributed equally"},{"id":"http://arxiv.org/abs/2501.07925v1","updated":"2025-01-14T08:26:58Z","published":"2025-01-14T08:26:58Z","title":"Phase of Flight Classification in Aviation Safety using LSTM, GRU, and\n  BiLSTM: A Case Study with ASN Dataset","summary":"  Safety is the main concern in the aviation industry, where even minor\noperational issues can lead to serious consequences. This study addresses the\nneed for comprehensive aviation accident analysis by leveraging natural\nlanguage processing (NLP) and advanced AI models to classify the phase of\nflight from unstructured aviation accident analysis narratives. The research\naims to determine whether the phase of flight can be inferred from narratives\nof post-accident events using NLP techniques. The classification performance of\nvarious deep learning models was evaluated. For single RNN-based models, LSTM\nachieved an accuracy of 63%, precision 60%, and recall 61%. BiLSTM recorded an\naccuracy of 64%, precision 63%, and a recall of 64%. GRU exhibited balanced\nperformance with an accuracy and recall of 60% and a precision of 63%. Joint\nRNN-based models further enhanced predictive capabilities. GRU-LSTM,\nLSTM-BiLSTM, and GRU-BiLSTM demonstrated accuracy rates of 62%, 67%, and 60%,\nrespectively, showcasing the benefits of combining these architectures. To\nprovide a comprehensive overview of model performance, single and combined\nmodels were compared in terms of the various metrics. These results underscore\nthe models' capacity to classify the phase of flight from raw text narratives,\nequipping aviation industry stakeholders with valuable insights for proactive\ndecision-making. Therefore, this research signifies a substantial advancement\nin the application of NLP and deep learning models to enhance aviation safety.\n","authors":["Aziida Nanyonga","Hassan Wasswa","Graham Wild"],"pdf_url":"https://arxiv.org/pdf/2501.07925v1.pdf","comment":"Aviation Safety, Deep learning algorithms, Flight phase, NLP, ASN,\n  and Classification"},{"id":"http://arxiv.org/abs/2501.07923v1","updated":"2025-01-14T08:18:41Z","published":"2025-01-14T08:18:41Z","title":"Aviation Safety Enhancement via NLP & Deep Learning: Classifying Flight\n  Phases in ATSB Safety Reports","summary":"  Aviation safety is paramount, demanding precise analysis of safety\noccurrences during different flight phases. This study employs Natural Language\nProcessing (NLP) and Deep Learning models, including LSTM, CNN, Bidirectional\nLSTM (BLSTM), and simple Recurrent Neural Networks (sRNN), to classify flight\nphases in safety reports from the Australian Transport Safety Bureau (ATSB).\nThe models exhibited high accuracy, precision, recall, and F1 scores, with LSTM\nachieving the highest performance of 87%, 88%, 87%, and 88%, respectively. This\nperformance highlights their effectiveness in automating safety occurrence\nanalysis. The integration of NLP and Deep Learning technologies promises\ntransformative enhancements in aviation safety analysis, enabling targeted\nsafety measures and streamlined report handling.\n","authors":["Aziida Nanyonga","Hassan Wasswa","Graham Wild"],"pdf_url":"https://arxiv.org/pdf/2501.07923v1.pdf","comment":"NLP, Aviation Safety, ATSB, Deep learning, Flight phase. arXiv admin\n  note: substantial text overlap with arXiv:2501.01694"},{"id":"http://arxiv.org/abs/2403.10568v3","updated":"2025-01-14T08:01:17Z","published":"2024-03-14T17:47:10Z","title":"MoPE: Mixture of Prompt Experts for Parameter-Efficient and Scalable\n  Multimodal Fusion","summary":"  Despite the demonstrated parameter efficiency of prompt-based multimodal\nfusion methods, their limited adaptivity and expressiveness often result in\nsuboptimal performance compared to other tuning approaches. In this paper, we\nintroduce the Mixture of Prompt Experts (MoPE), the first technique designed to\novercome these limitations by decomposing standard prompts to capture\ninstance-level features adaptively. Building on this decomposition, MoPE\nenhances prompt fusion's expressiveness by leveraging multimodal pairing priors\nto route the most effective prompt for each instance dynamically. Compared to\nvanilla prompting, our MoPE-based fusion method exhibits greater\nexpressiveness, scaling more effectively with the training data and the overall\nnumber of trainable parameters. We also investigate regularization terms for\nexpert routing, which lead to emergent expert specialization with enhanced\nadaptiveness and interpretablity. Extensive experiments across six multimodal\ndatasets spanning four modalities demonstrate state-of-the-art performance for\nprompt fusion, matching or even surpassing the performance of fine-tuning while\nrequiring only 0.8% of the trainable parameters. Project homepage:\nhttps://github.com/songrise/MoPE\n","authors":["Ruixiang Jiang","Lingbo Liu","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.10568v3.pdf","comment":"Under Review, Extended version of arxiv:2312.03734"},{"id":"http://arxiv.org/abs/2501.07905v1","updated":"2025-01-14T07:50:09Z","published":"2025-01-14T07:50:09Z","title":"Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence\n  Modeling for Resource-Constrained Environments","summary":"  Long-range sequence modeling is a crucial aspect of natural language\nprocessing and time series analysis. However, traditional models like Recurrent\nNeural Networks (RNNs) and Transformers suffer from computational and memory\ninefficiencies, especially when dealing with long sequences. This paper\nintroduces Logarithmic Memory Networks (LMNs), a novel architecture that\nleverages a hierarchical logarithmic tree structure to efficiently store and\nretrieve past information. LMNs dynamically summarize historical context,\nsignificantly reducing the memory footprint and computational complexity of\nattention mechanisms from O(n2) to O(log(n)). The model employs a\nsingle-vector, targeted attention mechanism to access stored information, and\nthe memory block construction worker (summarizer) layer operates in two modes:\na parallel execution mode during training for efficient processing of\nhierarchical tree structures and a sequential execution mode during inference,\nwhich acts as a memory management system. It also implicitly encodes positional\ninformation, eliminating the need for explicit positional encodings. These\nfeatures make LMNs a robust and scalable solution for processing long-range\nsequences in resource-constrained environments, offering practical improvements\nin efficiency and scalability. The code is publicly available under the MIT\nLicense on GitHub: https://github.com/AhmedBoin/LogarithmicMemory.\n","authors":["Mohamed A. Taha"],"pdf_url":"https://arxiv.org/pdf/2501.07905v1.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2501.07903v1","updated":"2025-01-14T07:46:33Z","published":"2025-01-14T07:46:33Z","title":"Optimal Classification Trees for Continuous Feature Data Using Dynamic\n  Programming with Branch-and-Bound","summary":"  Computing an optimal classification tree that provably maximizes training\nperformance within a given size limit, is NP-hard, and in practice, most\nstate-of-the-art methods do not scale beyond computing optimal trees of depth\nthree. Therefore, most methods rely on a coarse binarization of continuous\nfeatures to maintain scalability. We propose a novel algorithm that optimizes\ntrees directly on the continuous feature data using dynamic programming with\nbranch-and-bound. We develop new pruning techniques that eliminate many\nsub-optimal splits in the search when similar to previously computed splits and\nwe provide an efficient subroutine for computing optimal depth-two trees. Our\nexperiments demonstrate that these techniques improve runtime by one or more\norders of magnitude over state-of-the-art optimal methods and improve test\naccuracy by 5% over greedy heuristics.\n","authors":["Catalin E. Brita","Jacobus G. M. van der Linden","Emir Demirović"],"pdf_url":"https://arxiv.org/pdf/2501.07903v1.pdf","comment":"In the proceedings of AAAI-25"},{"id":"http://arxiv.org/abs/2411.02824v2","updated":"2025-01-14T07:30:20Z","published":"2024-11-05T05:50:51Z","title":"Layer-Adaptive State Pruning for Deep State Space Models","summary":"  Due to the lack of state dimension optimization methods, deep state space\nmodels (SSMs) have sacrificed model capacity, training search space, or\nstability to alleviate computational costs caused by high state dimensions. In\nthis work, we provide a structured pruning method for SSMs, Layer-Adaptive\nSTate pruning (LAST), which reduces the state dimension of each layer in\nminimizing model-level output energy loss by extending modal truncation for a\nsingle system. LAST scores are evaluated using the $\\mathcal{H}_{\\infty}$ norms\nof subsystems and layer-wise energy normalization. The scores serve as global\npruning criteria, enabling cross-layer comparison of states and layer-adaptive\npruning. Across various sequence benchmarks, LAST optimizes previous SSMs,\nrevealing the redundancy and compressibility of their state spaces. Notably, we\ndemonstrate that, on average, pruning 33% of states still maintains performance\nwith 0.52% accuracy loss in multi-input multi-output SSMs without retraining.\nCode is available at https://github.com/msgwak/LAST.\n","authors":["Minseon Gwak","Seongrok Moon","Joohwan Ko","PooGyeon Park"],"pdf_url":"https://arxiv.org/pdf/2411.02824v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2407.07368v2","updated":"2025-01-14T07:28:06Z","published":"2024-07-10T05:03:48Z","title":"Data-driven Bayesian State Estimation with Compressed Measurement of\n  Model-free Process using Semi-supervised Learning","summary":"  The research topic is: data-driven Bayesian state estimation with compressed\nmeasurement (BSCM) of model-free process, say for a (causal) tracking\napplication. The dimension of the temporal measurement vector is lower than the\ndimension of the temporal state vector to be estimated. Hence the state\nestimation problem is an underdetermined inverse problem. The underlying\ndynamical model of the states is assumed to be unknown and hence, we use the\nterminology 'model-free process'. In absence of the dynamical model, we can not\nemploy traditional model-driven methods like Kalman Filter (KF) and Particle\nFilter (PF), and instead require data-driven methods. We first experimentally\nshow that two existing unsupervised learning-based data-driven methods fail to\naddress the BSCM problem for model-free process; they are - data-driven\nnonlinear state estimation (DANSE) method and deep Markov model (DMM) method.\nThe unsupervised learning uses unlabelled data comprised of only noisy, linear\nmeasurements. While DANSE provides a good predictive / forecasting performance\nto model the temporal measurement data as time-series, its unsupervised\nlearning lacks a regularization for state estimation. We then investigate the\nuse of a semi-supervised learning approach, and develop a semi-supervised\nlearning-based DANSE method, referred to as SemiDANSE. In SemiDANSE, we use a\nlimited amount of labelled data along-with a large amount of unlabelled data,\nand that helps to bring the desired regularization for addressing the BSCM\nproblem. The labelled data means pairwise measurement-and-state data. Using\nthree chaotic dynamical systems (or processes) with nonlinear dynamical models\nas benchmark, we show that the data-driven SemiDANSE provides competitive\nperformance for BSCM against a hybrid method called KalmanNet and two\nmodel-driven methods -- an extended KF (EKF) and an unscented KF (UKF).\n","authors":["Anubhab Ghosh","Yonina C. Eldar","Saikat Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2407.07368v2.pdf","comment":"14 pages, under review at IEEE TSP"},{"id":"http://arxiv.org/abs/2410.15322v2","updated":"2025-01-14T06:59:12Z","published":"2024-10-20T07:32:16Z","title":"FoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion\n  Model","summary":"  Mobile traffic forecasting allows operators to anticipate network dynamics\nand performance in advance, offering substantial potential for enhancing\nservice quality and improving user experience. However, existing models are\noften task-oriented and are trained with tailored data, which limits their\neffectiveness in diverse mobile network tasks of Base Station (BS) deployment,\nresource allocation, energy optimization, etc. and hinders generalization\nacross different urban environments. Foundation models have made remarkable\nstrides across various domains of NLP and CV due to their multi-tasking\nadaption and zero/few-shot learning capabilities. In this paper, we propose an\ninnovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to\nhandle diverse forecasting tasks of short/long-term predictions and\ndistribution generation across multiple cities to support network planning and\noptimization. FoMo combines diffusion models and transformers, where various\nspatio-temporal masks are proposed to enable FoMo to learn intrinsic features\nof different tasks, and a contrastive learning strategy is developed to capture\nthe correlations between mobile traffic and urban contexts, thereby improving\nits transfer learning capability. Extensive experiments on 9 real-world\ndatasets demonstrate that FoMo outperforms current models concerning diverse\nforecasting tasks and zero/few-shot learning, showcasing a strong universality.\n","authors":["Haoye Chai","Xiaoqian Qi","Shiyuan Zhang","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.15322v2.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.07886v1","updated":"2025-01-14T06:54:17Z","published":"2025-01-14T06:54:17Z","title":"Iterative Label Refinement Matters More than Preference Optimization\n  under Weak Supervision","summary":"  Language model (LM) post-training relies on two stages of human supervision:\ntask demonstrations for supervised finetuning (SFT), followed by preference\ncomparisons for reinforcement learning from human feedback (RLHF). As LMs\nbecome more capable, the tasks they are given become harder to supervise. Will\npost-training remain effective under unreliable supervision? To test this, we\nsimulate unreliable demonstrations and comparison feedback using small LMs and\ntime-constrained humans. We find that in the presence of unreliable\nsupervision, SFT still retains some effectiveness, but DPO (a common RLHF\nalgorithm) fails to improve the model beyond SFT. To address this, we propose\niterative label refinement (ILR) as an alternative to RLHF. ILR improves the\nSFT data by using comparison feedback to decide whether human demonstrations\nshould be replaced by model-generated alternatives, then retrains the model via\nSFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with\nunreliable supervision (math, coding, and safe instruction-following). Our\nfindings suggest that as LMs are used for complex tasks where human supervision\nis unreliable, RLHF may no longer be the best use of human comparison feedback;\ninstead, it is better to direct feedback towards improving the training data\nrather than continually training the model. Our code and data are available at\nhttps://github.com/helloelwin/iterative-label-refinement.\n","authors":["Yaowen Ye","Cassidy Laidlaw","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2501.07886v1.pdf","comment":"22 pages, 10 figures"},{"id":"http://arxiv.org/abs/2501.07885v1","updated":"2025-01-14T06:51:27Z","published":"2025-01-14T06:51:27Z","title":"Mitigating Algorithmic Bias in Multiclass CNN Classifications Using\n  Causal Modeling","summary":"  This study describes a procedure for applying causal modeling to detect and\nmitigate algorithmic bias in a multiclass classification problem. The dataset\nwas derived from the FairFace dataset, supplemented with emotional labels\ngenerated by the DeepFace pre-trained model. A custom Convolutional Neural\nNetwork (CNN) was developed, consisting of four convolutional blocks, followed\nby fully connected layers and dropout layers to mitigate overfitting. Gender\nbias was identified in the CNN model's classifications: Females were more\nlikely to be classified as \"happy\" or \"sad,\" while males were more likely to be\nclassified as \"neutral.\" To address this, the one-vs-all (OvA) technique was\napplied. A causal model was constructed for each emotion class to adjust the\nCNN model's predicted class probabilities. The adjusted probabilities for the\nvarious classes were then aggregated by selecting the class with the highest\nprobability. The resulting debiased classifications demonstrated enhanced\ngender fairness across all classes, with negligible impact--or even a slight\nimprovement--on overall accuracy. This study highlights that algorithmic\nfairness and accuracy are not necessarily trade-offs. All data and code for\nthis study are publicly available for download.\n","authors":["Min Sik Byun","Wendy Wan Yee Hui","Wai Kwong Lau"],"pdf_url":"https://arxiv.org/pdf/2501.07885v1.pdf","comment":"7 pages; 6 figures"},{"id":"http://arxiv.org/abs/2501.07884v1","updated":"2025-01-14T06:50:56Z","published":"2025-01-14T06:50:56Z","title":"MD-Syn: Synergistic drug combination prediction based on the\n  multidimensional feature fusion method and attention mechanisms","summary":"  Drug combination therapies have shown promising therapeutic efficacy in\ncomplex diseases and have demonstrated the potential to reduce drug resistance.\nHowever, the huge number of possible drug combinations makes it difficult to\nscreen them all in traditional experiments. In this study, we proposed MD-Syn,\na computational framework, which is based on the multidimensional feature\nfusion method and multi-head attention mechanisms. Given drug pair-cell line\ntriplets, MD-Syn considers one-dimensional and two-dimensional feature spaces\nsimultaneously. It consists of a one-dimensional feature embedding module\n(1D-FEM), a two-dimensional feature embedding module (2D-FEM), and a deep\nneural network-based classifier for synergistic drug combination prediction.\nMD-Syn achieved the AUROC of 0.919 in 5-fold cross-validation, outperforming\nthe state-of-the-art methods. Further, MD-Syn showed comparable results over\ntwo independent datasets. In addition, the multi-head attention mechanisms not\nonly learn embeddings from different feature aspects but also focus on\nessential interactive feature elements, improving the interpretability of\nMD-Syn. In summary, MD-Syn is an interpretable framework to prioritize\nsynergistic drug combination pairs with chemicals and cancer cell line gene\nexpression profiles. To facilitate broader community access to this model, we\nhave developed a web portal (https://labyeh104-2.life.nthu.edu.tw/) that\nenables customized predictions of drug combination synergy effects based on\nuser-specified compounds.\n","authors":["XinXin Ge","Yi-Ting Lee","Shan-Ju Yeh"],"pdf_url":"https://arxiv.org/pdf/2501.07884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04539v4","updated":"2025-01-14T06:42:51Z","published":"2023-10-06T19:06:13Z","title":"Generating Less Certain Adversarial Examples Improves Robust\n  Generalization","summary":"  This paper revisits the robust overfitting phenomenon of adversarial\ntraining. Observing that models with better robust generalization performance\nare less certain in predicting adversarially generated training inputs, we\nargue that overconfidence in predicting adversarial examples is a potential\ncause. Therefore, we hypothesize that generating less certain adversarial\nexamples improves robust generalization, and propose a formal definition of\nadversarial certainty that captures the variance of the model's predicted\nlogits on adversarial examples. Our theoretical analysis of synthetic\ndistributions characterizes the connection between adversarial certainty and\nrobust generalization. Accordingly, built upon the notion of adversarial\ncertainty, we develop a general method to search for models that can generate\ntraining-time adversarial inputs with reduced certainty, while maintaining the\nmodel's capability in distinguishing adversarial examples. Extensive\nexperiments on image benchmarks demonstrate that our method effectively learns\nmodels with consistently improved robustness and mitigates robust overfitting,\nconfirming the importance of generating less certain adversarial examples for\nrobust generalization. Our implementations are available as open-source code\nat: https://github.com/TrustMLRG/AdvCertainty.\n","authors":["Minxing Zhang","Michael Backes","Xiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.04539v4.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2501.07879v1","updated":"2025-01-14T06:41:55Z","published":"2025-01-14T06:41:55Z","title":"Distributed Nonparametric Estimation: from Sparse to Dense Samples per\n  Terminal","summary":"  Consider the communication-constrained problem of nonparametric function\nestimation, in which each distributed terminal holds multiple i.i.d. samples.\nUnder certain regularity assumptions, we characterize the minimax optimal rates\nfor all regimes, and identify phase transitions of the optimal rates as the\nsamples per terminal vary from sparse to dense. This fully solves the problem\nleft open by previous works, whose scopes are limited to regimes with either\ndense samples or a single sample per terminal. To achieve the optimal rates, we\ndesign a layered estimation protocol by exploiting protocols for the parametric\ndensity estimation problem. We show the optimality of the protocol using\ninformation-theoretic methods and strong data processing inequalities, and\nincorporating the classic balls and bins model. The optimal rates are immediate\nfor various special cases such as density estimation, Gaussian, binary, Poisson\nand heteroskedastic regression models.\n","authors":["Deheng Yuan","Tao Guo","Zhongyi Huang"],"pdf_url":"https://arxiv.org/pdf/2501.07879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07145v2","updated":"2025-01-14T06:38:10Z","published":"2025-01-13T09:11:13Z","title":"A User's Guide to $\\texttt{KSig}$: GPU-Accelerated Computation of the\n  Signature Kernel","summary":"  The signature kernel is a positive definite kernel for sequential and\ntemporal data that has become increasingly popular in machine learning\napplications due to powerful theoretical guarantees, strong empirical\nperformance, and recently introduced various scalable variations. In this\nchapter, we give a short introduction to $\\texttt{KSig}$, a\n$\\texttt{Scikit-Learn}$ compatible Python package that implements various\nGPU-accelerated algorithms for computing signature kernels, and performing\ndownstream learning tasks. We also introduce a new algorithm based on tensor\nsketches which gives strong performance compared to existing algorithms. The\npackage is available at https://github.com/tgcsaba/ksig.\n","authors":["Csaba Tóth","Danilo Jr Dela Cruz","Harald Oberhauser"],"pdf_url":"https://arxiv.org/pdf/2501.07145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23111v6","updated":"2025-01-14T06:25:54Z","published":"2024-10-30T15:23:44Z","title":"Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies.\n","authors":["Navyansh Mahla","Kshitij Sharad Jadhav","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2410.23111v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19982v2","updated":"2025-01-14T06:18:03Z","published":"2024-10-25T21:46:25Z","title":"Random Policy Enables In-Context Reinforcement Learning within Trust\n  Horizons","summary":"  Pretrained foundation models have exhibited extraordinary in-context learning\nperformance, allowing zero-shot generalization to new tasks not encountered\nduring pretraining. In the case of reinforcement learning (RL), in-context RL\n(ICRL) emerges when pretraining FMs on decision-making problems in an\nautoregressive-supervised manner. Nevertheless, current state-of-the-art ICRL\nalgorithms, like Algorithm Distillation, Decision Pretrained Transformer and\nDecision Importance Transformer, impose stringent requirements on the\npretraining dataset concerning the source policies, context information, and\naction labels. Notably, these algorithms either demand optimal policies or\nrequire varying degrees of well-trained behavior policies for all pretraining\nenvironments. This significantly hinders the application of ICRL to real-world\nscenarios, where acquiring optimal or well-trained policies for a substantial\nvolume of real-world training environments can be intractable. To overcome this\nchallenge, we introduce a novel approach, termed State-Action Distillation\n(SAD), that allows to generate an effective pretraining dataset guided solely\nby random policies. In particular, SAD selects query states and corresponding\naction labels by distilling outstanding state-action pairs from the entire\nstate and action spaces by using random policies within a trust horizon, and\nthen inherits the classical autoregressive-supervised mechanism during\npretraining. To the best of our knowledge, this is the first work that enables\neffective ICRL under random policies and random contexts. We also establish\nquantitative analysis of the trustworthiness as well as the performance\nguarantees of SAD. Moreover, our empirical results across multiple popular ICRL\nbenchmark environments demonstrate that, on average, SAD outperforms the best\nbaseline by 236.3% in the offline evaluation and by 135.2% in the online\nevaluation.\n","authors":["Weiqin Chen","Santiago Paternain"],"pdf_url":"https://arxiv.org/pdf/2410.19982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10703v2","updated":"2025-01-14T06:02:00Z","published":"2024-12-14T06:22:49Z","title":"Doubly-Bounded Queue for Constrained Online Learning: Keeping Pace with\n  Dynamics of Both Loss and Constraint","summary":"  We consider online convex optimization with time-varying constraints and\nconduct performance analysis using two stringent metrics: dynamic regret with\nrespect to the online solution benchmark, and hard constraint violation that\ndoes not allow any compensated violation over time. We propose an efficient\nalgorithm called Constrained Online Learning with Doubly-bounded Queue (COLDQ),\nwhich introduces a novel virtual queue that is both lower and upper bounded,\nallowing tight control of the constraint violation without the need for the\nSlater condition. We prove via a new Lyapunov drift analysis that COLDQ\nachieves $O(T^\\frac{1+V_x}{2})$ dynamic regret and $O(T^{V_g})$ hard constraint\nviolation, where $V_x$ and $V_g$ capture the dynamics of the loss and\nconstraint functions. For the first time, the two bounds smoothly approach to\nthe best-known $O(T^\\frac{1}{2})$ regret and $O(1)$ violation, as the dynamics\nof the losses and constraints diminish. For strongly convex loss functions,\nCOLDQ matches the best-known $O(\\log{T})$ static regret while maintaining the\n$O(T^{V_g})$ hard constraint violation. We further introduce an expert-tracking\nvariation of COLDQ, which achieves the same performance bounds without any\nprior knowledge of the system dynamics. Simulation results demonstrate that\nCOLDQ outperforms the state-of-the-art approaches.\n","authors":["Juncheng Wang","Bingjie Yan","Yituo Liu"],"pdf_url":"https://arxiv.org/pdf/2412.10703v2.pdf","comment":"To appear in AAAI 2025"},{"id":"http://arxiv.org/abs/2501.07859v1","updated":"2025-01-14T05:55:20Z","published":"2025-01-14T05:55:20Z","title":"deepTerra -- AI Land Classification Made Easy","summary":"  deepTerra is a comprehensive platform designed to facilitate the\nclassification of land surface features using machine learning and satellite\nimagery. The platform includes modules for data collection, image augmentation,\ntraining, testing, and prediction, streamlining the entire workflow for image\nclassification tasks. This paper presents a detailed overview of the\ncapabilities of deepTerra, shows how it has been applied to various research\nareas, and discusses the future directions it might take.\n","authors":["Andrew Keith Wilkinson"],"pdf_url":"https://arxiv.org/pdf/2501.07859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19255v2","updated":"2025-01-14T05:48:07Z","published":"2024-12-26T15:45:45Z","title":"Multi-matrix Factorization Attention","summary":"  We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.\n","authors":["Jingcheng Hu","Houyi Li","Yinmin Zhang","Zili Wang","Shuigeng Zhou","Xiangyu Zhang","Heung-Yeung Shum","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.19255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07855v1","updated":"2025-01-14T05:43:59Z","published":"2025-01-14T05:43:59Z","title":"State-of-the-Art Transformer Models for Image Super-Resolution:\n  Techniques, Challenges, and Applications","summary":"  Image Super-Resolution (SR) aims to recover a high-resolution image from its\nlow-resolution counterpart, which has been affected by a specific degradation\nprocess. This is achieved by enhancing detail and visual quality. Recent\nadvancements in transformer-based methods have remolded image super-resolution\nby enabling high-quality reconstructions surpassing previous deep-learning\napproaches like CNN and GAN-based. This effectively addresses the limitations\nof previous methods, such as limited receptive fields, poor global context\ncapture, and challenges in high-frequency detail recovery. Additionally, the\npaper reviews recent trends and advancements in transformer-based SR models,\nexploring various innovative techniques and architectures that combine\ntransformers with traditional networks to balance global and local contexts.\nThese neoteric methods are critically analyzed, revealing promising yet\nunexplored gaps and potential directions for future research. Several\nvisualizations of models and techniques are included to foster a holistic\nunderstanding of recent trends. This work seeks to offer a structured roadmap\nfor researchers at the forefront of deep learning, specifically exploring the\nimpact of transformers on super-resolution techniques.\n","authors":["Debasish Dutta","Deepjyoti Chetia","Neeharika Sonowal","Sanjib Kr Kalita"],"pdf_url":"https://arxiv.org/pdf/2501.07855v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2501.07850v1","updated":"2025-01-14T05:23:42Z","published":"2025-01-14T05:23:42Z","title":"An Intra- and Cross-frame Topological Consistency Scheme for\n  Semi-supervised Atherosclerotic Coronary Plaque Segmentation","summary":"  Enhancing the precision of segmenting coronary atherosclerotic plaques from\nCT Angiography (CTA) images is pivotal for advanced Coronary Atherosclerosis\nAnalysis (CAA), which distinctively relies on the analysis of vessel\ncross-section images reconstructed via Curved Planar Reformation. This task\npresents significant challenges due to the indistinct boundaries and structures\nof plaques and blood vessels, leading to the inadequate performance of current\ndeep learning models, compounded by the inherent difficulty in annotating such\ncomplex data. To address these issues, we propose a novel dual-consistency\nsemi-supervised framework that integrates Intra-frame Topological Consistency\n(ITC) and Cross-frame Topological Consistency (CTC) to leverage labeled and\nunlabeled data. ITC employs a dual-task network for simultaneous segmentation\nmask and Skeleton-aware Distance Transform (SDT) prediction, achieving similar\nprediction of topology structure through consistency constraint without\nadditional annotations. Meanwhile, CTC utilizes an unsupervised estimator for\nanalyzing pixel flow between skeletons and boundaries of adjacent frames,\nensuring spatial continuity. Experiments on two CTA datasets show that our\nmethod surpasses existing semi-supervised methods and approaches the\nperformance of supervised methods on CAA. In addition, our method also performs\nbetter than other methods on the ACDC dataset, demonstrating its\ngeneralization.\n","authors":["Ziheng Zhang","Zihan Li","Dandan Shan","Yuehui Qiu","Qingqi Hong","Qingqiang Wu"],"pdf_url":"https://arxiv.org/pdf/2501.07850v1.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2411.03865v4","updated":"2025-01-14T05:23:03Z","published":"2024-11-06T12:19:01Z","title":"AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making","summary":"  Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety.\n","authors":["Yizhe Huang","Xingbo Wang","Hao Liu","Fanqi Kong","Aoyang Qin","Min Tang","Song-Chun Zhu","Mingjie Bi","Siyuan Qi","Xue Feng"],"pdf_url":"https://arxiv.org/pdf/2411.03865v4.pdf","comment":"Accepted at NeurIPS D&B 2024"},{"id":"http://arxiv.org/abs/2405.10480v2","updated":"2025-01-14T05:00:34Z","published":"2024-05-17T00:52:39Z","title":"Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers","summary":"  Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.\n","authors":["Rya Sanovar","Srikant Bharadwaj","Renee St. Amant","Victor Rühle","Saravan Rajmohan"],"pdf_url":"https://arxiv.org/pdf/2405.10480v2.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2404.14389v2","updated":"2025-01-14T04:58:26Z","published":"2024-04-22T17:50:27Z","title":"Poisoning Attacks on Federated Learning-based Wireless Traffic\n  Prediction","summary":"  Federated Learning (FL) offers a distributed framework to train a global\ncontrol model across multiple base stations without compromising the privacy of\ntheir local network data. This makes it ideal for applications like wireless\ntraffic prediction (WTP), which plays a crucial role in optimizing network\nresources, enabling proactive traffic flow management, and enhancing the\nreliability of downstream communication-aided applications, such as IoT\ndevices, autonomous vehicles, and industrial automation systems. Despite its\npromise, the security aspects of FL-based distributed wireless systems,\nparticularly in regression-based WTP problems, remain inadequately\ninvestigated. In this paper, we introduce a novel fake traffic injection (FTI)\nattack, designed to undermine the FL-based WTP system by injecting fabricated\ntraffic distributions with minimal knowledge. We further propose a defense\nmechanism, termed global-local inconsistency detection (GLID), which\nstrategically removes abnormal model parameters that deviate beyond a specific\npercentile range estimated through statistical methods in each dimension.\nExtensive experimental evaluations, performed on real-world wireless traffic\ndatasets, demonstrate that both our attack and defense strategies significantly\noutperform existing baselines.\n","authors":["Zifan Zhang","Minghong Fang","Jiayuan Huang","Yuchen Liu"],"pdf_url":"https://arxiv.org/pdf/2404.14389v2.pdf","comment":"Accepted by IFIP/IEEE Networking 2024"},{"id":"http://arxiv.org/abs/2501.03461v2","updated":"2025-01-14T04:53:30Z","published":"2025-01-07T01:35:56Z","title":"Radar Signal Recognition through Self-Supervised Learning and Domain\n  Adaptation","summary":"  Automatic radar signal recognition (RSR) plays a pivotal role in electronic\nwarfare (EW), as accurately classifying radar signals is critical for informing\ndecision-making processes. Recent advances in deep learning have shown\nsignificant potential in improving RSR performance in domains with ample\nannotated data. However, these methods fall short in EW scenarios where\nannotated RF data are scarce or impractical to obtain. To address these\nchallenges, we introduce a self-supervised learning (SSL) method which utilises\nmasked signal modelling and RF domain adaption to enhance RSR performance in\nenvironments with limited RF samples and labels. Specifically, we investigate\npre-training masked autoencoders (MAE) on baseband in-phase and quadrature\n(I/Q) signals from various RF domains and subsequently transfer the learned\nrepresentation to the radar domain, where annotated data are limited. Empirical\nresults show that our lightweight self-supervised ResNet model with domain\nadaptation achieves up to a 17.5% improvement in 1-shot classification accuracy\nwhen pre-trained on in-domain signals (i.e., radar signals) and up to a 16.31%\nimprovement when pre-trained on out-of-domain signals (i.e., comm signals),\ncompared to its baseline without SSL. We also provide reference results for\nseveral MAE designs and pre-training strategies, establishing a new benchmark\nfor few-shot radar signal classification.\n","authors":["Zi Huang","Simon Denman","Akila Pemasiri","Clinton Fookes","Terrence Martin"],"pdf_url":"https://arxiv.org/pdf/2501.03461v2.pdf","comment":"5 pages, 9 figures"},{"id":"http://arxiv.org/abs/2501.06366v2","updated":"2025-01-14T04:42:08Z","published":"2025-01-10T22:27:44Z","title":"Counterfactually Fair Reinforcement Learning via Sequential Data\n  Preprocessing","summary":"  When applied in healthcare, reinforcement learning (RL) seeks to dynamically\nmatch the right interventions to subjects to maximize population benefit.\nHowever, the learned policy may disproportionately allocate efficacious actions\nto one subpopulation, creating or exacerbating disparities in other\nsocioeconomically-disadvantaged subgroups. These biases tend to occur in\nmulti-stage decision making and can be self-perpetuating, which if unaccounted\nfor could cause serious unintended consequences that limit access to care or\ntreatment benefit. Counterfactual fairness (CF) offers a promising statistical\ntool grounded in causal inference to formulate and study fairness. In this\npaper, we propose a general framework for fair sequential decision making. We\ntheoretically characterize the optimal CF policy and prove its stationarity,\nwhich greatly simplifies the search for optimal CF policies by leveraging\nexisting RL algorithms. The theory also motivates a sequential data\npreprocessing algorithm to achieve CF decision making under an additive noise\nassumption. We prove and then validate our policy learning approach in\ncontrolling unfairness and attaining optimal value through simulations.\nAnalysis of a digital health dataset designed to reduce opioid misuse shows\nthat our proposal greatly enhances fair access to counseling.\n","authors":["Jitao Wang","Chengchun Shi","John D. Piette","Joshua R. Loftus","Donglin Zeng","Zhenke Wu"],"pdf_url":"https://arxiv.org/pdf/2501.06366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07834v1","updated":"2025-01-14T04:35:37Z","published":"2025-01-14T04:35:37Z","title":"Flow: A Modular Approach to Automated Agentic Workflow Generation","summary":"  Multi-agent frameworks powered by large language models (LLMs) have\ndemonstrated great success in automated planning and task execution. However,\nthe effective adjustment of Agentic workflows during execution has not been\nwell-studied. A effective workflow adjustment is crucial, as in many real-world\nscenarios, the initial plan must adjust to unforeseen challenges and changing\nconditions in real-time to ensure the efficient execution of complex tasks. In\nthis paper, we define workflows as an activity-on-vertex (AOV) graphs. We\ncontinuously refine the workflow by dynamically adjusting task allocations\nbased on historical performance and previous AOV with LLM agents. To further\nenhance system performance, we emphasize modularity in workflow design based on\nmeasuring parallelism and dependence complexity. Our proposed multi-agent\nframework achieved efficient sub-task concurrent execution, goal achievement,\nand error tolerance. Empirical results across different practical tasks\ndemonstrate dramatic improvements in the efficiency of multi-agent frameworks\nthrough dynamic workflow updating and modularization.\n","authors":["Boye Niu","Yiliao Song","Kai Lian","Yifan Shen","Yu Yao","Kun Zhang","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.07834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06408v2","updated":"2025-01-14T04:30:31Z","published":"2025-01-11T02:23:08Z","title":"Computational and Statistical Asymptotic Analysis of the JKO Scheme for\n  Iterative Algorithms to update distributions","summary":"  The seminal paper of Jordan, Kinderlehrer, and Otto introduced what is now\nwidely known as the JKO scheme, an iterative algorithmic framework for\ncomputing distributions. This scheme can be interpreted as a Wasserstein\ngradient flow and has been successfully applied in machine learning contexts,\nsuch as deriving policy solutions in reinforcement learning. In this paper, we\nextend the JKO scheme to accommodate models with unknown parameters.\nSpecifically, we develop statistical methods to estimate these parameters and\nadapt the JKO scheme to incorporate the estimated values. To analyze the\nadopted statistical JKO scheme, we establish an asymptotic theory via\nstochastic partial differential equations that describes its limiting dynamic\nbehavior. Our framework allows both the sample size used in parameter\nestimation and the number of algorithmic iterations to go to infinity. This\nstudy offers a unified framework for joint computational and statistical\nasymptotic analysis of the statistical JKO scheme. On the computational side,\nwe examine the scheme's dynamic behavior as the number of iterations increases,\nwhile on the statistical side, we investigate the large-sample behavior of the\nresulting distributions computed through the scheme. We conduct numerical\nsimulations to evaluate the finite-sample performance of the proposed methods\nand validate the developed asymptotic theory.\n","authors":["Shang Wu","Yazhen Wang"],"pdf_url":"https://arxiv.org/pdf/2501.06408v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11869v3","updated":"2025-01-14T04:25:23Z","published":"2024-08-19T02:27:00Z","title":"ELDER: Enhancing Lifelong Model Editing with Mixture-of-LoRA","summary":"  Large language models (LLMs) require model editing to efficiently update\nspecific knowledge within them and avoid factual errors. Most model editing\nmethods are solely designed for single-time use and result in a significant\nforgetting effect in lifelong editing scenarios, where sequential edits are\nconducted over time. Previous approaches manage sequential edits by freezing\noriginal parameters and discretely allocating new parameters for each knowledge\nupdate. However, these methods lack robustness to minor input variations due to\nthe discrete mapping between data and parameters. To overcome this challenge,\nwe propose ELDER, a novel approach to create a continuous association between\ndata and adapters. ELDER integrates multiple LoRAs through a router network and\nis trained to establish a smooth data-adapter association, thereby enhancing\nthe edit robustness and generalization of semantically equivalent inputs. To\nensure inputs containing the same knowledge will be processed by the same\nLoRAs, we design a novel loss to guide the model link LoRA allocations with\nedit knowledge. Furthermore, we propose a deferral mechanism to retain the\noriginal LLM capabilities post-edit. Extensive experiments on GPT-2 XL and\nLLaMA2-7B demonstrate that ELDER effectively edits models in the lifelong\nsetting, outperforming eight baselines while exhibiting strong scalability and\npreserving LLMs' general abilities on downstream tasks. Our code is available\nat https://github.com/JiaangL/ELDER.\n","authors":["Jiaang Li","Quan Wang","Zhongnan Wang","Yongdong Zhang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2408.11869v3.pdf","comment":"Accepted by AAAI-25"},{"id":"http://arxiv.org/abs/2410.12476v2","updated":"2025-01-14T04:19:49Z","published":"2024-10-16T11:46:32Z","title":"Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation","summary":"  Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.\n","authors":["Zerui Xu","Fang Wu","Yuanyuan Zhang","Yue Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15240v3","updated":"2025-01-14T04:10:46Z","published":"2024-11-22T01:58:35Z","title":"AI Foundation Models for Wearable Movement Data in Mental Health\n  Research","summary":"  Pretrained foundation models and transformer architectures have driven the\nsuccess of large language models (LLMs) and other modern AI breakthroughs.\nHowever, similar advancements in health data modeling remain limited due to the\nneed for innovative adaptations. Wearable movement data offers a valuable\navenue for exploration, as it's a core feature in nearly all commercial\nsmartwatches, well established in clinical and mental health research, and the\nsequential nature of the data shares similarities to language. We introduce the\nPretrained Actigraphy Transformer (PAT), the first open source foundation model\ndesigned for time-series wearable movement data. Leveraging transformer-based\narchitectures and novel techniques, such as patch embeddings, and pretraining\non data from 29,307 participants in a national U.S. sample, PAT achieves\nstate-of-the-art performance in several mental health prediction tasks. PAT is\nalso lightweight and easily interpretable, making it a robust tool for mental\nhealth research.\n  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/\n","authors":["Franklin Y. Ruan","Aiwei Zhang","Jenny Y. Oh","SouYoung Jin","Nicholas C. Jacobson"],"pdf_url":"https://arxiv.org/pdf/2411.15240v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07827v1","updated":"2025-01-14T04:02:08Z","published":"2025-01-14T04:02:08Z","title":"Prediction Interval Construction Method for Electricity Prices","summary":"  Accurate prediction of electricity prices plays an essential role in the\nelectricity market. To reflect the uncertainty of electricity prices, price\nintervals are predicted. This paper proposes a novel prediction interval\nconstruction method. A conditional generative adversarial network is first\npresented to generate electricity price scenarios, with which the prediction\nintervals can be constructed. Then, different generated scenarios are stacked\nto obtain the probability densities, which can be applied to accurately reflect\nthe uncertainty of electricity prices. Furthermore, a reinforced prediction\nmechanism based on the volatility level of weather factors is introduced to\naddress the spikes or volatile prices. A case study is conducted to verify the\neffectiveness of the proposed novel prediction interval construction method.\nThe method can also provide the probability density of each price scenario\nwithin the prediction interval and has the superiority to address the volatile\nprices and price spikes with a reinforced prediction mechanism.\n","authors":["Xin Lu"],"pdf_url":"https://arxiv.org/pdf/2501.07827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07824v1","updated":"2025-01-14T03:59:48Z","published":"2025-01-14T03:59:48Z","title":"Real-time Verification and Refinement of Language Model Text Generation","summary":"  Large language models (LLMs) have shown remarkable performance across a wide\nrange of natural language tasks. However, a critical challenge remains in that\nthey sometimes generate factually incorrect answers. To address this, while\nmany previous work has focused on identifying errors in their generation and\nfurther refining them, they are slow in deployment since they are designed to\nverify the response from LLMs only after their entire generation (from the\nfirst to last tokens) is done. Further, we observe that once LLMs generate\nincorrect tokens early on, there is a higher likelihood that subsequent tokens\nwill also be factually incorrect. To this end, in this work, we propose\nStreaming-VR (Streaming Verification and Refinement), a novel approach designed\nto enhance the efficiency of verification and refinement of LLM outputs.\nSpecifically, the proposed Streaming-VR enables on-the-fly verification and\ncorrection of tokens as they are being generated, similar to a streaming\nprocess, ensuring that each subset of tokens is checked and refined in\nreal-time by another LLM as the LLM constructs its response. Through\ncomprehensive evaluations on multiple datasets, we demonstrate that our\napproach not only enhances the factual accuracy of LLMs, but also offers a more\nefficient solution compared to prior refinement methods.\n","authors":["Joonho Ko","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2501.07824v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2501.07818v1","updated":"2025-01-14T03:43:23Z","published":"2025-01-14T03:43:23Z","title":"A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language\n  Models","summary":"  Among parameter-efficient fine-tuning methods, freezing has emerged as a\npopular strategy for speeding up training, reducing catastrophic forgetting,\nand improving downstream performance. We investigate the impact of freezing the\ndecoder in a multi-task setup comprising diverse natural language tasks, aiming\nto reduce deployment overhead and enhance portability to novel tasks. Our\nexperiments, conducted by fine-tuning both individual and multi-task setups on\nthe AlexaTM model, reveal that freezing decoders is highly effective for tasks\nwith natural language outputs and mitigates catastrophic forgetting in\nmultilingual tasks. However, we find that pairing frozen decoders with a larger\nmodel can effectively maintain or even enhance performance in structured and QA\ntasks, making it a viable strategy for a broader range of task types.\n","authors":["Kaustubh D. Dhole"],"pdf_url":"https://arxiv.org/pdf/2501.07818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00090v2","updated":"2025-01-14T03:27:10Z","published":"2024-11-27T12:34:45Z","title":"Energy-Efficient Split Learning for Fine-Tuning Large Language Models in\n  Edge Networks","summary":"  In this letter, we propose an energy-efficient split learning (SL) framework\nfor fine-tuning large language models (LLMs) using geo-distributed personal\ndata at the network edge, where LLMs are split and alternately across massive\nmobile devices and an edge server. Considering the device heterogeneity and\nchannel dynamics in edge networks, a \\underline{C}ut l\\underline{A}yer and\ncomputing \\underline{R}esource \\underline{D}ecision (CARD) algorithm is\ndeveloped to minimize training delay and energy consumption. Simulation results\ndemonstrate that the proposed approach reduces the average training delay and\nserver's energy consumption by 70.8% and 53.1%, compared to the benchmarks,\nrespectively.\n","authors":["Zuguang Li","Shaohua Wu","Liang Li","Songge Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.00090v2.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.07814v1","updated":"2025-01-14T03:26:05Z","published":"2025-01-14T03:26:05Z","title":"STTS-EAD: Improving Spatio-Temporal Learning Based Time Series\n  Prediction via","summary":"  Handling anomalies is a critical preprocessing step in multivariate time\nseries prediction. However, existing approaches that separate anomaly\npreprocessing from model training for multivariate time series prediction\nencounter significant limitations. Specifically, these methods fail to utilize\nauxiliary information crucial for identifying latent anomalies associated with\nspatiotemporal factors during the preprocessing stage. Instead, they rely\nsolely on data distribution for anomaly detection, which can result in the\nincorrect processing of numerous samples that could otherwise contribute\npositively to model training. To address this, we propose STTS-EAD, an\nend-to-end method that seamlessly integrates anomaly detection into the\ntraining process of multivariate time series forecasting and aims to improve\nSpatio-Temporal learning based Time Series prediction via Embedded Anomaly\nDetection. Our proposed STTS-EAD leverages spatio-temporal information for\nforecasting and anomaly detection, with the two parts alternately executed and\noptimized for each other. To the best of our knowledge, STTS-EAD is the first\nto integrate anomaly detection and forecasting tasks in the training phase for\nimproving the accuracy of multivariate time series forecasting. Extensive\nexperiments on a public stock dataset and two real-world sales datasets from a\nrenowned coffee chain enterprise show that our proposed method can effectively\nprocess detected anomalies in the training stage to improve forecasting\nperformance in the inference stage and significantly outperform baselines.\n","authors":["Yuanyuan Liang","Tianhao Zhang","Tingyu Xie"],"pdf_url":"https://arxiv.org/pdf/2501.07814v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2501.07809v1","updated":"2025-01-14T03:20:17Z","published":"2025-01-14T03:20:17Z","title":"Conformal mapping Coordinates Physics-Informed Neural Networks\n  (CoCo-PINNs): learning neural networks for designing neutral inclusions","summary":"  We focus on designing and solving the neutral inclusion problem via neural\nnetworks. The neutral inclusion problem has a long history in the theory of\ncomposite materials, and it is exceedingly challenging to identify the precise\ncondition that precipitates a general-shaped inclusion into a neutral\ninclusion. Physics-informed neural networks (PINNs) have recently become a\nhighly successful approach to addressing both forward and inverse problems\nassociated with partial differential equations. We found that traditional PINNs\nperform inadequately when applied to the inverse problem of designing neutral\ninclusions with arbitrary shapes. In this study, we introduce a novel approach,\nConformal mapping Coordinates Physics-Informed Neural Networks (CoCo-PINNs),\nwhich integrates complex analysis techniques into PINNs. This method exhibits\nstrong performance in solving forward-inverse problems to construct neutral\ninclusions of arbitrary shapes in two dimensions, where the imperfect interface\ncondition on the inclusion's boundary is modeled by training neural networks.\nNotably, we mathematically prove that training with a single linear field is\nsufficient to achieve neutrality for untrained linear fields in arbitrary\ndirections, given a minor assumption. We demonstrate that CoCo-PINNs offer\nenhanced performances in terms of credibility, consistency, and stability.\n","authors":["Daehee Cho","Hyeonmin Yun","Jaeyong Lee","Mikyoung Lim"],"pdf_url":"https://arxiv.org/pdf/2501.07809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12843v3","updated":"2025-01-14T03:08:02Z","published":"2024-06-18T17:57:49Z","title":"Can Go AIs be adversarially robust?","summary":"  Prior work found that superhuman Go AIs can be defeated by simple adversarial\nstrategies, especially \"cyclic\" attacks. In this paper, we study whether adding\nnatural countermeasures can achieve robustness in Go, a favorable domain for\nrobustness since it benefits from incredible average-case capability and a\nnarrow, innately adversarial setting. We test three defenses: adversarial\ntraining on hand-constructed positions, iterated adversarial training, and\nchanging the network architecture. We find that though some of these defenses\nprotect against previously discovered attacks, none withstand freshly trained\nadversaries. Furthermore, most of the reliably effective attacks these\nadversaries discover are different realizations of the same overall class of\ncyclic attacks. Our results suggest that building robust AI systems is\nchallenging even with extremely superhuman systems in some of the most\ntractable settings, and highlight two key gaps: efficient generalization of\ndefenses, and diversity in training. For interactive examples of attacks and a\nlink to our codebase, see https://goattack.far.ai.\n","authors":["Tom Tseng","Euan McLean","Kellin Pelrine","Tony T. Wang","Adam Gleave"],"pdf_url":"https://arxiv.org/pdf/2406.12843v3.pdf","comment":"63 pages, AAAI 2025"},{"id":"http://arxiv.org/abs/2501.07800v1","updated":"2025-01-14T02:56:19Z","published":"2025-01-14T02:56:19Z","title":"BioPose: Biomechanically-accurate 3D Pose Estimation from Monocular\n  Videos","summary":"  Recent advancements in 3D human pose estimation from single-camera images and\nvideos have relied on parametric models, like SMPL. However, these models\noversimplify anatomical structures, limiting their accuracy in capturing true\njoint locations and movements, which reduces their applicability in\nbiomechanics, healthcare, and robotics. Biomechanically accurate pose\nestimation, on the other hand, typically requires costly marker-based motion\ncapture systems and optimization techniques in specialized labs. To bridge this\ngap, we propose BioPose, a novel learning-based framework for predicting\nbiomechanically accurate 3D human pose directly from monocular videos. BioPose\nincludes three key components: a Multi-Query Human Mesh Recovery model\n(MQ-HMR), a Neural Inverse Kinematics (NeurIK) model, and a 2D-informed pose\nrefinement technique. MQ-HMR leverages a multi-query deformable transformer to\nextract multi-scale fine-grained image features, enabling precise human mesh\nrecovery. NeurIK treats the mesh vertices as virtual markers, applying a\nspatial-temporal network to regress biomechanically accurate 3D poses under\nanatomical constraints. To further improve 3D pose estimations, a 2D-informed\nrefinement step optimizes the query tokens during inference by aligning the 3D\nstructure with 2D pose observations. Experiments on benchmark datasets\ndemonstrate that BioPose significantly outperforms state-of-the-art methods.\nProject website:\n\\url{https://m-usamasaleem.github.io/publication/BioPose/BioPose.html}.\n","authors":["Farnoosh Koleini","Muhammad Usama Saleem","Pu Wang","Hongfei Xue","Ahmed Helmy","Abbey Fenwick"],"pdf_url":"https://arxiv.org/pdf/2501.07800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15274v3","updated":"2025-01-14T02:52:40Z","published":"2024-10-20T04:17:59Z","title":"Physically Guided Deep Unsupervised Inversion for 1D Magnetotelluric\n  Models","summary":"  The global demand for unconventional energy sources such as geothermal energy\nand white hydrogen requires new exploration techniques for precise subsurface\nstructure characterization and potential reservoir identification. The\nMagnetotelluric (MT) method is crucial for these tasks, providing critical\ninformation on the distribution of subsurface electrical resistivity at depths\nranging from hundreds to thousands of meters. However, traditional iterative\nalgorithm-based inversion methods require the adjustment of multiple\nparameters, demanding time-consuming and exhaustive tuning processes to achieve\nproper cost function minimization. Recent advances have incorporated deep\nlearning algorithms for MT inversion, primarily based on supervised learning,\nand large labeled datasets are needed for training. This work utilizes\nTensorFlow operations to create a differentiable forward MT operator,\nleveraging its automatic differentiation capability. Moreover, instead of\nsolving for the subsurface model directly, as classical algorithms perform,\nthis paper presents a new deep unsupervised inversion algorithm guided by\nphysics to estimate 1D MT models. Instead of using datasets with the observed\ndata and their respective model as labels during training, our method employs a\ndifferentiable modeling operator that physically guides the cost function\nminimization, making the proposed method solely dependent on observed data.\nTherefore, the optimization algorithm updates the network weights to minimize\nthe data misfit. We test the proposed method with field and synthetic data at\ndifferent acquisition frequencies, demonstrating that the resistivity models\nobtained are more accurate than those calculated using other techniques.\n","authors":["Paul Goyes-Peñafiel","Umair bin Waheed","Henry Arguello"],"pdf_url":"https://arxiv.org/pdf/2410.15274v3.pdf","comment":"5 pages, 6 figures, github repository, submitted to IEEE-GRSL"},{"id":"http://arxiv.org/abs/2501.06252v2","updated":"2025-01-14T02:52:26Z","published":"2025-01-09T01:19:21Z","title":"$\\text{Transformer}^2$: Self-adaptive LLMs","summary":"  Self-adaptive large language models (LLMs) aim to solve the challenges posed\nby traditional fine-tuning methods, which are often computationally intensive\nand static in their ability to handle diverse tasks. We introduce\n$\\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for\nunseen tasks in real-time by selectively adjusting only the singular components\nof their weight matrices. During inference, $\\text{Transformer}^2$ employs a\ntwo-pass mechanism: first, a dispatch system identifies the task properties,\nand then task-specific \"expert\" vectors, trained using reinforcement learning,\nare dynamically mixed to obtain targeted behavior for the incoming prompt. Our\nmethod outperforms ubiquitous approaches such as LoRA, with fewer parameters\nand greater efficiency. $\\text{Transformer}^2$ demonstrates versatility across\ndifferent LLM architectures and modalities, including vision-language tasks.\n$\\text{Transformer}^2$ represents a significant leap forward, offering a\nscalable, efficient solution for enhancing the adaptability and task-specific\nperformance of LLMs, paving the way for truly dynamic, self-organizing AI\nsystems.\n","authors":["Qi Sun","Edoardo Cetin","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2501.06252v2.pdf","comment":"18 panges, 11 figures, 9 tables"},{"id":"http://arxiv.org/abs/2501.07564v2","updated":"2025-01-14T02:38:26Z","published":"2025-01-13T18:53:23Z","title":"E2ESlack: An End-to-End Graph-Based Framework for Pre-Routing Slack\n  Prediction","summary":"  Pre-routing slack prediction remains a critical area of research in\nElectronic Design Automation (EDA). Despite numerous machine learning-based\napproaches targeting this task, there is still a lack of a truly end-to-end\nframework that engineers can use to obtain TNS/WNS metrics from raw circuit\ndata at the placement stage. Existing works have demonstrated effectiveness in\nArrival Time (AT) prediction but lack a mechanism for Required Arrival Time\n(RAT) prediction, which is essential for slack prediction and obtaining TNS/WNS\nmetrics. In this work, we propose E2ESlack, an end-to-end graph-based framework\nfor pre-routing slack prediction. The framework includes a TimingParser that\nsupports DEF, SDF and LIB files for feature extraction and graph construction,\nan arrival time prediction model and a fast RAT estimation module. To the best\nof our knowledge, this is the first work capable of predicting path-level\nslacks at the pre-routing stage. We perform extensive experiments and\ndemonstrate that our proposed RAT estimation method outperforms the SOTA\nML-based prediction method and also pre-routing STA tool. Additionally, the\nproposed E2ESlack framework achieves TNS/WNS values comparable to post-routing\nSTA results while saving up to 23x runtime.\n","authors":["Saurabh Bodhe","Zhanguang Zhang","Atia Hamidizadeh","Shixiong Kai","Yingxue Zhang","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2501.07564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07794v1","updated":"2025-01-14T02:33:40Z","published":"2025-01-14T02:33:40Z","title":"Linearly Convergent Mixup Learning","summary":"  Learning in the reproducing kernel Hilbert space (RKHS) such as the support\nvector machine has been recognized as a promising technique. It continues to be\nhighly effective and competitive in numerous prediction tasks, particularly in\nsettings where there is a shortage of training data or computational\nlimitations exist. These methods are especially valued for their ability to\nwork with small datasets and their interpretability. To address the issue of\nlimited training data, mixup data augmentation, widely used in deep learning,\nhas remained challenging to apply to learning in RKHS due to the generation of\nintermediate class labels. Although gradient descent methods handle these\nlabels effectively, dual optimization approaches are typically not directly\napplicable. In this study, we present two novel algorithms that extend to a\nbroader range of binary classification models. Unlike gradient-based\napproaches, our algorithms do not require hyperparameters like learning rates,\nsimplifying their implementation and optimization. Both the number of\niterations to converge and the computational cost per iteration scale linearly\nwith respect to the dataset size. The numerical experiments demonstrate that\nour algorithms achieve faster convergence to the optimal solution compared to\ngradient descent approaches, and that mixup data augmentation consistently\nimproves the predictive performance across various loss functions.\n","authors":["Gakuto Obi","Ayato Saito","Yuto Sasaki","Tsuyoshi Kato"],"pdf_url":"https://arxiv.org/pdf/2501.07794v1.pdf","comment":"none"},{"id":"http://arxiv.org/abs/2407.02772v2","updated":"2025-01-14T02:30:09Z","published":"2024-07-03T03:01:43Z","title":"Gradient descent with generalized Newton's method","summary":"  We propose the generalized Newton's method (GeN) -- a Hessian-informed\napproach that applies to any optimizer such as SGD and Adam, and covers the\nNewton-Raphson method as a sub-case. Our method automatically and dynamically\nselects the learning rate that accelerates the convergence, without the\nintensive tuning of the learning rate scheduler. In practice, our method is\neasily implementable, since it only requires additional forward passes with\nalmost zero computational overhead (in terms of training time and memory cost),\nif the overhead is amortized over many iterations. We present extensive\nexperiments on language and vision tasks (e.g. GPT and ResNet) to showcase that\nGeN optimizers match the state-of-the-art performance, which was achieved with\ncarefully tuned learning rate schedulers.\n","authors":["Zhiqi Bu","Shiyun Xu"],"pdf_url":"https://arxiv.org/pdf/2407.02772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19784v4","updated":"2025-01-14T02:28:28Z","published":"2024-12-27T18:25:27Z","title":"Can AI Help with Your Personal Finances?","summary":"  In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising.\n","authors":["Oudom Hean","Utsha Saha","Binita Saha"],"pdf_url":"https://arxiv.org/pdf/2412.19784v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12463v2","updated":"2025-01-14T01:57:04Z","published":"2024-08-22T15:04:59Z","title":"Smartphone-based Eye Tracking System using Edge Intelligence and Model\n  Optimisation","summary":"  A significant limitation of current smartphone-based eye-tracking algorithms\nis their low accuracy when applied to video-type visual stimuli, as they are\ntypically trained on static images. Also, the increasing demand for real-time\ninteractive applications like games, VR, and AR on smartphones requires\novercoming the limitations posed by resource constraints such as limited\ncomputational power, battery life, and network bandwidth. Therefore, we\ndeveloped two new smartphone eye-tracking techniques for video-type visuals by\ncombining Convolutional Neural Networks (CNN) with two different Recurrent\nNeural Networks (RNN), namely Long Short Term Memory (LSTM) and Gated Recurrent\nUnit (GRU). Our CNN+LSTM and CNN+GRU models achieved an average Root Mean\nSquare Error of 0.955 cm and 1.091 cm, respectively. To address the\ncomputational constraints of smartphones, we developed an edge intelligence\narchitecture to enhance the performance of smartphone-based eye tracking. We\napplied various optimisation methods like quantisation and pruning to deep\nlearning models for better energy, CPU, and memory usage on edge devices,\nfocusing on real-time processing. Using model quantisation, the model inference\ntime in the CNN+LSTM and CNN+GRU models was reduced by 21.72% and 19.50%,\nrespectively, on edge devices.\n","authors":["Nishan Gunawardena","Gough Yumu Lui","Jeewani Anupama Ginige","Bahman Javadi"],"pdf_url":"https://arxiv.org/pdf/2408.12463v2.pdf","comment":"I have included the three papers as reference, which are closely\n  related. We have expanded the future work section to provide a more thorough\n  discussion of the concepts of \"varying lighting conditions\" and \"dynamic user\n  environments.\" We have added a note below Table 4 to clarify the\n  abbreviations' meaning. Elaborated the role of the Domain Expert within the\n  presentation layer in Section 4.1"},{"id":"http://arxiv.org/abs/2312.09982v4","updated":"2025-01-14T01:42:46Z","published":"2023-12-15T17:49:24Z","title":"ACPO: AI-Enabled Compiler Framework","summary":"  The key to performance optimization of a program is to decide correctly when\na certain transformation should be applied by a compiler. This is an ideal\nopportunity to apply machine-learning models to speed up the tuning process;\nwhile this realization has been around since the late 90s, only recent\nadvancements in ML enabled a practical application of ML to compilers as an\nend-to-end framework.\n  This paper presents ACPO: An AI-Enabled Compiler Framework, a novel framework\nthat provides LLVM with simple and comprehensive tools to benefit from\nemploying ML models for different optimization passes. We first showcase the\nhigh-level view, class hierarchy, and functionalities of ACPO and subsequently,\ndemonstrate \\taco{a couple of use cases of ACPO by ML-enabling the Loop Unroll\nand Function Inlining passes used in LLVM's O3. and finally, describe how ACPO\ncan be leveraged to optimize other passes. Experimental results reveal that the\nACPO model for Loop Unroll can gain on average 4%, 3%, 5.4%, and 0.2% compared\nto LLVM's vanilla O3 optimization when deployed on Polybench, Coral-2,\nCoreMark, and Graph-500, respectively. Furthermore, by including both Function\nInlining and Loop Unroll models, ACPO can provide a combined speedup of 4.5% on\nPolybench and 2.4% on Cbench when compared with LLVM's O3, respectively.\n","authors":["Amir H. Ashouri","Muhammad Asif Manzoor","Duc Minh Vu","Raymond Zhang","Colin Toft","Ziwen Wang","Angel Zhang","Bryan Chan","Tomasz S. Czajkowski","Yaoqing Gao"],"pdf_url":"https://arxiv.org/pdf/2312.09982v4.pdf","comment":"ACPO (12 pages)"},{"id":"http://arxiv.org/abs/2404.12404v4","updated":"2025-01-14T01:41:21Z","published":"2024-04-15T17:49:16Z","title":"EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular\n  Data Classification via Large Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable in-context learning\ncapabilities across diverse applications. In this work, we explore the\neffectiveness of LLMs for generating realistic synthetic tabular data,\nidentifying key prompt design elements to optimize performance. We introduce\nEPIC, a novel approach that leverages balanced, grouped data samples and\nconsistent formatting with unique variable mapping to guide LLMs in generating\naccurate synthetic data across all classes, even for imbalanced datasets.\nEvaluations on real-world datasets show that EPIC achieves state-of-the-art\nmachine learning classification performance, significantly improving generation\nefficiency. These findings highlight the effectiveness of EPIC for synthetic\ntabular data generation, particularly in addressing class imbalance. Our source\ncode for our work is available at:\nhttps://seharanul17.github.io/project-synthetic-tabular-llm/\n","authors":["Jinhee Kim","Taesung Kim","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2404.12404v4.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2210.01272v3","updated":"2025-01-14T01:34:10Z","published":"2022-10-03T23:44:38Z","title":"A systematic review of the use of Deep Learning in Satellite Imagery for\n  Agriculture","summary":"  Agricultural research is essential for increasing food production to meet the\nrequirements of an increasing population in the coming decades. Recently,\nsatellite technology has been improving rapidly and deep learning has seen much\nsuccess in generic computer vision tasks and many application areas which\npresents an important opportunity to improve analysis of agricultural land.\nHere we present a systematic review of 150 studies to find the current uses of\ndeep learning on satellite imagery for agricultural research. Although we\nidentify 5 categories of agricultural monitoring tasks, the majority of the\nresearch interest is in crop segmentation and yield prediction. We found that,\nwhen used, modern deep learning methods consistently outperformed traditional\nmachine learning across most tasks; the only exception was that Long Short-Term\nMemory (LSTM) Recurrent Neural Networks did not consistently outperform Random\nForests (RF) for yield prediction. The reviewed studies have largely adopted\nmethodologies from generic computer vision, except for one major omission:\nbenchmark datasets are not utilised to evaluate models across studies, making\nit difficult to compare results. Additionally, some studies have specifically\nutilised the extra spectral resolution available in satellite imagery, but\nother divergent properties of satellite images - such as the hugely different\nscales of spatial patterns - are not being taken advantage of in the reviewed\nstudies.\n","authors":["Brandon Victor","Zhen He","Aiden Nibali"],"pdf_url":"https://arxiv.org/pdf/2210.01272v3.pdf","comment":"23 pages, 5 figures and 10 tables in main paper. Final version, as\n  submitted and accepted at JSTARS"},{"id":"http://arxiv.org/abs/2302.01313v8","updated":"2025-01-14T01:28:03Z","published":"2023-02-02T18:39:30Z","title":"Double Equivariance for Inductive Link Prediction for Both New Nodes and\n  New Relation Types","summary":"  The task of fully inductive link prediction in knowledge graphs has gained\nsignificant attention, with various graph neural networks being proposed to\naddress it. This task presents greater challenges than traditional inductive\nlink prediction tasks with only new nodes, as models must be capable of\nzero-shot generalization to both unseen nodes and unseen relation types in the\ninference graph. Despite the development of novel models, a unifying\ntheoretical understanding of their success remains elusive, and the limitations\nof these methods are not well-studied. In this work, we introduce the concept\nof double permutation-equivariant representations and demonstrate its necessity\nfor effective performance in this task. We show that many existing models,\ndespite their diverse architectural designs, conform to this framework.\nHowever, we also identify inherent limitations in double\npermutation-equivariant representations, which restrict these models's ability\nto learn effectively on datasets with varying characteristics. Our findings\nsuggest that while double equivariance is necessary for meta-learning across\nknowledge graphs from different domains, it is not sufficient. There remains a\nfundamental gap between double permutation-equivariant models and the concept\nof foundation models designed to learn patterns across all domains.\n","authors":["Jincheng Zhou","Yucheng Zhang","Jianfei Gao","Yangze Zhou","Bruno Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2302.01313v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07774v1","updated":"2025-01-14T01:16:30Z","published":"2025-01-14T01:16:30Z","title":"Transforming Indoor Localization: Advanced Transformer Architecture for\n  NLOS Dominated Wireless Environments with Distributed Sensors","summary":"  Indoor localization in challenging non-line-of-sight (NLOS) environments\noften leads to mediocre accuracy with traditional approaches. Deep learning\n(DL) has been applied to tackle these challenges; however, many DL approaches\noverlook computational complexity, especially for floating-point operations\n(FLOPs), making them unsuitable for resource-limited devices. Transformer-based\nmodels have achieved remarkable success in natural language processing (NLP)\nand computer vision (CV) tasks, motivating their use in wireless applications.\nHowever, their use in indoor localization remains nascent, and directly\napplying Transformers for indoor localization can be both computationally\nintensive and exhibit limitations in accuracy. To address these challenges, in\nthis work, we introduce a novel tokenization approach, referred to as Sensor\nSnapshot Tokenization (SST), which preserves variable-specific representations\nof power delay profile (PDP) and enhances attention mechanisms by effectively\ncapturing multi-variate correlation. Complementing this, we propose a\nlightweight Swish-Gated Linear Unit-based Transformer (L-SwiGLU Transformer)\nmodel, designed to reduce computational complexity without compromising\nlocalization accuracy. Together, these contributions mitigate the computational\nburden and dependency on large datasets, making Transformer models more\nefficient and suitable for resource-constrained scenarios. The proposed\ntokenization method enables the Vanilla Transformer to achieve a 90th\npercentile positioning error of 0.388 m in a highly NLOS indoor factory,\nsurpassing conventional tokenization methods. The L-SwiGLU ViT further reduces\nthe error to 0.355 m, achieving an 8.51% improvement. Additionally, the\nproposed model outperforms a 14.1 times larger model with a 46.13% improvement,\nunderscoring its computational efficiency.\n","authors":["Saad Masrur"," Jung-Fu"," Cheng","Atieh R. Khamesi","Ismail Guvenc"],"pdf_url":"https://arxiv.org/pdf/2501.07774v1.pdf","comment":"The paper has been submitted to IEEE Transactions on Machine Learning\n  in Communications and Networking"},{"id":"http://arxiv.org/abs/2501.07773v1","updated":"2025-01-14T01:08:15Z","published":"2025-01-14T01:08:15Z","title":"Symmetry-Aware Generative Modeling through Learned Canonicalization","summary":"  Generative modeling of symmetric densities has a range of applications in AI\nfor science, from drug discovery to physics simulations. The existing\ngenerative modeling paradigm for invariant densities combines an invariant\nprior with an equivariant generative process. However, we observe that this\ntechnique is not necessary and has several drawbacks resulting from the\nlimitations of equivariant networks. Instead, we propose to model a learned\nslice of the density so that only one representative element per orbit is\nlearned. To accomplish this, we learn a group-equivariant canonicalization\nnetwork that maps training samples to a canonical pose and train a\nnon-equivariant generative model over these canonicalized samples. We implement\nthis idea in the context of diffusion models. Our preliminary experimental\nresults on molecular modeling are promising, demonstrating improved sample\nquality and faster inference time.\n","authors":["Kusha Sareen","Daniel Levy","Arnab Kumar Mondal","Sékou-Oumar Kaba","Tara Akhound-Sadegh","Siamak Ravanbakhsh"],"pdf_url":"https://arxiv.org/pdf/2501.07773v1.pdf","comment":"NeurReps 2024 Workshop Version"},{"id":"http://arxiv.org/abs/2501.07769v1","updated":"2025-01-14T00:59:55Z","published":"2025-01-14T00:59:55Z","title":"BMIP: Bi-directional Modality Interaction Prompt Learning for VLM","summary":"  Vision-language models (VLMs) have exhibited remarkable generalization\ncapabilities, and prompt learning for VLMs has attracted great attention for\nthe ability to adapt pre-trained VLMs to specific downstream tasks. However,\nexisting studies mainly focus on single-modal prompts or uni-directional\nmodality interaction, overlooking the powerful alignment effects resulting from\nthe interaction between the vision and language modalities. To this end, we\npropose a novel prompt learning method called\n$\\underline{\\textbf{B}}i-directional \\underline{\\textbf{M}}odality\n\\underline{\\textbf{I}}nteraction \\underline{\\textbf{P}}rompt (BMIP)$, which\ndynamically weights bi-modal information through learning the information of\nthe attention layer, enhancing trainability and inter-modal consistency\ncompared to simple information aggregation methods. To evaluate the\neffectiveness of prompt learning methods, we propose a more realistic\nevaluation paradigm called open-world generalization complementing the widely\nadopted cross-dataset transfer and domain generalization tasks. Comprehensive\nexperiments on various datasets reveal that BMIP not only outperforms current\nstate-of-the-art methods across all three evaluation paradigms but is also\nflexible enough to be combined with other prompt-based methods for consistent\nperformance enhancement.\n","authors":["Song-Lin Lv","Yu-Yang Chen","Zhi Zhou","Ming Yang","Lan-Zhe Guo"],"pdf_url":"https://arxiv.org/pdf/2501.07769v1.pdf","comment":null}]},"2025-01-13T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2501.07689v1","updated":"2025-01-13T21:05:04Z","published":"2025-01-13T21:05:04Z","title":"Real-Time Outlier Connections Detection in Databases Network Traffic","summary":"  The article describes a practical method for detecting outlier database\nconnections in real-time. Outlier connections are detected with a specified\nlevel of confidence. The method is based on generalized security rules and a\nsimple but effective real-time machine learning mechanism. The described method\nis non-intrusive to the database and does not depend on the type of database.\nThe method is used to proactively control access even before database\nconnection is established, minimize false positives, and maintain the required\nresponse speed to detected database connection outliers. The capabilities of\nthe system are demonstrated with several examples of outliers in real-world\nscenarios.\n","authors":["Leonid Rodniansky","Tania Butovsky","Mikhail Shpak"],"pdf_url":"https://arxiv.org/pdf/2501.07689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07449v1","updated":"2025-01-13T16:20:55Z","published":"2025-01-13T16:20:55Z","title":"On the effects of logical database design on database size, query\n  complexity, query performance, and energy consumption","summary":"  Database normalization theory is the basis for logical design of relational\ndatabases. Normalization reduces data redundancy and consequently eliminates\npotential data anomalies, while increasing the computational cost of read\noperations. Despite decades worth of applications of normalization theory, it\nstill remains largely unclear to what extent normalization affects database\nsize and efficiency. In this study, we study the effects of database\nnormalization using the Internet Movie Database (IMDb) public dataset and\nPostgreSQL. The results indicate, rather intuitively, that (i) database size on\ndisk is reduced through normalization from 1NF to 2NF by 10%, but not from 2NF\nto 4NF, (ii) the number of tables and table rows in total increase\nmonotonically from 1NF to 2NF to 4NF, and that (iii) query complexity increases\nwith further normalization. Surprisingly, however, the results also indicate\nthat (iv) normalization from 1NF to 2NF increases throughput by a factor of 4,\nand consequently, (v) energy consumption per transaction reduces by 74% with\nnormalization from 1NF to 2NF. The results imply that the gains of\nnormalization from 2NF to 4NF in terms of throughput and energy consumption are\nminimal, yet increase the storage space requirements by approximately 7%. While\nthese results represent merely one specific case, they provide needed empirical\nevaluation on the practical effects and magnitude of database normalization.\n","authors":["Toni Taipalus"],"pdf_url":"https://arxiv.org/pdf/2501.07449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00344v2","updated":"2025-01-13T15:50:17Z","published":"2024-06-01T07:42:34Z","title":"Efficient Historical Butterfly Counting in Large Temporal Bipartite\n  Networks via Graph Structure-aware Index","summary":"  Bipartite graphs are ubiquitous in many domains, e.g., e-commerce platforms,\nsocial networks, and academia, by modeling interactions between distinct entity\nsets. Within these graphs, the butterfly motif, a complete 2*2 biclique,\nrepresents the simplest yet significant subgraph structure, crucial for\nanalyzing complex network patterns. Counting the butterflies offers significant\nbenefits across various applications, including community analysis and\nrecommender systems. Additionally, the temporal dimension of bipartite graphs,\nwhere edges activate within specific time frames, introduces the concept of\nhistorical butterfly counting, i.e., counting butterflies within a given time\ninterval. This temporal analysis sheds light on the dynamics and evolution of\nnetwork interactions, offering new insights into their mechanisms. Despite its\nimportance, no existing algorithm can efficiently solve the historical\nbutterfly counting task. To address this, we design two novel indices whose\nmemory footprints are dependent on #butterflies and #wedges, respectively.\nCombining these indices, we propose a graph structure-aware indexing approach\nthat significantly reduces memory usage while preserving exceptional query\nspeed. We theoretically prove that our approach is particularly advantageous on\npower-law graphs, a common characteristic of real-world bipartite graphs, by\nsurpassing traditional complexity barriers for general graphs. Extensive\nexperiments reveal that our query algorithms outperform existing methods by up\nto five magnitudes, effectively balancing speed with manageable memory\nrequirements.\n","authors":["Qiuyang Mang","Jingbang Chen","Hangrui Zhou","Yu Gao","Yingli Zhou","Qingyu Shi","Richard Peng","Yixiang Fang","Chenhao Ma"],"pdf_url":"https://arxiv.org/pdf/2406.00344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07398v1","updated":"2025-01-13T15:17:01Z","published":"2025-01-13T15:17:01Z","title":"An ontology-based description of nano computed tomography measurements\n  in electronic laboratory notebooks: from metadata schema to first user\n  experience","summary":"  In recent years, the importance of well-documented metadata has been\ndiscussed increasingly in many research fields. Making all metadata generated\nduring scientific research available in a findable, accessible, interoperable,\nand reusable (FAIR) manner remains a significant challenge for researchers\nacross fields. Scientific communities are agreeing to achieve this by making\nall data available in a semantically annotated knowledge graph using semantic\nweb technologies. Most current approaches do not gather metadata in a\nconsistent and community-agreed standardized way, and there are insufficient\ntools to support the process of turning them into a knowledge graph. We present\nan example solution in which the creation of a schema and ontology are placed\nat the beginning of the scientific process which is then - using the electronic\nlaboratory notebook framework Herbie - turned into a bespoke data collection\nplatform to facilitate validation and semantic annotation of the metadata\nimmediately during an experiment. Using the example of synchrotron\nradiation-based nano computed tomography measurements, we present a holistic\napproach which can capture the complex metadata of such research instruments in\na flexible and straightforward manner. Different instrument setups of this\nbeamline can be considered, allowing a user-friendly experience. We show how\nHerbie turns all semantic documents into an accessible user interface, where\nall data entered automatically fulfills all requirements of being FAIR, and\npresent how data can be directly extracted via competency questions without\nrequiring familiarity with the fine-grained structure of the knowledge graph.\n","authors":["Fabian Kirchner","D. C. Florian Wieland","Sarah Irvine","Sven Schimek","Jan Reimers","Rossella Aversa","Alexey Boubnov","Christian Lucas","Silja Flenner","Imke Greving","André Lopes Marinho","Tak Ming Wong","Regine Willumeit-Römer","Catriona Eschke","Berit Zeller-Plumhoff"],"pdf_url":"https://arxiv.org/pdf/2501.07398v1.pdf","comment":"21 pages, 13 figures, 5 tables. Fabian Kirchner and Florian Wieland\n  have contributed equally to the manuscript. Corresponding authors:\n  fabian.kirchner@hereon.de, catriona.eschke@hereon.de"},{"id":"http://arxiv.org/abs/2411.14754v2","updated":"2025-01-13T08:37:43Z","published":"2024-11-22T06:23:10Z","title":"Subspace Collision: An Efficient and Accurate Framework for\n  High-dimensional Approximate Nearest Neighbor Search","summary":"  Approximate Nearest Neighbor (ANN) search in high-dimensional Euclidean\nspaces is a fundamental problem with a wide range of applications. However,\nthere is currently no ANN method that performs well in both indexing and query\nanswering performance, while providing rigorous theoretical guarantees for the\nquality of the answers. In this paper, we first design SC-score, a metric that\nwe show follows the Pareto principle and can act as a proxy for the Euclidean\ndistance between data points. Inspired by this, we propose a novel ANN search\nframework called Subspace Collision (SC), which can provide theoretical\nguarantees on the quality of its results. We further propose SuCo, which\nachieves efficient and accurate ANN search by designing a clustering-based\nlightweight index and query strategies for our proposed subspace collision\nframework. Extensive experiments on real-world datasets demonstrate that both\nthe indexing and query answering performance of SuCo outperform\nstate-of-the-art ANN methods that can provide theoretical guarantees,\nperforming 1-2 orders of magnitude faster query answering with only up to\none-tenth of the index memory footprint. Moreover, SuCo achieves top\nperformance (best for hard datasets) even when compared to methods that do not\nprovide theoretical guarantees. This paper was published in SIGMOD 2025.\n","authors":["Jiuqi Wei","Xiaodong Lee","Zhenyu Liao","Themis Palpanas","Botao Peng"],"pdf_url":"https://arxiv.org/pdf/2411.14754v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07106v1","updated":"2025-01-13T07:38:12Z","published":"2025-01-13T07:38:12Z","title":"Efficient Multiple Temporal Network Kernel Density Estimation","summary":"  Kernel density estimation (KDE) has become a popular method for visual\nanalysis in various fields, such as financial risk forecasting, crime\nclustering, and traffic monitoring. KDE can identify high-density areas from\ndiscrete datasets. However, most existing works only consider planar distance\nand spatial data. In this paper, we introduce a new model, called TN-KDE, that\napplies KDE-based techniques to road networks with temporal data. Specifically,\nwe introduce a novel solution, Range Forest Solution (RFS), which can\nefficiently compute KDE values on spatiotemporal road networks. To support the\ninsertion operation, we present a dynamic version, called Dynamic Range Forest\nSolution (DRFS). We also propose an optimization called Lixel Sharing (LS) to\nshare similar KDE values between two adjacent lixels. Furthermore, our\nsolutions support many non-polynomial kernel functions and still report exact\nvalues. Experimental results show that our solutions achieve up to 6 times\nfaster than the state-of-the-art method.\n","authors":["Yu Shao","Peng Cheng","Xiang Lian","Lei Chen","Wangze Ni","Xuemin Lin","Chen Zhang","Liping Wang"],"pdf_url":"https://arxiv.org/pdf/2501.07106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07078v1","updated":"2025-01-13T06:22:52Z","published":"2025-01-13T06:22:52Z","title":"ADKGD: Anomaly Detection in Knowledge Graphs with Dual-Channel Training","summary":"  In the current development of large language models (LLMs), it is important\nto ensure the accuracy and reliability of the underlying data sources. LLMs are\ncritical for various applications, but they often suffer from hallucinations\nand inaccuracies due to knowledge gaps in the training data. Knowledge graphs\n(KGs), as a powerful structural tool, could serve as a vital external\ninformation source to mitigate the aforementioned issues. By providing a\nstructured and comprehensive understanding of real-world data, KGs enhance the\nperformance and reliability of LLMs. However, it is common that errors exist in\nKGs while extracting triplets from unstructured data to construct KGs. This\ncould lead to degraded performance in downstream tasks such as\nquestion-answering and recommender systems. Therefore, anomaly detection in KGs\nis essential to identify and correct these errors. This paper presents an\nanomaly detection algorithm in knowledge graphs with dual-channel learning\n(ADKGD). ADKGD leverages a dual-channel learning approach to enhance\nrepresentation learning from both the entity-view and triplet-view\nperspectives. Furthermore, using a cross-layer approach, our framework\nintegrates internal information aggregation and context information\naggregation. We introduce a kullback-leibler (KL)-loss component to improve the\naccuracy of the scoring function between the dual channels. To evaluate ADKGD's\nperformance, we conduct empirical studies on three real-world KGs: WN18RR,\nFB15K, and NELL-995. Experimental results demonstrate that ADKGD outperforms\nthe state-of-the-art anomaly detection algorithms. The source code and datasets\nare publicly available at https://github.com/csjywu1/ADKGD.\n","authors":["Jiayang Wu","Wensheng Gan","Jiahao Zhang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2501.07078v1.pdf","comment":"Preprint. 11 figures, 6 tables"}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2407.05467v2","updated":"2025-01-13T22:53:34Z","published":"2024-07-07T18:39:33Z","title":"The infrastructure powering IBM's Gen AI model development","summary":"  AI Infrastructure plays a key role in the speed and cost-competitiveness of\ndeveloping and deploying advanced AI models. The current demand for powerful AI\ninfrastructure for model training is driven by the emergence of generative AI\nand foundational models, where on occasion thousands of GPUs must cooperate on\na single training job for the model to be trained in a reasonable time.\nDelivering efficient and high-performing AI training requires an end-to-end\nsolution that combines hardware, software and holistic telemetry to cater for\nmultiple types of AI workloads. In this report, we describe IBM's hybrid cloud\ninfrastructure that powers our generative AI model development. This\ninfrastructure includes (1) Vela: an AI-optimized supercomputing capability\ndirectly integrated into the IBM Cloud, delivering scalable, dynamic,\nmulti-tenant and geographically distributed infrastructure for large-scale\nmodel training and other AI workflow steps and (2) Blue Vela: a large-scale,\npurpose-built, on-premises hosting environment that is optimized to support our\nlargest and most ambitious AI model training tasks. Vela provides IBM with the\ndual benefit of high performance for internal use along with the flexibility to\nadapt to an evolving commercial landscape. Blue Vela provides us with the\nbenefits of rapid development of our largest and most ambitious models, as well\nas future-proofing against the evolving model landscape in the industry. Taken\ntogether, they provide IBM with the ability to rapidly innovate in the\ndevelopment of both AI models and commercial offerings.\n","authors":["Talia Gershon","Seetharami Seelam","Brian Belgodere","Milton Bonilla","Lan Hoang","Danny Barnett","I-Hsin Chung","Apoorve Mohan","Ming-Hung Chen","Lixiang Luo","Robert Walkup","Constantinos Evangelinos","Shweta Salaria","Marc Dombrowa","Yoonho Park","Apo Kayi","Liran Schour","Alim Alim","Ali Sydney","Pavlos Maniotis","Laurent Schares","Bernard Metzler","Bengi Karacali-Akyamac","Sophia Wen","Tatsuhiro Chiba","Sunyanan Choochotkaew","Takeshi Yoshimura","Claudia Misale","Tonia Elengikal","Kevin O Connor","Zhuoran Liu","Richard Molina","Lars Schneidenbach","James Caden","Christopher Laibinis","Carlos Fonseca","Vasily Tarasov","Swaminathan Sundararaman","Frank Schmuck","Scott Guthridge","Jeremy Cohn","Marc Eshel","Paul Muench","Runyu Liu","William Pointer","Drew Wyskida","Bob Krull","Ray Rose","Brent Wolfe","William Cornejo","John Walter","Colm Malone","Clifford Perucci","Frank Franco","Nigel Hinds","Bob Calio","Pavel Druyan","Robert Kilduff","John Kienle","Connor McStay","Andrew Figueroa","Matthew Connolly","Edie Fost","Gina Roma","Jake Fonseca","Ido Levy","Michele Payne","Ryan Schenkel","Amir Malki","Lion Schneider","Aniruddha Narkhede","Shekeba Moshref","Alexandra Kisin","Olga Dodin","Bill Rippon","Henry Wrieth","John Ganci","Johnny Colino","Donna Habeger-Rose","Rakesh Pandey","Aditya Gidh","Aditya Gaur","Dennis Patterson","Samsuddin Salmani","Rambilas Varma","Rumana Rumana","Shubham Sharma","Aditya Gaur","Mayank Mishra","Rameswar Panda","Aditya Prasad","Matt Stallone","Gaoyuan Zhang","Yikang Shen","David Cox","Ruchir Puri","Dakshi Agrawal","Drew Thorstensen","Joel Belog","Brent Tang","Saurabh Kumar Gupta","Amitabha Biswas","Anup Maheshwari","Eran Gampel","Jason Van Patten","Matthew Runion","Sai Kaki","Yigal Bogin","Brian Reitz","Steve Pritko","Shahan Najam","Surya Nambala","Radhika Chirra","Rick Welp","Frank DiMitri","Felipe Telles","Amilcar Arvelo","King Chu","Ed Seminaro","Andrew Schram","Felix Eickhoff","William Hanson","Eric Mckeever","Michael Light","Dinakaran Joseph","Piyush Chaudhary","Piyush Shivam","Puneet Chaudhary","Wesley Jones","Robert Guthrie","Chris Bostic","Rezaul Islam","Steve Duersch","Wayne Sawdon","John Lewars","Matthew Klos","Michael Spriggs","Bill McMillan","George Gao","Ashish Kamra","Gaurav Singh","Marc Curry","Tushar Katarki","Joe Talerico","Zenghui Shi","Sai Sindhur Malleni","Erwan Gallen"],"pdf_url":"https://arxiv.org/pdf/2407.05467v2.pdf","comment":"Corresponding Authors: Talia Gershon, Seetharami Seelam,Brian\n  Belgodere, Milton Bonilla"},{"id":"http://arxiv.org/abs/2501.07705v1","updated":"2025-01-13T21:32:42Z","published":"2025-01-13T21:32:42Z","title":"Autonomous Electrochemistry Platform with Real-Time Normality Testing of\n  Voltammetry Measurements Using ML","summary":"  Electrochemistry workflows utilize various instruments and computing systems\nto execute workflows consisting of electrocatalyst synthesis, testing and\nevaluation tasks. The heterogeneity of the software and hardware of these\necosystems makes it challenging to orchestrate a complete workflow from\nproduction to characterization by automating its tasks. We propose an\nautonomous electrochemistry computing platform for a multi-site ecosystem that\nprovides the services for remote experiment steering, real-time measurement\ntransfer, and AI/ML-driven analytics. We describe the integration of a mobile\nrobot and synthesis workstation into the ecosystem by developing custom\nhub-networks and software modules to support remote operations over the\necosystem's wireless and wired networks. We describe a workflow task for\ngenerating I-V voltammetry measurements using a potentiostat, and a machine\nlearning framework to ensure their normality by detecting abnormal conditions\nsuch as disconnected electrodes. We study a number of machine learning methods\nfor the underlying detection problem, including smooth, non-smooth, structural\nand statistical methods, and their fusers. We present experimental results to\nillustrate the effectiveness of this platform, and also validate the proposed\nML method by deriving its rigorous generalization equations.\n","authors":["Anees Al-Najjar","Nageswara S. V. Rao","Craig A. Bridges","Sheng Dai","Alex Walters"],"pdf_url":"https://arxiv.org/pdf/2501.07705v1.pdf","comment":"10 pages, 14 figures, accepted in the IEEE 20th International\n  Conference on e-Science (e-Science), 2024"},{"id":"http://arxiv.org/abs/2501.07676v1","updated":"2025-01-13T20:24:10Z","published":"2025-01-13T20:24:10Z","title":"Smells-sus: Sustainability Smells in IaC","summary":"  Practitioners use Infrastructure as Code (IaC) scripts to efficiently\nconfigure IT infrastructures through machine-readable definition files.\nHowever, during the development of these scripts, some code patterns or\ndeployment choices may lead to sustainability issues like inefficient resource\nutilization or redundant provisioning for example. We call this type of\npatterns sustainability smells. These inefficiencies pose significant\nenvironmental and financial challenges, given the growing scale of cloud\ncomputing. This research focuses on Terraform, a widely adopted IaC tool. Our\nstudy involves defining seven sustainability smells and validating them through\na survey with 19 IaC practitioners. We utilized a dataset of 28,327 Terraform\nscripts from 395 open-source repositories. We performed a detailed qualitative\nanalysis of a randomly sampled 1,860 Terraform scripts from the original\ndataset to identify code patterns that correspond to the sustainability smells\nand used the other 26,467 Terraform scripts to study the prevalence of the\ndefined sustainability smells. Our results indicate varying prevalence rates of\nthese smells across the dataset. The most prevalent smell is Monolithic\nInfrastructure, which appears in 9.67\\% of the scripts. Additionally, our\nfindings highlight the complexity of conducting root cause analysis for\nsustainability issues, as these smells often arise from a confluence of script\nstructures, configuration choices, and deployment contexts.\n","authors":["Seif Ashraf","Mohammad Hamdaqa"],"pdf_url":"https://arxiv.org/pdf/2501.07676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07526v1","updated":"2025-01-13T17:56:39Z","published":"2025-01-13T17:56:39Z","title":"Communication-Efficient, 2D Parallel Stochastic Gradient Descent for\n  Distributed-Memory Optimization","summary":"  Distributed-memory implementations of numerical optimization algorithm, such\nas stochastic gradient descent (SGD), require interprocessor communication at\nevery iteration of the algorithm. On modern distributed-memory clusters where\ncommunication is more expensive than computation, the scalability and\nperformance of these algorithms are limited by communication cost. This work\ngeneralizes prior work on 1D $s$-step SGD and 1D Federated SGD with Averaging\n(FedAvg) to yield a 2D parallel SGD method (HybridSGD) which attains a\ncontinuous performance trade off between the two baseline algorithms. We\npresent theoretical analysis which show the convergence, computation,\ncommunication, and memory trade offs between $s$-step SGD, FedAvg, 2D parallel\nSGD, and other parallel SGD variants. We implement all algorithms in C++ and\nMPI and evaluate their performance on a Cray EX supercomputing system. Our\nempirical results show that HybridSGD achieves better convergence than FedAvg\nat similar processor scales while attaining speedups of $5.3\\times$ over\n$s$-step SGD and speedups up to $121\\times$ over FedAvg when used to solve\nbinary classification tasks using the convex, logistic regression model on\ndatasets obtained from the LIBSVM repository.\n","authors":["Aditya Devarakonda","Ramakrishnan Kannan"],"pdf_url":"https://arxiv.org/pdf/2501.07526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07503v1","updated":"2025-01-13T17:19:38Z","published":"2025-01-13T17:19:38Z","title":"Big Atomics","summary":"  In this paper, we give theoretically and practically efficient\nimplementations of Big Atomics, i.e., $k$-word linearizable registers that\nsupport the load, store, and compare-and-swap (CAS) operations. While modern\nhardware supports $k = 1$ and sometimes $k = 2$ (e.g., double-width\ncompare-and-swap in x86), our implementations support arbitrary $k$. Big\nAtomics are useful in many applications, including atomic manipulation of\ntuples, version lists, and implementing load-linked/store-conditional (LL/SC).\nWe design fast, lock-free implementations of big atomics based on a novel\nfast-path-slow-path approach we develop. We then use them to develop an\nefficient concurrent hash table, as evidence of their utility.\n  We experimentally validate the approach by comparing a variety of\nimplementations of big atomics under a variety of workloads (thread counts,\nload/store ratios, contention, oversubscription, and number of atomics). The\nexperiments compare two of our lock-free variants with C++ std::atomic, a\nlock-based version, a version using sequence locks, and an indirect version.\nThe results show that our approach is close to the fastest under all conditions\nand far outperforms others under oversubscription. We also compare our big\natomics based concurrent hash table to a variety of other state-of-the-art hash\ntables that support arbitrary length keys and values, including implementations\nfrom Intel's TBB, Facebook's Folly, libcuckoo, and a recent release from Boost.\nThe results show that our approach of using big atomics in the design of hash\ntables is a promising direction.\n","authors":["Daniel Anderson","Guy E. Blelloch","Siddhartha Jayanti"],"pdf_url":"https://arxiv.org/pdf/2501.07503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19706v3","updated":"2025-01-13T14:15:59Z","published":"2024-12-27T16:00:24Z","title":"Geometric Freeze-Tag Problem","summary":"  We study the Freeze-Tag Problem (FTP), introduced by Arkin et al. (SODA'02),\nwhere the objective is to activate a group of n robots, starting from a single\ninitially active robot. Robots are positioned in $\\mathbb{R}^d$, and once\nactivated, they move at a constant speed to wake up others. The goal is to\nminimize the time required to activate the last robot, known as the makespan.\nWe establish new upper bounds for the makespan under the $l_1$ and $l_2$ norms\nin $\\mathbb{R}^2$ and $\\mathbb{R}^3$. Specifically, we improve the previous\nupper bound for $(\\mathbb{R}^2, l_2)$ from $7.07r$ (Bonichon et al., DISC'24)\nto $5.064r$. For $(\\mathbb{R}^3, l_1)$, we derive a makespan bound of $13r$,\nwhich translates to $22.52r$ for $(\\mathbb{R}^3, l_2)$. Here, $r$ denotes the\nmaximum distance of any robot from the initially active robot under the given\nnorm. To our knowledge, these are the first makespan bounds for FTP in\n$\\mathbb{R}^3$. Additionally, we show that the maximum makespan for $n$ robots\nis not necessarily achieved when robots are equally distributed along the\nboundary in $(\\mathbb{R}^2, l_2)$. We further investigate FTP in\n$(\\mathbb{R}^3, l_2)$ for specific configurations where robots lie on a\nboundary, providing insights into practical scenarios.\n","authors":["Sharareh Alipour","Kajal Baghestani","Mahdis Mirzaei","Soroush Sahraei"],"pdf_url":"https://arxiv.org/pdf/2412.19706v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07207v1","updated":"2025-01-13T11:01:41Z","published":"2025-01-13T11:01:41Z","title":"Beyond Security-by-design: Securing a compromised system","summary":"  Digital infrastructures are seeing convergence and connectivity at\nunprecedented scale. This is true for both current critical national\ninfrastructures and emerging future systems that are highly cyber-physical in\nnature with complex intersections between humans and technologies, e.g., smart\ncities, intelligent transportation, high-value manufacturing and Industry 4.0.\nDiverse legacy and non-legacy software systems underpinned by heterogeneous\nhardware compose on-the-fly to deliver services to millions of users with\nvarying requirements and unpredictable actions. This complexity is compounded\nby intricate and complicated supply-chains with many digital assets and\nservices outsourced to third parties. The reality is that, at any particular\npoint in time, there will be untrusted, partially-trusted or compromised\nelements across the infrastructure. Given this reality, and the societal scale\nof digital infrastructures, delivering secure and resilient operations is a\nmajor challenge. We argue that this requires us to move beyond the paradigm of\nsecurity-by-design and embrace the challenge of securing-a-compromised-system.\n","authors":["Awais Rashid","Sana Belguith","Matthew Bradbury","Sadie Creese","Ivan Flechais","Neeraj Suri"],"pdf_url":"https://arxiv.org/pdf/2501.07207v1.pdf","comment":"Article for the Rossfest Symposium in memory of Ross Anderson,\n  Cambridge, UK, 25 March 2025"},{"id":"http://arxiv.org/abs/2409.01990v3","updated":"2025-01-13T10:02:27Z","published":"2024-09-03T15:35:01Z","title":"Efficient Large Foundation Models Design: A Perspective From Model and\n  System Co-Design","summary":"  This paper focuses on modern efficient training and inference technologies on\nfoundation models and illustrates them from two perspectives: model and system\ndesign. Model and System Design optimize LLM training and inference from\ndifferent aspects to save computational resources, making LLMs more efficient,\naffordable, and more accessible. The paper list repository is available at\n\\url{https://github.com/NoakLiu/Efficient-Foundation-Models-Survey}\n","authors":["Dong Liu","Yanxuan Yu","Zhixin Lai","Yite Wang","Jing Wu","Zhongwei Wan","Sina Alinejad","Benjamin Lengerich","Ying Nian Wu"],"pdf_url":"https://arxiv.org/pdf/2409.01990v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07130v1","updated":"2025-01-13T08:34:04Z","published":"2025-01-13T08:34:04Z","title":"KubeDSM: A Kubernetes-based Dynamic Scheduling and Migration Framework\n  for Cloud-Assisted Edge Clusters","summary":"  Edge computing has become critical for enabling latency-sensitive\napplications, especially when paired with cloud resources to form\ncloud-assisted edge clusters. However, efficient resource management remains\nchallenging due to edge nodes' limited capacity and unreliable connectivity.\nThis paper introduces KubeDSM, a Kubernetes-based dynamic scheduling and\nmigration framework tailored for cloud-assisted edge environments. KubeDSM\naddresses the challenges of resource fragmentation, dynamic scheduling, and\nlive migration while ensuring Quality of Service (QoS) for latency-sensitive\napplications. Unlike Kubernetes' default scheduler, KubeDSM adopts batch\nscheduling to minimize resource fragmentation and incorporates a live migration\nmechanism to optimize edge resource utilization. Specifically, KubeDSM\nfacilitates three key operations: intra-edge migration to reduce fragmentation,\nedge-to-cloud migration during resource shortages, and cloud-to-edge migration\nwhen resources become available, thereby increasing the number of pods\nallocated to the edge. Our results demonstrate that KubeDSM consistently\nachieves a higher average edge ratio and a lower standard deviation in edge\nratios, highlighting its ability to provide more effective and stable\nscheduling across different deployments. We also explore the impact of\nmigration strategies and Quality of Service (QoS) configurations on the edge\nratios achieved by KubeDSM. The findings reveal that enabling migrations\nsignificantly enhances the edge ratio by reducing fragmentation. Additionally,\nKubeDSM's adaptability in respecting QoS requirements while maximizing overall\nedge ratios is confirmed through different QoS scenarios.\n","authors":["Amirhossein Pashaeehir","Sina Shariati","Shayan Shafaghi","Manni Moghimi","Mahmoud Momtazpour"],"pdf_url":"https://arxiv.org/pdf/2501.07130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07959v2","updated":"2025-01-13T06:25:57Z","published":"2024-11-12T17:36:20Z","title":"On the Convergence of Continual Federated Learning Using Incrementally\n  Aggregated Gradients","summary":"  The holy grail of machine learning is to enable Continual Federated Learning\n(CFL) to enhance the efficiency, privacy, and scalability of AI systems while\nlearning from streaming data. The primary challenge of a CFL system is to\novercome global catastrophic forgetting, wherein the accuracy of the global\nmodel trained on new tasks declines on the old tasks. In this work, we propose\nContinual Federated Learning with Aggregated Gradients (C-FLAG), a novel\nreplay-memory based federated strategy consisting of edge-based gradient\nupdates on memory and aggregated gradients on the current data. We provide\nconvergence analysis of the C-FLAG approach which addresses forgetting and bias\nwhile converging at a rate of $O(1/\\sqrt{T})$ over $T$ communication rounds. We\nformulate an optimization sub-problem that minimizes catastrophic forgetting,\ntranslating CFL into an iterative algorithm with adaptive learning rates that\nensure seamless learning across tasks. We empirically show that C-FLAG\noutperforms several state-of-the-art baselines on both task and\nclass-incremental settings with respect to metrics such as accuracy and\nforgetting.\n","authors":["Satish Kumar Keshri","Nazreen Shah","Ranjitha Prasad"],"pdf_url":"https://arxiv.org/pdf/2411.07959v2.pdf","comment":"30 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.07056v1","updated":"2025-01-13T04:31:04Z","published":"2025-01-13T04:31:04Z","title":"Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs","summary":"  Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.\n","authors":["Jordi Wolfson-Pou","Jan Laukemann","Fabrizio Petrini"],"pdf_url":"https://arxiv.org/pdf/2501.07056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07036v1","updated":"2025-01-13T03:26:29Z","published":"2025-01-13T03:26:29Z","title":"A Simple Lower Bound for Set Agreement in Dynamic Networks","summary":"  Given a positive integer $k$, $k$-set agreement is the distributed task in\nwhich each process $i\\in [n]$ in a group of $n$ processing nodes starts with an\ninput value $x_i$ in the set $\\{0,\\dots,k\\}$, and must output a value $y_i$\nsuch that (1) for every $i \\in [n]$, $y_i$ is the input value of some process,\nand (2)$|\\{y_i : i\\in [n]\\}|\\leq k$. That is, at most $k$ different values in\ntotal must be outputted by the processes. The case $k=1$ correspond to (binary)\nconsensus, arguably the most studied problem in distributed computing. While\nlower bounds for consensus have been obtained for most of the standard\ndistributed computing models, the design of lower bounds for $k$-set agreement\nwith $k>1$ is notoriously known to be much more difficult, and remains open for\nmany models. The main techniques for designing lower bounds for k-set agreement\nwith $k>1$ use tools from algebraic topology.\n  The algebraic topology tools are difficult to manipulate, and require a lot\nof care for avoiding mistakes. This difficulty increases when the\ncommunications are mediated by a network of arbitrary structure. Recently, the\nKNOWALL model has been specifically designed as a first attempt to understand\nthe LOCAL model through the lens of algebraic topology, and Casta\\~neda et\nal.(2021) have designed lower bounds for $k$-set agreement in the KNOWALL\nmodel, with applications to dynamic networks.\n  In this work, we re-prove the same lower bound for $k$-set agreement in the\nKNOWALL model. This new proof stands out in its simplicity, which makes it\naccessible to a broader audience, and increases confidence in the result.\n","authors":["Pierre Fraigniaud","Minh Hang Nguyen","Ami Paz"],"pdf_url":"https://arxiv.org/pdf/2501.07036v1.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.19714v2","updated":"2025-01-13T02:43:47Z","published":"2024-11-29T14:02:00Z","title":"The Streetscape Application Services Stack (SASS): Towards a Distributed\n  Sensing Architecture for Urban Applications","summary":"  As urban populations grow, cities are becoming more complex, driving the\ndeployment of interconnected sensing systems to realize the vision of smart\ncities. These systems aim to improve safety, mobility, and quality of life\nthrough applications that integrate diverse sensors with real-time\ndecision-making. Streetscape applications-focusing on challenges like\npedestrian safety and adaptive traffic management-depend on managing\ndistributed, heterogeneous sensor data, aligning information across time and\nspace, and enabling real-time processing. These tasks are inherently complex\nand often difficult to scale. The Streetscape Application Services Stack (SASS)\naddresses these challenges with three core services: multimodal data\nsynchronization, spatiotemporal data fusion, and distributed edge computing. By\nstructuring these capabilities as clear, composable abstractions with clear\nsemantics, SASS allows developers to scale streetscape applications efficiently\nwhile minimizing the complexity of multimodal integration.\n  We evaluated SASS in two real-world testbed environments: a controlled\nparking lot and an urban intersection in a major U.S. city. These testbeds\nallowed us to test SASS under diverse conditions, demonstrating its practical\napplicability. The Multimodal Data Synchronization service reduced temporal\nmisalignment errors by 88%, achieving synchronization accuracy within 50\nmilliseconds. Spatiotemporal Data Fusion service improved detection accuracy\nfor pedestrians and vehicles by over 10%, leveraging multicamera integration.\nThe Distributed Edge Computing service increased system throughput by more than\nan order of magnitude. Together, these results show how SASS provides the\nabstractions and performance needed to support real-time, scalable urban\napplications, bridging the gap between sensing infrastructure and actionable\nstreetscape intelligence.\n","authors":["Navid Salami Pargoo","Mahshid Ghasemi","Shuren Xia","Mehmet Kerem Turkcan","Taqiya Ehsan","Chengbo Zang","Yuan Sun","Javad Ghaderi","Gil Zussman","Zoran Kostic","Jorge Ortiz"],"pdf_url":"https://arxiv.org/pdf/2411.19714v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.03060v6","updated":"2025-01-13T01:48:29Z","published":"2022-05-06T07:53:21Z","title":"Regular Model Checking Upside-Down: An Invariant-Based Approach","summary":"  Regular model checking is a technique for the verification of infinite-state\nsystems whose configurations can be represented as finite words over a suitable\nalphabet. The form we are studying applies to systems whose set of initial\nconfigurations is regular, and whose transition relation is captured by a\nlength-preserving transducer. To verify safety properties, regular model\nchecking iteratively computes automata recognizing increasingly larger regular\nsets of reachable configurations, and checks if they contain unsafe\nconfigurations. Since this procedure often does not terminate, acceleration,\nabstraction, and widening techniques have been developed to compute a regular\nsuperset of the reachable configurations.\n  In this paper, we develop a complementary procedure. Instead of approaching\nthe set of reachable configurations from below, we start with the set of all\nconfigurations and approach it from above. We use that the set of reachable\nconfigurations is equal to the intersection of all inductive invariants of the\nsystem. Since this intersection is non-regular in general, we introduce\nb-invariants, defined as those representable by CNF-formulas with at most b\nclauses. We prove that, for every $b\\geq0$, the intersection of all inductive\nb-invariants is regular, and we construct an automaton recognizing it. We show\nthat whether this automaton accepts some unsafe configuration is in EXPSPACE\nfor every $b\\geq0$, and PSPACE-complete for b=1. Finally, we study how large\nmust b be to prove safety properties of a number of benchmarks.\n","authors":["Javier Esparza","Michael Raskin","Christoph Welzel-Mohr"],"pdf_url":"https://arxiv.org/pdf/2205.03060v6.pdf","comment":null}]},"2025-01-12T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2501.06978v1","updated":"2025-01-12T23:30:30Z","published":"2025-01-12T23:30:30Z","title":"Towards a visually interpretable analysis of Two-Phase Locking\n  membership","summary":"  Two-phase locking (2PL) is a consolidated policy commonly adopted by Database\nManagement Systems to enforce serializability of a schedule. While the policy\nis well understood, both in its standard and in the strict version,\nautomatically deriving a suitable tabular/graphical analysis of schedules with\nrespect to 2PL is far from trivial, and requires several technicalities that do\nnot straightforwardly translate to visual cues. In this paper, we delve into\nthe details of the development of a tool for 2PL analysis.\n","authors":["Davide Martinenghi"],"pdf_url":"https://arxiv.org/pdf/2501.06978v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2501.06705v1","updated":"2025-01-12T04:15:40Z","published":"2025-01-12T04:15:40Z","title":"Quantum Data Sketches","summary":"  Recent advancements in quantum technologies, particularly in quantum sensing\nand simulation, have facilitated the generation and analysis of inherently\nquantum data. This progress underscores the necessity for developing efficient\nand scalable quantum data management strategies. This goal faces immense\nchallenges due to the exponential dimensionality of quantum data and its unique\nquantum properties such as no-cloning and measurement stochasticity.\nSpecifically, classical storage and manipulation of an arbitrary n-qubit\nquantum state requires exponential space and time. Hence, there is a critical\nneed to revisit foundational data management concepts and algorithms for\nquantum data. In this paper, we propose succinct quantum data sketches to\nsupport basic database operations such as search and selection. We view our\nwork as an initial step towards the development of quantum data management\nmodel, opening up many possibilities for future research in this direction.\n","authors":["Qin Zhang","Mohsen Heidari"],"pdf_url":"https://arxiv.org/pdf/2501.06705v1.pdf","comment":"34 pages"}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2501.02380v2","updated":"2025-01-12T20:08:46Z","published":"2025-01-04T20:59:34Z","title":"Reciprocating Locks","summary":"  We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.\n","authors":["Dave Dice","Alex Kogan"],"pdf_url":"https://arxiv.org/pdf/2501.02380v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06872v1","updated":"2025-01-12T17:01:40Z","published":"2025-01-12T17:01:40Z","title":"On Optimizing Locality of Graph Transposition on Modern Architectures","summary":"  This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.\n","authors":["Mohsen Koohi Esfahani","Hans Vandierendonck"],"pdf_url":"https://arxiv.org/pdf/2501.06872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06856v1","updated":"2025-01-12T16:14:21Z","published":"2025-01-12T16:14:21Z","title":"CoCoI: Distributed Coded Inference System for Straggler Mitigation","summary":"  Convolutional neural networks (CNNs) are widely applied in real-time\napplications on resource-constrained devices. To accelerate CNN inference,\nprior works proposed to distribute the inference workload across multiple\ndevices. However, they did not address stragglers and device failures in\ndistributed inference, which is challenging due to the devices' time-varying\nand possibly unknown computation/communication capacities. To address this, we\npropose a distributed coded inference system, called CoCoI. It splits the\nconvolutional layers of CNN, considering the data dependency of\nhigh-dimensional inputs and outputs, and then adapts coding schemes to generate\ntask redundancy. With CoCoI, the inference results can be determined once a\nsubset of devices complete their subtasks, improving robustness against\nstragglers and failures. To theoretically analyze the tradeoff between\nredundancy and subtask workload, we formulate an optimal splitting problem to\nminimize the expected inference latency. Despite its non-convexity, we\ndetermine an approximate strategy with minor errors, and prove that CoCoI\noutperforms uncoded benchmarks. For performance evaluation, we build a testbed\nwith Raspberry Pi 4Bs. The experimental results show that the approximate\nstrategy closely matches the optimal solution. When compared with uncoded\nbenchmarks, CoCoI reduces inference latency by up to 34.2% in the presence of\nstragglers and device failures.\n","authors":["Xing Liu","Chao Huang","Ming Tang"],"pdf_url":"https://arxiv.org/pdf/2501.06856v1.pdf","comment":"11 pages, and the last 3 are appendix"},{"id":"http://arxiv.org/abs/2501.06780v1","updated":"2025-01-12T11:31:25Z","published":"2025-01-12T11:31:25Z","title":"COMPASS: A Compiler Framework for Resource-Constrained Crossbar-Array\n  Based In-Memory Deep Learning Accelerators","summary":"  Recently, crossbar array based in-memory accelerators have been gaining\ninterest due to their high throughput and energy efficiency. While software and\ncompiler support for the in-memory accelerators has also been introduced, they\nare currently limited to the case where all weights are assumed to be on-chip.\nThis limitation becomes apparent with the significantly increasing network\nsizes compared to the in-memory footprint.\n  Weight replacement schemes are essential to address this issue. We propose\nCOMPASS, a compiler framework for resource-constrained crossbar-based\nprocessing-in-memory (PIM) deep neural network (DNN) accelerators. COMPASS is\nspecially targeted for networks that exceed the capacity of PIM crossbar\narrays, necessitating access to external memories. We propose an algorithm to\ndetermine the optimal partitioning that divides the layers so that each\npartition can be accelerated on chip. Our scheme takes into account the data\ndependence between layers, core utilization, and the number of write\ninstructions to minimize latency, memory accesses, and improve energy\nefficiency. Simulation results demonstrate that COMPASS can accommodate much\nmore networks using a minimal memory footprint, while improving throughput by\n1.78X and providing 1.28X savings in energy-delay product (EDP) over baseline\npartitioning methods.\n","authors":["Jihoon Park","Jeongin Choe","Dohyun Kim","Jae-Joon Kim"],"pdf_url":"https://arxiv.org/pdf/2501.06780v1.pdf","comment":"Accepted IEEE DATE 2025"},{"id":"http://arxiv.org/abs/2412.12370v3","updated":"2025-01-12T05:17:53Z","published":"2024-12-16T21:56:01Z","title":"Scam Detection for Ethereum Smart Contracts: Leveraging Graph\n  Representation Learning for Secure Blockchain","summary":"  Due to the increasing abuse of fraudulent activities that result in\nsignificant financial and reputational harm, Ethereum smart contracts face a\nsignificant problem in detecting fraud. Existing monitoring methods typically\nrely on lease code analysis or physically extracted features, which suffer from\nscalability and adaptability limitations. In this study, we use graph\nrepresentation learning to observe purchase trends and find fraudulent deals.\nWe can achieve powerful categorisation performance by using innovative machine\nlearning versions and transforming Ethereum invoice data into graph structures.\nOur method addresses label imbalance through SMOTE-ENN techniques and evaluates\nmodels like Multi-Layer Perceptron ( MLP ) and Graph Convolutional Networks (\nGCN). Experimental results show that the MLP type surpasses the GCN in this\nenvironment, with domain-specific assessments closely aligned with real-world\nassessments. This study provides a scalable and efficient way to improve\nEthereum's ecosystem's confidence and security.\n","authors":["Yihong Jin","Ze Yang"],"pdf_url":"https://arxiv.org/pdf/2412.12370v3.pdf","comment":"Accepted to BDICN 2025"},{"id":"http://arxiv.org/abs/2501.06709v1","updated":"2025-01-12T04:29:39Z","published":"2025-01-12T04:29:39Z","title":"Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management","summary":"  Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.\n","authors":["Liu Qianli","Hong Zicong","Chen Fahao","Li Peng","Guo Song"],"pdf_url":"https://arxiv.org/pdf/2501.06709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06706v1","updated":"2025-01-12T04:17:39Z","published":"2025-01-12T04:17:39Z","title":"AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling\n  Autonomous Clouds","summary":"  AI for IT Operations (AIOps) aims to automate complex operational tasks, such\nas fault localization and root cause analysis, to reduce human workload and\nminimize customer impact. While traditional DevOps tools and AIOps algorithms\noften focus on addressing isolated operational tasks, recent advances in Large\nLanguage Models (LLMs) and AI agents are revolutionizing AIOps by enabling\nend-to-end and multitask automation. This paper envisions a future where AI\nagents autonomously manage operational tasks throughout the entire incident\nlifecycle, leading to self-healing cloud systems, a paradigm we term AgentOps.\nRealizing this vision requires a comprehensive framework to guide the design,\ndevelopment, and evaluation of these agents. To this end, we present AIOPSLAB,\na framework that not only deploys microservice cloud environments, injects\nfaults, generates workloads, and exports telemetry data but also orchestrates\nthese components and provides interfaces for interacting with and evaluating\nagents. We discuss the key requirements for such a holistic framework and\ndemonstrate how AIOPSLAB can facilitate the evaluation of next-generation AIOps\nagents. Through evaluations of state-of-the-art LLM agents within the benchmark\ncreated by AIOPSLAB, we provide insights into their capabilities and\nlimitations in handling complex operational tasks in cloud environments.\n","authors":["Yinfang Chen","Manish Shetty","Gagan Somashekar","Minghua Ma","Yogesh Simmhan","Jonathan Mace","Chetan Bansal","Rujia Wang","Saravan Rajmohan"],"pdf_url":"https://arxiv.org/pdf/2501.06706v1.pdf","comment":null}]},"2025-01-11T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2501.06659v1","updated":"2025-01-11T23:07:04Z","published":"2025-01-11T23:07:04Z","title":"TWIX: Automatically Reconstructing Structured Data from Templatized\n  Documents","summary":"  Many documents, that we call templatized documents, are programmatically\ngenerated by populating fields in a visual template. Effective data extraction\nfrom these documents is crucial to supporting downstream analytical tasks.\nCurrent data extraction tools often struggle with complex document layouts,\nincur high latency and/or cost on large datasets, and often require significant\nhuman effort, when extracting tables or values given user-specified fields from\ndocuments. The key insight of our tool, TWIX, is to predict the underlying\ntemplate used to create such documents, modeling the visual and structural\ncommonalities across documents. Data extraction based on this predicted\ntemplate provides a more principled, accurate, and efficient solution at a low\ncost. Comprehensive evaluations on 34 diverse real-world datasets show that\nuncovering the template is crucial for data extraction from templatized\ndocuments. TWIX achieves over 90% precision and recall on average,\noutperforming tools from industry: Textract and Azure Document Intelligence,\nand vision-based LLMs like GPT-4-Vision, by over 25% in precision and recall.\nTWIX scales easily to large datasets and is 734X faster and 5836X cheaper than\nvision-based LLMs for extracting data from a large document collection with 817\npages.\n","authors":["Yiming Lin","Mawil Hasan","Rohan Kosalge","Alvin Cheung","Aditya G. Parameswaran"],"pdf_url":"https://arxiv.org/pdf/2501.06659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06189v3","updated":"2025-01-11T16:16:04Z","published":"2024-12-09T03:57:03Z","title":"Fast Matrix Multiplication meets the Submodular Width","summary":"  One fundamental question in database theory is the following: Given a Boolean\nconjunctive query Q, what is the best complexity for computing the answer to Q\nin terms of the input database size N? When restricted to the class of\ncombinatorial algorithms, it is known that the best known complexity for any\nquery Q is captured by the submodular width of Q. However, beyond combinatorial\nalgorithms, certain queries are known to admit faster algorithms that often\ninvolve a clever combination of fast matrix multiplication and data\npartitioning. Nevertheless, there is no systematic way to derive and analyze\nthe complexity of such algorithms for arbitrary queries Q.\n  In this work, we introduce a general framework that captures the best\ncomplexity for answering any Boolean conjunctive query Q using matrix\nmultiplication. Our framework unifies both combinatorial and non-combinatorial\ntechniques under the umbrella of information theory. It generalizes the notion\nof submodular width to a new stronger notion called the omega-submodular width\nthat naturally incorporates the power of fast matrix multiplication. We\ndescribe a matching algorithm that computes the answer to any query Q in time\ncorresponding to the omega-submodular width of Q. We show that our framework\nrecovers the best known complexities for Boolean queries that have been studied\nin the literature, to the best of our knowledge, and also discovers new\nalgorithms for some classes of queries that improve upon the best known\ncomplexities.\n","authors":["Mahmoud Abo-Khamis","Xiao Hu","Dan Suciu"],"pdf_url":"https://arxiv.org/pdf/2412.06189v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06570v1","updated":"2025-01-11T15:39:17Z","published":"2025-01-11T15:39:17Z","title":"Aster: Enhancing LSM-structures for Scalable Graph Database","summary":"  There is a proliferation of applications requiring the management of\nlarge-scale, evolving graphs under workloads with intensive graph updates and\nlookups. Driven by this challenge, we introduce Poly-LSM, a high-performance\nkey-value storage engine for graphs with the following novel techniques: (1)\nPoly-LSM is embedded with a new design of graph-oriented LSM-tree structure\nthat features a hybrid storage model for concisely and effectively storing\ngraph data. (2) Poly-LSM utilizes an adaptive mechanism to handle edge\ninsertions and deletions on graphs with optimized I/O efficiency. (3) Poly-LSM\nexploits the skewness of graph data to encode the key-value entries. Building\nupon this foundation, we further implement Aster, a robust and versatile graph\ndatabase that supports Gremlin query language facilitating various graph\napplications. In our experiments, we compared Aster against several mainstream\nreal-world graph databases. The results demonstrate that Aster outperforms all\nbaseline graph databases, especially on large-scale graphs. Notably, on the\nbillion-scale Twitter graph dataset, Aster achieves up to 17x throughput\nimprovement compared to the best-performing baseline graph system.\n","authors":["Dingheng Mo","Junfeng Liu","Fan Wang","Siqiang Luo"],"pdf_url":"https://arxiv.org/pdf/2501.06570v1.pdf","comment":"Accepted by SIGMOD 2025"},{"id":"http://arxiv.org/abs/2403.12605v4","updated":"2025-01-11T15:09:13Z","published":"2024-03-19T10:14:48Z","title":"Online Marketplace: A Benchmark for Data Management in Microservices","summary":"  Microservice architectures have become a popular approach for designing\nscalable distributed applications. Despite their extensive use in industrial\nsettings for over a decade, there is limited understanding of the data\nmanagement challenges that arise in these applications. Consequently, it has\nbeen difficult to advance data system technologies that effectively support\nmicroservice applications. To fill this gap, we present Online Marketplace, a\nmicroservice benchmark that highlights core data management challenges that\nexisting benchmarks fail to address. These challenges include transaction\nprocessing, query processing, event processing, constraint enforcement, and\ndata replication. We have defined criteria for various data management issues\nto enable proper comparison across data systems and platforms.\n  Through case studies with state-of-the-art data platforms, we discuss the\nissues encountered while implementing and meeting Online Marketplace's\ncriteria. By capturing the overhead of meeting the key data management\nrequirements that are overlooked by existing benchmarks, we gain actionable\ninsights into the experimental platforms. This highlights the significance of\nOnline Marketplace in advancing future data systems to meet the needs of\nmicroservice practitioners.\n","authors":["Rodrigo Laigner","Zhexiang Zhang","Yijian Liu","Leonardo Freitas Gomes","Yongluan Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.12605v4.pdf","comment":"Version accepted at SIGMOD'25"}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2501.06650v1","updated":"2025-01-11T22:20:20Z","published":"2025-01-11T22:20:20Z","title":"SafeSplit: A Novel Defense Against Client-Side Backdoor Attacks in Split\n  Learning","summary":"  Split Learning (SL) is a distributed deep learning approach enabling multiple\nclients and a server to collaboratively train and infer on a shared deep neural\nnetwork (DNN) without requiring clients to share their private local data. The\nDNN is partitioned in SL, with most layers residing on the server and a few\ninitial layers and inputs on the client side. This configuration allows\nresource-constrained clients to participate in training and inference. However,\nthe distributed architecture exposes SL to backdoor attacks, where malicious\nclients can manipulate local datasets to alter the DNN's behavior. Existing\ndefenses from other distributed frameworks like Federated Learning are not\napplicable, and there is a lack of effective backdoor defenses specifically\ndesigned for SL.\n  We present SafeSplit, the first defense against client-side backdoor attacks\nin Split Learning (SL). SafeSplit enables the server to detect and filter out\nmalicious client behavior by employing circular backward analysis after a\nclient's training is completed, iteratively reverting to a trained checkpoint\nwhere the model under examination is found to be benign. It uses a two-fold\nanalysis to identify client-induced changes and detect poisoned models. First,\na static analysis in the frequency domain measures the differences in the\nlayer's parameters at the server. Second, a dynamic analysis introduces a novel\nrotational distance metric that assesses the orientation shifts of the server's\nlayer parameters during training. Our comprehensive evaluation across various\ndata distributions, client counts, and attack scenarios demonstrates the high\nefficacy of this dual analysis in mitigating backdoor attacks while preserving\nmodel utility.\n","authors":["Phillip Rieger","Alessandro Pegoraro","Kavita Kumari","Tigist Abera","Jonathan Knauer","Ahmad-Reza Sadeghi"],"pdf_url":"https://arxiv.org/pdf/2501.06650v1.pdf","comment":"To appear at NDSS 2025; 18 pages, 6 Tables, and 11 figures"},{"id":"http://arxiv.org/abs/2303.09287v5","updated":"2025-01-11T21:09:48Z","published":"2023-03-16T13:09:48Z","title":"Semitopology: a topological approach to decentralised collaborative\n  action","summary":"  We introduce semitopology, a generalisation of point-set topology that\nremoves the restriction that intersections of open sets need necessarily be\nopen. The intuition is that points represent participants in a decentralised\nsystem, and open sets represent collections of participants that collectively\nhave the authority to collaborate to update their local state; we call this an\nactionable coalition.\n  Examples of actionable coalition include: majority stakes in proof-of-stake\nblockchains; communicating peers in peer-to-peer networks; and even pedestrians\nworking together to not bump into one another in the street. Where actionable\ncoalitions exist, they have in common that: collaborations are local (updating\nthe states of the participants in the coalition, but not immediately those of\nthe whole system); collaborations are voluntary (up to and including breaking\nrules); participants may be heterogeneous in their computing power or in their\ngoals (not all pedestrians want to go to the same place); participants can\nchoose with whom to collaborate; and they are not assumed subject to permission\nor synchronisation by a central authority.\n  We develop a topology-flavoured mathematics that goes some way to explaining\nhow and why these complex decentralised systems can exhibit order, and gives us\nnew ways to understand existing practical implementations.\n","authors":["Murdoch Gabbay"],"pdf_url":"https://arxiv.org/pdf/2303.09287v5.pdf","comment":"See also arXiv:2310.00956, which takes a point-free algebraic\n  approach (\"semiframes\"). This update updates metadata and content"},{"id":"http://arxiv.org/abs/2501.06589v1","updated":"2025-01-11T17:06:30Z","published":"2025-01-11T17:06:30Z","title":"Ladder-residual: parallelism-aware architecture for accelerating large\n  model inference with communication overlapping","summary":"  Large language model inference is both memory-intensive and time-consuming,\noften requiring distributed algorithms to efficiently scale. Various model\nparallelism strategies are used in multi-gpu training and inference to\npartition computation across multiple devices, reducing memory load and\ncomputation time. However, using model parallelism necessitates communication\nof information between GPUs, which has been a major bottleneck and limits the\ngains obtained by scaling up the number of devices. We introduce Ladder\nResidual, a simple architectural modification applicable to all residual-based\nmodels that enables straightforward overlapping that effectively hides the\nlatency of communication. Our insight is that in addition to systems\noptimization, one can also redesign the model architecture to decouple\ncommunication from computation. While Ladder Residual can allow\ncommunication-computation decoupling in conventional parallelism patterns, we\nfocus on Tensor Parallelism in this paper, which is particularly bottlenecked\nby its heavy communication. For a Transformer model with 70B parameters,\napplying Ladder Residual to all its layers can achieve 30% end-to-end wall\nclock speed up at inference time with TP sharding over 8 devices. We refer the\nresulting Transformer model as the Ladder Transformer. We train a 1B and 3B\nLadder Transformer from scratch and observe comparable performance to a\nstandard dense transformer baseline. We also show that it is possible to\nconvert parts of the Llama-3.1 8B model to our Ladder Residual architecture\nwith minimal accuracy degradation by only retraining for 3B tokens.\n","authors":["Muru Zhang","Mayank Mishra","Zhongzhu Zhou","William Brandon","Jue Wang","Yoon Kim","Jonathan Ragan-Kelley","Shuaiwen Leon Song","Ben Athiwaratkun","Tri Dao"],"pdf_url":"https://arxiv.org/pdf/2501.06589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17918v4","updated":"2025-01-11T15:26:48Z","published":"2024-06-25T20:00:32Z","title":"GraphSnapShot: Caching Local Structure for Fast Graph Learning","summary":"  In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.\n","authors":["Dong Liu","Roger Waleffe","Meng Jiang","Shivaram Venkataraman"],"pdf_url":"https://arxiv.org/pdf/2406.17918v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06531v1","updated":"2025-01-11T12:41:46Z","published":"2025-01-11T12:41:46Z","title":"Stingray: Fast Concurrent Transactions Without Consensus","summary":"  Recent advances have improved the throughput and latency of blockchains by\nprocessing transactions accessing different parts of the state concurrently.\nHowever, these systems are unable to concurrently process (a) transactions\naccessing the same state, even if they are (almost) commutative, e.g., payments\nmuch smaller than an account's balance, and (b) multi-party transactions, e.g.,\nasset swaps. Moreover, they are slow to recover from contention, requiring\nonce-in-a-day synchronization. We present Stingray, a novel blockchain\narchitecture that addresses these limitations. The key conceptual contributions\nare a replicated bounded counter that processes (almost) commutative\ntransactions concurrently, and a FastUnlock protocol that uses a fallback\nconsensus protocol for fast contention recovery. We prove Stingray's security\nin an asynchronous network with Byzantine faults and demonstrate on a global\ntestbed that Stingray achieves 10,000 times the throughput of prior systems for\ncommutative workloads.\n","authors":["Srivatsan Sridhar","Alberto Sonnino","Lefteris Kokoris-Kogias"],"pdf_url":"https://arxiv.org/pdf/2501.06531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12117v2","updated":"2025-01-11T10:17:25Z","published":"2024-07-16T18:59:49Z","title":"Efficiently Training 7B LLM with 1 Million Sequence Length on 8 GPUs","summary":"  Nowadays, Large Language Models (LLMs) have been trained using extended\ncontext lengths to foster more creative applications. However, long context\ntraining poses great challenges considering the constraint of GPU memory. It\nnot only leads to substantial activation memory consumption during training,\nbut also incurs considerable memory fragmentation. To facilitate long context\ntraining, existing frameworks have adopted strategies such as recomputation and\nvarious forms of parallelisms. Nevertheless, these techniques rely on redundant\ncomputation or extensive communication, resulting in low Model FLOPS\nUtilization (MFU). In this paper, we propose MEMO, a novel LLM training\nframework designed for fine-grained activation memory management. Given the\nquadratic scaling of computation and linear scaling of memory with sequence\nlengths when using FlashAttention, we offload memory-consuming activations to\nCPU memory after each layer's forward pass and fetch them during the backward\npass. To maximize the swapping of activations without hindering computation,\nand to avoid exhausting limited CPU memory, we implement a token-wise\nactivation recomputation and swapping mechanism. Furthermore, we tackle the\nmemory fragmentation issue by employing a bi-level Mixed Integer Programming\n(MIP) approach, optimizing memory reuse across transformer layers. Empirical\nresults demonstrate that MEMO achieves an average of 1.97x and 1.80x MFU\ncompared to Megatron-LM and DeepSpeed, respectively. This improvement is\nattributed to MEMO's ability to minimize memory fragmentation, reduce\nrecomputation and intensive communication, and circumvent the delays associated\nwith the memory reorganization process due to fragmentation. By leveraging\nfine-grained activation memory management, MEMO facilitates efficient training\nof 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU\nof 52.30%.\n","authors":["Pinxue Zhao","Hailin Zhang","Fangcheng Fu","Xiaonan Nie","Qibin Liu","Fang Yang","Yuanbo Peng","Dian Jiao","Shuaipeng Li","Jinbao Xue","Yangyu Tao","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2407.12117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06120v2","updated":"2025-01-11T08:39:49Z","published":"2024-12-09T00:54:00Z","title":"Lightweight Federated Learning with Differential Privacy and Straggler\n  Resilience","summary":"  Federated learning (FL) enables collaborative model training through model\nparameter exchanges instead of raw data. To avoid potential inference attacks\nfrom exchanged parameters, differential privacy (DP) offers rigorous guarantee\nagainst various attacks. However, conventional methods of ensuring DP by adding\nlocal noise alone often result in low training accuracy. Combining secure\nmulti-party computation (SMPC) with DP, while improving the accuracy, incurs\nhigh communication and computation overheads as well as straggler\nvulnerability, in either client-to-server or client-to-client links. In this\npaper, we propose LightDP-FL, a novel lightweight scheme that ensures provable\nDP against untrusted peers and server, while maintaining straggler resilience,\nlow overheads and high training accuracy. Our scheme incorporates both\nindividual and pairwise noise into each client's parameter, which can be\nimplemented with minimal overheads. Given the uncertain straggler and colluder\nsets, we utilize the upper bound on the numbers of stragglers and colluders to\nprove sufficient noise variance conditions to ensure DP in the worst case.\nMoreover, we optimize the expected convergence bound to ensure accuracy\nperformance by flexibly controlling the noise variances. Using the CIFAR-10\ndataset, our experimental results demonstrate that LightDP-FL achieves faster\nconvergence and stronger straggler resilience compared to baseline methods of\nthe same DP level.\n","authors":["Shu Hong","Xiaojun Lin","Lingjie Duan"],"pdf_url":"https://arxiv.org/pdf/2412.06120v2.pdf","comment":"To appear at IEEE International Conference on Computer Communications\n  (INFOCOM) 2025"},{"id":"http://arxiv.org/abs/2404.08973v3","updated":"2025-01-11T06:32:11Z","published":"2024-04-13T11:40:05Z","title":"PraFFL: A Preference-Aware Scheme in Fair Federated Learning","summary":"  Fairness in federated learning has emerged as a critical concern, aiming to\ndevelop an unbiased model among groups (e.g., male or female) of diverse\nsensitive features. However, there is a trade-off between model performance and\nfairness, i.e., improving model fairness will decrease model performance.\nExisting approaches have characterized such a trade-off by introducing\nhyperparameters to quantify client's preferences for model fairness and model\nperformance. Nevertheless, these approaches are limited to scenarios where each\nclient has only a single pre-defined preference, and fail to work in practical\nsystems where each client generally has multiple preferences. To this end, we\npropose a Preference-aware scheme in Fair Federated Learning (called PraFFL) to\ngenerate preference-specific models in real time. PraFFL can adaptively adjust\nthe model based on each client's preferences to meet their needs. We\ntheoretically prove that PraFFL can offer the optimal model tailored to an\narbitrary preference of each client, and show its linear convergence.\nExperimental results show that our proposed PraFFL outperforms six fair\nfederated learning algorithms in terms of the model's capability of adapting to\nclients' different preferences. Our implementation is available at\nhttps://github.com/rG223/PraFFL.\n","authors":["Rongguang Ye","Wei-Bin Kou","Ming Tang"],"pdf_url":"https://arxiv.org/pdf/2404.08973v3.pdf","comment":"Accepted by KDD 2025"}]},"2025-01-10T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2406.00251v2","updated":"2025-01-10T20:47:27Z","published":"2024-06-01T01:15:12Z","title":"Measures in SQL","summary":"  SQL has attained widespread adoption, but Business Intelligence tools still\nuse their own higher level languages based upon a multidimensional paradigm.\nComposable calculations are what is missing from SQL, and we propose a new kind\nof column, called a measure, that attaches a calculation to a table. Like\nregular tables, tables with measures are composable and closed when used in\nqueries. SQL-with-measures has the power, conciseness and reusability of\nmultidimensional languages but retains SQL semantics. Measure invocations can\nbe expanded in place to simple, clear SQL. To define the evaluation semantics\nfor measures, we introduce context-sensitive expressions (a way to evaluate\nmultidimensional expressions that is consistent with existing SQL semantics), a\nconcept called evaluation context, and several operations for setting and\nmodifying the evaluation context.\n","authors":["Julian Hyde","John Fremlin"],"pdf_url":"https://arxiv.org/pdf/2406.00251v2.pdf","comment":"To be published in SIGMOD-Companion 24, June 9-15, 2024, Santiago,\n  AA, Chile; 10 pages; updated with corrections as of 2024/05/31, and for\n  formatting as of 2025/01/10"},{"id":"http://arxiv.org/abs/2407.14953v2","updated":"2025-01-10T20:40:19Z","published":"2024-07-20T18:12:11Z","title":"AgileDART: An Agile and Scalable Edge Stream Processing Engine","summary":"  Edge applications generate a large influx of sensor data on massive scales,\nand these massive data streams must be processed shortly to derive actionable\nintelligence. However, traditional data processing systems are not well-suited\nfor these edge applications as they often do not scale well with a large number\nof concurrent stream queries, do not support low-latency processing under\nlimited edge computing resources, and do not adapt to the level of\nheterogeneity and dynamicity commonly present in edge computing environments.\nAs such, we present AgileDart, an agile and scalable edge stream processing\nengine that enables fast stream processing of many concurrently running\nlow-latency edge applications' queries at scale in dynamic, heterogeneous edge\nenvironments. The novelty of our work lies in a dynamic dataflow abstraction\nthat leverages distributed hash table-based peer-to-peer overlay networks to\nautonomously place, chain, and scale stream operators to reduce query\nlatencies, adapt to workload variations, and recover from failures and a\nbandit-based path planning model that re-plans the data shuffling paths to\nadapt to unreliable and heterogeneous edge networks. We show that AgileDart\noutperforms Storm and EdgeWise on query latency and significantly improves\nscalability and adaptability when processing many real-world edge stream\napplications' queries.\n","authors":["Cheng-Wei Ching","Xin Chen","Chaeeun Kim","Tongze Wang","Dong Chen","Dilma Da Silva","Liting Hu"],"pdf_url":"https://arxiv.org/pdf/2407.14953v2.pdf","comment":"To appear in IEEE Transactions on Mobile Computing (TMC); 18 pages\n  for the main paper and 5 pages for the appendices"},{"id":"http://arxiv.org/abs/2501.02278v2","updated":"2025-01-10T15:41:52Z","published":"2025-01-04T13:02:28Z","title":"An experimental comparison of tree-data structures for connectivity\n  queries on fully-dynamic undirected graphs (Extended Version)","summary":"  During the past decades significant efforts have been made to propose data\nstructures for answering connectivity queries on fully dynamic graphs, i.e.,\ngraphs with frequent insertions and deletions of edges. However, a\ncomprehensive understanding of how these data structures perform in practice is\nmissing, since not all of them have been implemented, let alone evaluated\nexperimentally. We provide reference implementations for the proposed data\nstructures and experimentally evaluate them on a wide range of graphs. Our\nfindings show that the current solutions are not ready to be deployed in\nsystems as is, as every data structure has critical weaknesses when used in\npractice. Key limitations that must be overcome are the space and time overhead\nincurred by balanced data structures, the degeneration of the runtime of\nspace-efficient data structures in worst case scenarios, and the maintenance\ncosts for balanced data structures. We detail our findings in the experimental\nevaluation and provide recommendations for implementing robust solutions for\nanswering connectivity queries on dynamic graphs.\n","authors":["Qing Chen","Michael H. Böhlen","Sven Helmer"],"pdf_url":"https://arxiv.org/pdf/2501.02278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04190v2","updated":"2025-01-10T01:46:58Z","published":"2025-01-07T23:57:10Z","title":"Partition Constraints for Conjunctive Queries: Bounds and Worst-Case\n  Optimal Joins","summary":"  In the last decade, various works have used statistics on relations to\nimprove both the theory and practice of conjunctive query execution. Starting\nwith the AGM bound which took advantage of relation sizes, later works\nincorporated statistics like functional dependencies and degree constraints.\nEach new statistic prompted work along two lines; bounding the size of\nconjunctive query outputs and worst-case optimal join algorithms. In this work,\nwe continue in this vein by introducing a new statistic called a\n\\emph{partition constraint}. This statistic captures latent structure within\nrelations by partitioning them into sub-relations which each have much tighter\ndegree constraints. We show that this approach can both refine existing\ncardinality bounds and improve existing worst-case optimal join algorithms.\n","authors":["Kyle Deeds","Timo Camillo Merkl"],"pdf_url":"https://arxiv.org/pdf/2501.04190v2.pdf","comment":null}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2407.14953v2","updated":"2025-01-10T20:40:19Z","published":"2024-07-20T18:12:11Z","title":"AgileDART: An Agile and Scalable Edge Stream Processing Engine","summary":"  Edge applications generate a large influx of sensor data on massive scales,\nand these massive data streams must be processed shortly to derive actionable\nintelligence. However, traditional data processing systems are not well-suited\nfor these edge applications as they often do not scale well with a large number\nof concurrent stream queries, do not support low-latency processing under\nlimited edge computing resources, and do not adapt to the level of\nheterogeneity and dynamicity commonly present in edge computing environments.\nAs such, we present AgileDart, an agile and scalable edge stream processing\nengine that enables fast stream processing of many concurrently running\nlow-latency edge applications' queries at scale in dynamic, heterogeneous edge\nenvironments. The novelty of our work lies in a dynamic dataflow abstraction\nthat leverages distributed hash table-based peer-to-peer overlay networks to\nautonomously place, chain, and scale stream operators to reduce query\nlatencies, adapt to workload variations, and recover from failures and a\nbandit-based path planning model that re-plans the data shuffling paths to\nadapt to unreliable and heterogeneous edge networks. We show that AgileDart\noutperforms Storm and EdgeWise on query latency and significantly improves\nscalability and adaptability when processing many real-world edge stream\napplications' queries.\n","authors":["Cheng-Wei Ching","Xin Chen","Chaeeun Kim","Tongze Wang","Dong Chen","Dilma Da Silva","Liting Hu"],"pdf_url":"https://arxiv.org/pdf/2407.14953v2.pdf","comment":"To appear in IEEE Transactions on Mobile Computing (TMC); 18 pages\n  for the main paper and 5 pages for the appendices"},{"id":"http://arxiv.org/abs/2501.05450v2","updated":"2025-01-10T18:58:11Z","published":"2025-01-09T18:59:56Z","title":"Decentralized Diffusion Models","summary":"  Large-scale AI model training divides work across thousands of GPUs, then\nsynchronizes gradients across them at each step. This incurs a significant\nnetwork burden that only centralized, monolithic clusters can support, driving\nup infrastructure costs and straining power systems. We propose Decentralized\nDiffusion Models, a scalable framework for distributing diffusion model\ntraining across independent clusters or datacenters by eliminating the\ndependence on a centralized, high-bandwidth networking fabric. Our method\ntrains a set of expert diffusion models over partitions of the dataset, each in\nfull isolation from one another. At inference time, the experts ensemble\nthrough a lightweight router. We show that the ensemble collectively optimizes\nthe same objective as a single model trained over the whole dataset. This means\nwe can divide the training burden among a number of \"compute islands,\" lowering\ninfrastructure costs and improving resilience to localized GPU failures.\nDecentralized diffusion models empower researchers to take advantage of\nsmaller, more cost-effective and more readily available compute like on-demand\nGPU nodes rather than central integrated systems. We conduct extensive\nexperiments on ImageNet and LAION Aesthetics, showing that decentralized\ndiffusion models FLOP-for-FLOP outperform standard diffusion models. We finally\nscale our approach to 24 billion parameters, demonstrating that high-quality\ndiffusion models can now be trained with just eight individual GPU nodes in\nless than a week.\n","authors":["David McAllister","Matthew Tancik","Jiaming Song","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2501.05450v2.pdf","comment":"Project webpage: https://decentralizeddiffusion.github.io/"},{"id":"http://arxiv.org/abs/2501.06175v1","updated":"2025-01-10T18:55:07Z","published":"2025-01-10T18:55:07Z","title":"Batched DGEMMs for scientific codes running on long vector architectures","summary":"  In this work, we evaluate the performance of SeisSol, a simulator of seismic\nwave phenomena and earthquake dynamics, on a RISC-V-based system utilizing a\nvector processing unit. We focus on GEMM libraries and address their limited\nability to leverage long vector architectures by developing a batched DGEMM\nlibrary in plain C. This library achieves speedups ranging from approximately\n3.5x to 32.6x compared to the reference implementation. We then integrate the\nbatched approach into the SeisSol application, ensuring portability across\ndifferent CPU architectures. Lastly, we demonstrate that our implementation is\nportable to an Intel CPU, resulting in improved execution times in most cases.\n","authors":["Fabio Banchelli","Marta Garcia-Gasulla","Filippo Mantovani"],"pdf_url":"https://arxiv.org/pdf/2501.06175v1.pdf","comment":"Accepted at the First PPAM Workshop on RISC-V (PPAM24)"},{"id":"http://arxiv.org/abs/2501.06128v1","updated":"2025-01-10T17:33:18Z","published":"2025-01-10T17:33:18Z","title":"Benchmarking Different Application Types across Heterogeneous Cloud\n  Compute Services","summary":"  Infrastructure as a Service (IaaS) clouds have become the predominant\nunderlying infrastructure for the operation of modern and smart technology.\nIaaS clouds have proven to be useful for multiple reasons such as reduced\ncosts, increased speed and efficiency, and better reliability and scalability.\nCompute services offered by such clouds are heterogeneous -- they offer a set\nof architecturally diverse machines that fit efficiently executing different\nworkloads. However, there has been little study to shed light on the\nperformance of popular application types on these heterogeneous compute servers\nacross different clouds. Such a study can help organizations to optimally (in\nterms of cost, latency, throughput, consumed energy, carbon footprint, etc.)\nemploy cloud compute services. At HPCC lab, we have focused on such benchmarks\nin different research projects and, in this report, we curate those benchmarks\nin a single document to help other researchers in the community using them.\nSpecifically, we introduce our benchmarks datasets for three application types\nin three different domains, namely: Deep Neural Networks (DNN) Inference for\nindustrial applications, Machine Learning (ML) Inference for assistive\ntechnology applications, and video transcoding for multimedia use cases.\n","authors":["Nivedhitha Duggi","Masoud Rafiei","Mohsen Amini Salehi"],"pdf_url":"https://arxiv.org/pdf/2501.06128v1.pdf","comment":"Technical Report. arXiv admin note: text overlap with\n  arXiv:2011.11711 by other authors"},{"id":"http://arxiv.org/abs/2412.14975v3","updated":"2025-01-10T16:27:08Z","published":"2024-12-19T15:50:47Z","title":"Minimizing speculation overhead in a parallel recognizer for regular\n  texts","summary":"  Speculative data-parallel algorithms for language recognition have been\nwidely experimented for various types of finite-state automata (FA),\ndeterministic (DFA) and nondeterministic (NFA), often derived from regular\nexpressions (RE). Such an algorithm cuts the input string into chunks,\nindependently recognizes each chunk in parallel by means of identical FAs, and\nat last joins the chunk results and checks overall consistency. In chunk\nrecognition, it is necessary to speculatively start the FAs in any state, thus\ncausing an overhead that reduces the speedup compared to a serial algorithm.\nExisting data-parallel DFA-based recognizers suffer from the excessive number\nof starting states, and the NFA-based ones suffer from the number of\nnondeterministic transitions. Our data-parallel algorithm is based on the new\nFA type called reduced interface DFA (RI-DFA), which minimizes the speculation\noverhead without incurring in the penalty of nondeterministic transitions or of\nimpractically enlarged DFA machines. The algorithm is proved to be correct and\ntheoretically efficient, because it combines the state-reduction of an NFA with\nthe speed of deterministic transitions, thus improving on both DFA-based and\nNFA-based existing implementations. The practical applicability of the RI-DFA\napproach is confirmed by a quantitative comparison of the number of starting\nstates for a large public benchmark of complex FAs. On multi-core computing\narchitectures, the RI-DFA recognizer is much faster than the NFA-based one on\nall benchmarks, while it matches the DFA-based one on some benchmarks and\nperforms much better on some others. The extra time cost needed to construct an\nRI-DFA compared to a DFA is moderate and is compatible with a practical use.\n","authors":["Angelo Borsotti","Luca Breveglieri","Stefano Crespi Reghizzi","Angelo Morzenti"],"pdf_url":"https://arxiv.org/pdf/2412.14975v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06080v1","updated":"2025-01-10T16:15:23Z","published":"2025-01-10T16:15:23Z","title":"Scale-up Unlearnable Examples Learning with High-Performance Computing","summary":"  Recent advancements in AI models are structured to retain user interactions,\nwhich could inadvertently include sensitive healthcare data. In the healthcare\nfield, particularly when radiologists use AI-driven diagnostic tools hosted on\nonline platforms, there is a risk that medical imaging data may be repurposed\nfor future AI training without explicit consent, spotlighting critical privacy\nand intellectual property concerns around healthcare data usage. Addressing\nthese privacy challenges, a novel approach known as Unlearnable Examples (UEs)\nhas been introduced, aiming to make data unlearnable to deep learning models. A\nprominent method within this area, called Unlearnable Clustering (UC), has\nshown improved UE performance with larger batch sizes but was previously\nlimited by computational resources. To push the boundaries of UE performance\nwith theoretically unlimited resources, we scaled up UC learning across various\ndatasets using Distributed Data Parallel (DDP) training on the Summit\nsupercomputer. Our goal was to examine UE efficacy at high-performance\ncomputing (HPC) levels to prevent unauthorized learning and enhance data\nsecurity, particularly exploring the impact of batch size on UE's\nunlearnability. Utilizing the robust computational capabilities of the Summit,\nextensive experiments were conducted on diverse datasets such as Pets,\nMedMNist, Flowers, and Flowers102. Our findings reveal that both overly large\nand overly small batch sizes can lead to performance instability and affect\naccuracy. However, the relationship between batch size and unlearnability\nvaried across datasets, highlighting the necessity for tailored batch size\nstrategies to achieve optimal data protection. Our results underscore the\ncritical role of selecting appropriate batch sizes based on the specific\ncharacteristics of each dataset to prevent learning and ensure data security in\ndeep learning applications.\n","authors":["Yanfan Zhu","Issac Lyngaas","Murali Gopalakrishnan Meena","Mary Ellen I. Koran","Bradley Malin","Daniel Moyer","Shunxing Bao","Anuj Kapadia","Xiao Wang","Bennett Landman","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2501.06080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06044v1","updated":"2025-01-10T15:24:56Z","published":"2025-01-10T15:24:56Z","title":"Beyond Optimal Fault Tolerance","summary":"  The optimal fault-tolerance achievable by any protocol has been characterized\nin a wide range of settings. For example, for state machine replication (SMR)\nprotocols operating in the partially synchronous setting, it is possible to\nsimultaneously guarantee consistency against $\\alpha$-bounded adversaries\n(i.e., adversaries that control less than an $\\alpha$ fraction of the\nparticipants) and liveness against $\\beta$-bounded adversaries if and only if\n$\\alpha + 2\\beta \\leq 1$.\n  This paper characterizes to what extent \"better-than-optimal\" fault-tolerance\nguarantees are possible for SMR protocols when the standard consistency\nrequirement is relaxed to allow a bounded number $r$ of consistency violations.\nWe prove that bounding rollback is impossible without additional timing\nassumptions and investigate protocols that tolerate and recover from\nconsistency violations whenever message delays around the time of an attack are\nbounded by a parameter $\\Delta^*$ (which may be arbitrarily larger than the\nparameter $\\Delta$ that bounds post-GST message delays in the partially\nsynchronous model). Here, a protocol's fault-tolerance can be a non-constant\nfunction of $r$, and we prove, for each $r$, matching upper and lower bounds on\nthe optimal ``recoverable fault-tolerance'' achievable by any SMR protocol. For\nexample, for protocols that guarantee liveness against 1/3-bounded adversaries\nin the partially synchronous setting, a 5/9-bounded adversary can always cause\none consistency violation but not two, and a 2/3-bounded adversary can always\ncause two consistency violations but not three. Our positive results are\nachieved through a generic ``recovery procedure'' that can be grafted on to any\naccountable SMR protocol and restores consistency following a violation while\nrolling back only transactions that were finalized in the previous $2\\Delta^*$\ntimesteps.\n","authors":["Andrew Lewis-Pye","Tim Roughgarden"],"pdf_url":"https://arxiv.org/pdf/2501.06044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05938v1","updated":"2025-01-10T13:02:22Z","published":"2025-01-10T13:02:22Z","title":"ML-Based Optimum Number of CUDA Streams for the GPU Implementation of\n  the Tridiagonal Partition Method","summary":"  This paper presents a heuristic for finding the optimum number of CUDA\nstreams by using tools common to the modern AI-oriented approaches and applied\nto the parallel partition algorithm. A time complexity model for the GPU\nrealization of the partition method is built. Further, a refined time\ncomplexity model for the partition algorithm being executed on multiple CUDA\nstreams is formulated. Computational experiments for different SLAE sizes are\nconducted, and the optimum number of CUDA streams for each of them is found\nempirically. Based on the collected data a model for the sum of the times for\nthe non-dominant GPU operations (that take part in the stream overlap) is\nformulated using regression analysis. A fitting non-linear model for the\noverhead time connected with the creation of CUDA streams is created.\nStatistical analysis is done for all the built models. An algorithm for finding\nthe optimum number of CUDA streams is formulated. Using this algorithm,\ntogether with the two models mentioned above, predictions for the optimum\nnumber of CUDA streams are made. Comparing the predicted values with the actual\ndata, the algorithm is deemed to be acceptably good.\n","authors":["Milena Veneva","Toshiyuki Imamura"],"pdf_url":"https://arxiv.org/pdf/2501.05938v1.pdf","comment":"7 pages, 4 figures, 5 tables, MMCP conference 2024, Yerevan, Armenia"},{"id":"http://arxiv.org/abs/2501.05934v1","updated":"2025-01-10T12:56:19Z","published":"2025-01-10T12:56:19Z","title":"Encoded Spatial Attribute in Multi-Tier Federated Learning","summary":"  This research presents an Encoded Spatial Multi-Tier Federated Learning\napproach for a comprehensive evaluation of aggregated models for geospatial\ndata. In the client tier, encoding spatial information is introduced to better\npredict the target outcome. The research aims to assess the performance of\nthese models across diverse datasets and spatial attributes, highlighting\nvariations in predictive accuracy. Using evaluation metrics such as accuracy,\nour research reveals insights into the complexities of spatial granularity and\nthe challenges of capturing underlying patterns in the data. We extended the\nscope of federated learning (FL) by having multi-tier along with the\nfunctionality of encoding spatial attributes. Our N-tier FL approach used\nencoded spatial data to aggregate in different tiers. We obtained multiple\nmodels that predicted the different granularities of spatial data. Our findings\nunderscore the need for further research to improve predictive accuracy and\nmodel generalization, with potential avenues including incorporating additional\nfeatures, refining model architectures, and exploring alternative modeling\napproaches. Our experiments have several tiers representing different levels of\nspatial aspects. We obtained accuracy of 75.62% and 89.52% for the global model\nwithout having to train the model using the data constituted with the\ndesignated tier. The research also highlights the importance of the proposed\napproach in real-time applications.\n","authors":["Asfia Kawnine","Francis Palma","Seyed Alireza Rahimi Azghadi","Hung Cao"],"pdf_url":"https://arxiv.org/pdf/2501.05934v1.pdf","comment":"IEEE ICCE 2025"},{"id":"http://arxiv.org/abs/2311.04140v3","updated":"2025-01-10T09:15:45Z","published":"2023-11-07T17:12:58Z","title":"A Nearly Linear-Time Distributed Algorithm for Maximum Cardinality\n  Matching","summary":"  In this paper, we propose a randomized $\\tilde{O}(\\mu(G))$-round algorithm\nfor the maximum cardinality matching problem in the CONGEST model, where\n$\\mu(G)$ means the maximum size of a matching of the input graph $G$. The\nproposed algorithm substantially improves the current best worst-case running\ntime. The key technical ingredient is a new randomized algorithm of finding an\naugmenting path of length $\\ell$ with high probability within $\\tilde{O}(\\ell)$\nrounds, which positively settles an open problem left in the prior work by\nAhmadi and Kuhn [DISC'20].\n  The idea of our augmenting path algorithm is based on a recent result by\nKitamura and Izumi [IEICE Trans.'22], which efficiently identifies a sparse\nsubstructure of the input graph containing an augmenting path, following a new\nconcept called \\emph{alternating base trees}. Their algorithm, however, resorts\nin part to a centralized approach of collecting the entire information of the\nsubstructure into a single vertex for constructing a long augmenting path. The\ntechnical highlight of this paper is to provide a fully-decentralized\ncounterpart of such a centralized method. To develop the algorithm, we prove\nseveral new structural properties of alternating base trees, which are of\nindependent interest.\n","authors":["Taisuke Izumi","Naoki Kitamura","Yutaro Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2311.04140v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05775v1","updated":"2025-01-10T08:15:02Z","published":"2025-01-10T08:15:02Z","title":"STHFL: Spatio-Temporal Heterogeneous Federated Learning","summary":"  Federated learning is a new framework that protects data privacy and allows\nmultiple devices to cooperate in training machine learning models. Previous\nstudies have proposed multiple approaches to eliminate the challenges posed by\nnon-iid data and inter-domain heterogeneity issues. However, they ignore the\n\\textbf{spatio-temporal} heterogeneity formed by different data distributions\nof increasing task data in the intra-domain. Moreover, the global data is\ngenerally a long-tailed distribution rather than assuming the global data is\nbalanced in practical applications. To tackle the \\textbf{spatio-temporal}\ndilemma, we propose a novel setting named \\textbf{Spatio-Temporal\nHeterogeneity} Federated Learning (STHFL). Specially, the Global-Local Dynamic\nPrototype (GLDP) framework is designed for STHFL. In GLDP, the model in each\nclient contains personalized layers which can dynamically adapt to different\ndata distributions. For long-tailed data distribution, global prototypes are\nserved as complementary knowledge for the training on classes with few samples\nin clients without leaking privacy. As tasks increase in clients, the knowledge\nof local prototypes generated in previous tasks guides for training in the\ncurrent task to solve catastrophic forgetting. Meanwhile, the global-local\nprototypes are updated through the moving average method after training local\nprototypes in clients. Finally, we evaluate the effectiveness of GLDP, which\nachieves remarkable results compared to state-of-the-art methods in STHFL\nscenarios.\n","authors":["Shunxin Guo","Hongsong Wang","Shuxia Lin","Xu Yang","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2501.05775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05651v1","updated":"2025-01-10T01:42:05Z","published":"2025-01-10T01:42:05Z","title":"A Practical Cross-Layer Approach for ML-Driven Storage Placement in\n  Warehouse-Scale Computers","summary":"  Storage systems account for a major portion of the total cost of ownership\n(TCO) of warehouse-scale computers, and thus have a major impact on the overall\nsystem's efficiency. Machine learning (ML)-based methods for solving key\nproblems in storage system efficiency, such as data placement, have shown\nsignificant promise. However, there are few known practical deployments of such\nmethods. Studying this problem in the context of real-world hyperscale data\ncenter deployments at Google, we identify a number of challenges that we\nbelieve cause this lack of practical adoption. Specifically, prior work assumes\na monolithic model that resides entirely within the storage layer, an\nunrealistic assumption in real-world data center deployments. We propose a\ncross-layer approach that moves ML out of the storage system and performs it in\nthe application running on top of it, co-designed with a scheduling algorithm\nat the storage layer that consumes predictions from these application-level\nmodels. This approach combines small, interpretable models with a co-designed\nheuristic that adapts to different online environments. We build a\nproof-of-concept of this approach in a production distributed computation\nframework at Google. Evaluations in a test deployment and large-scale\nsimulation studies using production traces show improvements of as much as\n3.47x in TCO savings compared to state of the art baselines. We believe this\nwork represents a significant step towards more practical ML-driven storage\nplacement in warehouse-scale computers.\n","authors":["Chenxi Yang","Yan Li","Martin Maas","Mustafa Uysal","Ubaid Ullah Hafeez","Arif Merchant","Richard McDougall"],"pdf_url":"https://arxiv.org/pdf/2501.05651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05647v1","updated":"2025-01-10T01:27:12Z","published":"2025-01-10T01:27:12Z","title":"Collaboration of Large Language Models and Small Recommendation Models\n  for Device-Cloud Recommendation","summary":"  Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising\nresearch direction that has demonstrated exceptional performance in this field.\nHowever, its inability to capture real-time user preferences greatly limits the\npractical application of LLM4Rec because (i) LLMs are costly to train and infer\nfrequently, and (ii) LLMs struggle to access real-time data (its large number\nof parameters poses an obstacle to deployment on devices). Fortunately, small\nrecommendation models (SRMs) can effectively supplement these shortcomings of\nLLM4Rec diagrams by consuming minimal resources for frequent training and\ninference, and by conveniently accessing real-time data on devices.\n  In light of this, we designed the Device-Cloud LLM-SRM Collaborative\nRecommendation Framework (LSC4Rec) under a device-cloud collaboration setting.\nLSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the\nbenefits of cloud and edge computing, achieving a complementary synergy. We\nenhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent request.\nDuring training, LLM generates candidate lists to enhance the ranking ability\nof SRM in collaborative scenarios and enables SRM to update adaptively to\ncapture real-time user interests. During inference, LLM and SRM are deployed on\nthe cloud and on the device, respectively. LLM generates candidate lists and\ninitial ranking results based on user behavior, and SRM get reranking results\nbased on the candidate list, with final results integrating both LLM's and\nSRM's scores. The device determines whether a new candidate list is needed by\ncomparing the consistency of the LLM's and SRM's sorted lists. Our\ncomprehensive and extensive experimental analysis validates the effectiveness\nof each strategy in LSC4Rec.\n","authors":["Zheqi Lv","Tianyu Zhan","Wenjie Wang","Xinyu Lin","Shengyu Zhang","Wenqiao Zhang","Jiwei Li","Kun Kuang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2501.05647v1.pdf","comment":"Published on KDD'25: Proceedings of the ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining 2025"},{"id":"http://arxiv.org/abs/2501.05637v1","updated":"2025-01-10T00:50:23Z","published":"2025-01-10T00:50:23Z","title":"Constrained Over-the-Air Model Updating for Wireless Online Federated\n  Learning with Delayed Information","summary":"  We study online federated learning over a wireless network, where the central\nserver updates an online global model sequence to minimize the time-varying\nloss of multiple local devices over time. The server updates the global model\nthrough over-the-air model-difference aggregation from the local devices over a\nnoisy multiple-access fading channel. We consider the practical scenario where\ninformation on both the local loss functions and the channel states is delayed,\nand each local device is under a time-varying power constraint. We propose\nConstrained Over-the-air Model Updating with Delayed infOrmation (COMUDO),\nwhere a new lower-and-upper-bounded virtual queue is introduced to counter the\ndelayed information and control the hard constraint violation. We show that its\nlocal model updates can be efficiently computed in closed-form expressions.\nFurthermore, through a new Lyapunov drift analysis, we show that COMUDO\nprovides bounds on the dynamic regret, static regret, and hard constraint\nviolation. Simulation results on image classification tasks under practical\nwireless network settings show substantial accuracy gain of COMUDO over\nstate-of-the-art approaches, especially in the low-power region.\n","authors":["Juncheng Wang","Yituo Liu","Ben Liang","Min Dong"],"pdf_url":"https://arxiv.org/pdf/2501.05637v1.pdf","comment":"To appear in INFOCOM 2025"}]},"2025-01-09T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2501.05295v1","updated":"2025-01-09T14:57:19Z","published":"2025-01-09T14:57:19Z","title":"GaussDB-Global: A Geographically Distributed Database System","summary":"  Geographically distributed database systems use remote replication to protect\nagainst regional failures. These systems are sensitive to severe latency\npenalties caused by centralized transaction management, remote access to\nsharded data, and log shipping over long distances. To tackle these issues, we\npresent GaussDB-Global, a sharded geographically distributed database system\nwith asynchronous replication, for OLTP applications. To tackle the transaction\nmanagement bottleneck, we take a decentralized approach using synchronized\nclocks. Our system can seamlessly transition between centralized and\ndecentralized transaction management, providing efficient fault tolerance and\nstreamlining deployment. To alleviate the remote read and log shipping issues,\nwe support reads on asynchronous replicas with strong consistency, tunable\nfreshness guarantees, and dynamic load balancing. Our experimental results on a\ngeographically distributed cluster show that our approach provides up to 14x\nhigher read throughput, and 50% more TPC-C throughput compared to our baseline.\n","authors":["Puya Memarzia","Huaxin Zhang","Kelvin Ho","Ronen Grosman","Jiang Wang"],"pdf_url":"https://arxiv.org/pdf/2501.05295v1.pdf","comment":"8 pages, 11 figures, published in ICDE 2024"},{"id":"http://arxiv.org/abs/2501.05138v1","updated":"2025-01-09T10:44:10Z","published":"2025-01-09T10:44:10Z","title":"Preference Queries over Taxonomic Domains","summary":"  When composing multiple preferences characterizing the most suitable results\nfor a user, several issues may arise. Indeed, preferences can be partially\ncontradictory, suffer from a mismatch with the level of detail of the actual\ndata, and even lack natural properties such as transitivity. In this paper we\nformally investigate the problem of retrieving the best results complying with\nmultiple preferences expressed in a logic-based language. Data are stored in\nrelational tables with taxonomic domains, which allow the specification of\npreferences also over values that are more generic than those in the database.\nIn this framework, we introduce two operators that rewrite preferences for\nenforcing the important properties of transitivity, which guarantees soundness\nof the result, and specificity, which solves all conflicts among preferences.\nAlthough, as we show, these two properties cannot be fully achieved together,\nwe use our operators to identify the only two alternatives that ensure\ntransitivity and minimize the residual conflicts. Building on this finding, we\ndevise a technique, based on an original heuristics, for selecting the best\nresults according to the two possible alternatives. We finally show, with a\nnumber of experiments over both synthetic and real-world datasets, the\neffectiveness and practical feasibility of the overall approach.\n","authors":["Paolo Ciaccia","Davide Martinenghi","Riccardo Torlone"],"pdf_url":"https://arxiv.org/pdf/2501.05138v1.pdf","comment":"43 pages, 11 figures, this is an extended version of a paper\n  published in PVLDB 2021"},{"id":"http://arxiv.org/abs/2404.14692v3","updated":"2025-01-09T07:00:54Z","published":"2024-04-23T02:49:58Z","title":"Deep Overlapping Community Search via Subspace Embedding","summary":"  Overlapping Community Search (OCS) identifies nodes that interact with\nmultiple communities based on a specified query. Existing community search\napproaches fall into two categories: algorithm-based models and Machine\nLearning-based (ML) models. Despite the long-standing focus on this topic\nwithin the database domain, current solutions face two major limitations: 1)\nBoth approaches fail to address personalized user requirements in OCS,\nconsistently returning the same set of nodes for a given query regardless of\nuser differences. 2) Existing ML-based CS models suffer from severe training\nefficiency issues. In this paper, we formally redefine the problem of OCS. By\nanalyzing the gaps in both types of approaches, we then propose a general\nsolution for OCS named Sparse Subspace Filter (SSF), which can extend any\nML-based CS model to enable personalized search in overlapping structures. To\novercome the efficiency issue in the current models, we introduce Simplified\nMulti-hop Attention Networks (SMN), a lightweight yet effective community\nsearch model with larger receptive fields. To the best of our knowledge, this\nis the first ML-based study of overlapping community search. Extensive\nexperiments validate the superior performance of SMN within the SSF pipeline,\nachieving a 13.73% improvement in F1-Score and up to 3 orders of magnitude\nacceleration in model efficiency compared to state-of-the-art approaches.\n","authors":["Qing Sima","Jianke Yu","Xiaoyang Wang","Wenjie Zhang","Ying Zhang","Xuemin Lin"],"pdf_url":"https://arxiv.org/pdf/2404.14692v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04919v2","updated":"2025-01-09T07:00:24Z","published":"2024-08-09T08:01:37Z","title":"SEA-SQL: Semantic-Enhanced Text-to-SQL with Adaptive Refinement","summary":"  Recent advancements in large language models (LLMs) have significantly\ncontributed to the progress of the Text-to-SQL task. A common requirement in\nmany of these works is the post-correction of SQL queries. However, the\nmajority of this process entails analyzing error cases to develop prompts with\nrules that eliminate model bias. And there is an absence of execution\nverification for SQL queries. In addition, the prevalent techniques primarily\ndepend on GPT-4 and few-shot prompts, resulting in expensive costs. To\ninvestigate the effective methods for SQL refinement in a cost-efficient\nmanner, we introduce Semantic-Enhanced Text-to-SQL with Adaptive Refinement\n(SEA-SQL), which includes Adaptive Bias Elimination and Dynamic Execution\nAdjustment, aims to improve performance while minimizing resource expenditure\nwith zero-shot prompts. Specifically, SEA-SQL employs a semantic-enhanced\nschema to augment database information and optimize SQL queries. During the SQL\nquery generation, a fine-tuned adaptive bias eliminator is applied to mitigate\ninherent biases caused by the LLM. The dynamic execution adjustment is utilized\nto guarantee the executability of the bias eliminated SQL query. We conduct\nexperiments on the Spider and BIRD datasets to demonstrate the effectiveness of\nthis framework. The results demonstrate that SEA-SQL achieves state-of-the-art\nperformance in the GPT3.5 scenario with 9%-58% of the generation cost.\nFurthermore, SEA-SQL is comparable to GPT-4 with only 0.9%-5.3% of the\ngeneration cost.\n","authors":["Chaofan Li","Yingxia Shao","Yawen Li","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2408.04919v2.pdf","comment":"The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-41136-3}"},{"id":"http://arxiv.org/abs/2501.05006v1","updated":"2025-01-09T06:59:08Z","published":"2025-01-09T06:59:08Z","title":"CHASE: A Native Relational Database for Hybrid Queries on Structured and\n  Unstructured Data","summary":"  Querying both structured and unstructured data has become a new paradigm in\ndata analytics and recommendation. With unstructured data, such as text and\nvideos, are converted to high-dimensional vectors and queried with approximate\nnearest neighbor search (ANNS). State-of-the-art database systems implement\nvector search as a plugin in the relational query engine, which tries to\nutilize the ANN index to enhance performance. After investigating a broad range\nof hybrid queries, we find that such designs may miss potential optimization\nopportunities and achieve suboptimal performance for certain queries. In this\npaper, we propose CHASE, a query engine that is natively designed to support\nefficient hybrid queries on structured and unstructured data. CHASE performs\nspecific designs and optimizations on multiple stages in query processing.\nFirst, semantic analysis is performed to categorize queries and optimize query\nplans dynamically. Second, new physical operators are implemented to avoid\nredundant computations, which is the case with existing operators. Third,\ncompilation-based techniques are adopted for efficient machine code generation.\nExtensive evaluations using real-world datasets demonstrate that CHASE achieves\nsubstantial performance improvements, with speedups ranging from 13% to an\nextraordinary 7500 times compared to existing systems. These results highlight\nCHASE's potential as a robust solution for executing hybrid queries.\n","authors":["Rui Ma","Kai Zhang","Zhenying He","Yinan Jing","X. Sean Wang","Zhenqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2501.05006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04216v2","updated":"2025-01-09T03:02:31Z","published":"2025-01-08T01:23:29Z","title":"Optimal Oblivious Algorithms for Multi-way Joins","summary":"  In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.\n","authors":["Xiao Hu","Zhiang Wu"],"pdf_url":"https://arxiv.org/pdf/2501.04216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04901v1","updated":"2025-01-09T01:26:59Z","published":"2025-01-09T01:26:59Z","title":"ThriftLLM: On Cost-Effective Selection of Large Language Models for\n  Classification Queries","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\ncomprehending and generating natural language content, attracting widespread\npopularity in both industry and academia in recent years. An increasing number\nof services have sprung up which offer LLMs for various tasks via APIs.\nDifferent LLMs demonstrate expertise in different domains of queries (e.g.,\ntext classification queries). Meanwhile, LLMs of different scales, complexity,\nand performance are priced diversely. Driven by this observation, a growing\nnumber of researchers are investigating the LLM ensemble strategy with a focus\non cost-effectiveness, aiming to decrease overall usage costs while enhancing\nperformance. However, to the best of our knowledge, none of the existing works\naddresses the problem, i.e., how to find an LLM ensemble subject to a cost\nbudget, which maximizes the ensemble performance.\n  In this paper, we formalize the performance of an ensemble of models (LLMs)\nusing the notion of prediction accuracy which we formally define. We develop an\napproach for aggregating responses from multiple LLMs to enhance ensemble\nperformance. Building on this, we formulate the ensemble selection problem as\nthat of selecting a set of LLMs subject to a cost budget such that the overall\nprediction accuracy is maximized. We theoretically establish the non-decreasing\nand non-submodular properties of the prediction accuracy function and provide\nevidence that the Optimal Ensemble Selection problem is likely to be NP-hard.\nSubsequently, we apply dynamic programming and propose an algorithm called\nThriftLLM. We prove that ThriftLLM achieves a near-optimal approximation\nguarantee. In addition, it achieves state-of-the-art query performance on\nmultiple real-world datasets against 3 competitors in our extensive\nexperimental evaluation, strongly supporting the effectiveness and superiority\nof our method.\n","authors":["Keke Huang","Yimin Shi","Dujian Ding","Yifei Li","Yang Fei","Laks Lakshmanan","Xiaokui Xiao"],"pdf_url":"https://arxiv.org/pdf/2501.04901v1.pdf","comment":null}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2501.05587v1","updated":"2025-01-09T21:43:16Z","published":"2025-01-09T21:43:16Z","title":"Popcorn: Accelerating Kernel K-means on GPUs through Sparse Linear\n  Algebra","summary":"  K-means is a popular clustering algorithm with significant applications in\nnumerous scientific and engineering areas. One drawback of K-means is its\ninability to identify non-linearly separable clusters, which may lead to\ninaccurate solutions in certain cases. Kernel K-means is a variant of classical\nK-means that can find non-linearly separable clusters. However, it scales\nquadratically with respect to the size of the dataset, taking several minutes\nto cluster even medium-sized datasets on traditional CPU-based machines. In\nthis paper, we present a formulation of Kernel K-means using sparse-dense\nmatrix multiplication (SpMM) and sparse matrix-vector multiplication (SpMV),\nand we show that our formulation enables the rapid implementation of a fast\nGPU-based version of Kernel K-means with little programming effort. Our\nimplementation, named Popcorn, is the first open-source GPU-based\nimplementation of Kernel K-means. Popcorn achieves a speedup of up to 123.8x\nover a CPU implementation of Kernel K-means and a speedup of up to 2.6x over a\nGPU implementation of Kernel K-means that does not use sparse matrix\ncomputations. Our results support the effectiveness of sparse matrices as tools\nfor efficient parallel programming.\n","authors":["Julian Bellavita","Thomas Pasquali","Laura Del Rio Martin","Flavio Vella","Giulia Guidi"],"pdf_url":"https://arxiv.org/pdf/2501.05587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05563v1","updated":"2025-01-09T20:19:01Z","published":"2025-01-09T20:19:01Z","title":"Prediction-Assisted Online Distributed Deep Learning Workload Scheduling\n  in GPU Clusters","summary":"  The recent explosive growth of deep learning (DL) models has necessitated a\ncompelling need for efficient job scheduling for distributed deep learning\ntraining with mixed parallelisms (DDLwMP) in GPU clusters. This paper proposes\nan adaptive shortest-remaining-processing-time-first (A-SRPT) scheduling\nalgorithm, a novel prediction-assisted online scheduling approach designed to\nmitigate the challenges associated with DL cluster scheduling. By modeling each\njob as a graph corresponding to heterogeneous Deep Neural Network (DNN) models\nand their associated distributed training configurations, A-SRPT strategically\nassigns jobs to the available GPUs, thereby minimizing inter-server\ncommunication overhead. Observing that most DDLwMP jobs recur, A-SRPT\nincorporates a random forest regression model to predict training iterations.\nCrucially, A-SRPT maps the complex scheduling problem into a single-machine\ninstance, which is addressed optimally by a preemptive\n\"shortest-remaining-processing-time-first\" strategy. This optimized solution\nserves as a guide for actual job scheduling within the GPU clusters, leading to\na theoretically provable competitive scheduling efficiency. We conduct\nextensive real-world testbed and simulation experiments to verify our proposed\nalgorithms.\n","authors":["Ziyue Luo","Jia Liu","Myungjin Lee","Ness B. Shroff"],"pdf_url":"https://arxiv.org/pdf/2501.05563v1.pdf","comment":"INFOCOM 2025"},{"id":"http://arxiv.org/abs/2501.05535v1","updated":"2025-01-09T19:17:43Z","published":"2025-01-09T19:17:43Z","title":"On Fair Ordering and Differential Privacy","summary":"  In blockchain systems, fair transaction ordering is crucial for a trusted and\nregulation-compliant economic ecosystem. Unlike traditional State Machine\nReplication (SMR) systems, which focus solely on liveness and safety,\nblockchain systems also require a fairness property. This paper examines these\nproperties and aims to eliminate algorithmic bias in transaction ordering\nservices.\n  We build on the notion of equal opportunity. We characterize transactions in\nterms of relevant and irrelevant features, requiring that the order be\ndetermined solely by the relevant ones. Specifically, transactions with\nidentical relevant features should have an equal chance of being ordered before\none another. We extend this framework to define a property where the greater\nthe distance in relevant features between transactions, the higher the\nprobability of prioritizing one over the other.\n  We reveal a surprising link between equal opportunity in SMR and Differential\nPrivacy (DP), showing that any DP mechanism can be used to ensure fairness in\nSMR. This connection not only enhances our understanding of the interplay\nbetween privacy and fairness in distributed computing but also opens up new\nopportunities for designing fair distributed protocols using well-established\nDP techniques.\n","authors":["Shir Cohen","Neel Basu","Soumya Basu","Lorenzo Alvisi"],"pdf_url":"https://arxiv.org/pdf/2501.05535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05520v1","updated":"2025-01-09T19:00:18Z","published":"2025-01-09T19:00:18Z","title":"Track reconstruction as a service for collider physics","summary":"  Optimizing charged-particle track reconstruction algorithms is crucial for\nefficient event reconstruction in Large Hadron Collider (LHC) experiments due\nto their significant computational demands. Existing track reconstruction\nalgorithms have been adapted to run on massively parallel coprocessors, such as\ngraphics processing units (GPUs), to reduce processing time. Nevertheless,\nchallenges remain in fully harnessing the computational capacity of\ncoprocessors in a scalable and non-disruptive manner. This paper proposes an\ninference-as-a-service approach for particle tracking in high energy physics\nexperiments. To evaluate the efficacy of this approach, two distinct tracking\nalgorithms are tested: Patatrack, a rule-based algorithm, and Exa$.$TrkX, a\nmachine learning-based algorithm. The as-a-service implementations show\nenhanced GPU utilization and can process requests from multiple CPU cores\nconcurrently without increasing per-request latency. The impact of data\ntransfer is minimal and insignificant compared to running on local\ncoprocessors. This approach greatly improves the computational efficiency of\ncharged particle tracking, providing a solution to the computing challenges\nanticipated in the High-Luminosity LHC era.\n","authors":["Yuan-Tang Chou","Miles Cochran-Branson","Javier Duarte","Yongbin Feng","Philip Harris","Shih-Chieh Hsu","Xiangyang Ju","Miaoyuan Liu","William Patrick McCormack","Kevin Pedro","Jan-Frederik Schulte","Nhan Tran","Yao Yao","Haoran Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.05520v1.pdf","comment":"19 pages, 8 figures, submitted to JINST"},{"id":"http://arxiv.org/abs/2408.13386v3","updated":"2025-01-09T18:23:25Z","published":"2024-08-23T21:40:56Z","title":"CloudSim 7G: An Integrated Toolkit for Modeling and Simulation of Future\n  Generation Cloud Computing Environments","summary":"  Cloud Computing has established itself as an efficient and cost-effective\nparadigm for the execution of web-based applications, and scientific workloads,\nthat need elasticity and on-demand scalability capabilities. However, the\nevaluation of novel resource provisioning and management techniques is a major\nchallenge due to the complexity of large-scale data centers. Therefore, Cloud\nsimulators are an essential tool for academic and industrial researchers, to\ninvestigate the effectiveness of novel algorithms and mechanisms in large-scale\nscenarios. This paper proposes CloudSim 7G, the seventh generation of CloudSim,\nwhich features a re-engineered and generalized internal architecture to\nfacilitate the integration of multiple CloudSim extensions within the same\nsimulated environment. As part of the new design, we introduced a set of\nstandardized interfaces to abstract common functionalities and carried out\nextensive refactoring and refinement of the codebase. The result is a\nsubstantial reduction in lines of code with no loss in functionality,\nsignificant improvements in run-time performance and memory efficiency (up to\n25% less heap memory allocated), as well as increased flexibility, ease-of-use,\nand extensibility of the framework. These improvements benefit not only\nCloudSim developers but also researchers and practitioners using the framework\nfor modeling and simulating next-generation Cloud Computing environments.\n","authors":["Remo Andreoli","Jie Zhao","Tommaso Cucinotta","Rajkumar Buyya"],"pdf_url":"https://arxiv.org/pdf/2408.13386v3.pdf","comment":"Second revision of paper, submitted to Wiley Online Software:\n  Practice and Experience"},{"id":"http://arxiv.org/abs/2501.05408v1","updated":"2025-01-09T18:05:33Z","published":"2025-01-09T18:05:33Z","title":"TimeRL: Efficient Deep Reinforcement Learning with Polyhedral Dependence\n  Graphs","summary":"  Modern deep learning (DL) workloads increasingly use complex deep\nreinforcement learning (DRL) algorithms that generate training data within the\nlearning loop. This results in programs with several nested loops and dynamic\ndata dependencies between tensors. While DL systems with eager execution\nsupport such dynamism, they lack the optimizations and smart scheduling of\ngraph-based execution. Graph-based execution, however, cannot express dynamic\ntensor shapes, instead requiring the use of multiple static subgraphs. Either\nexecution model for DRL thus leads to redundant computation, reduced\nparallelism, and less efficient memory management.\n  We describe TimeRL, a system for executing dynamic DRL programs that combines\nthe dynamism of eager execution with the whole-program optimizations and\nscheduling of graph-based execution. TimeRL achieves this by introducing the\ndeclarative programming model of recurrent tensors, which allows users to\ndefine dynamic dependencies as intuitive recurrence equations. TimeRL\ntranslates recurrent tensors into a polyhedral dependence graph (PDG) with\ndynamic dependencies as symbolic expressions. Through simple PDG\ntransformations, TimeRL applies whole-program optimizations, such as automatic\nvectorization, incrementalization, and operator fusion. The PDG also allows for\nthe computation of an efficient program-wide execution schedule, which decides\non buffer deallocations, buffer donations, and GPU/CPU memory swapping. We show\nthat TimeRL executes current DRL algorithms up to 47$\\times$ faster than\nexisting DRL systems, while using 16$\\times$ less GPU peak memory.\n","authors":["Pedro F. Silvestre","Peter Pietzuch"],"pdf_url":"https://arxiv.org/pdf/2501.05408v1.pdf","comment":"17 pages, 11 figures, 5 bibliography pages"},{"id":"http://arxiv.org/abs/2501.05377v1","updated":"2025-01-09T16:59:56Z","published":"2025-01-09T16:59:56Z","title":"Byzantine Fault Tolerant Protocols with Near-Constant Work per Node\n  without Signatures","summary":"  Numerous distributed tasks have to be handled in a setting where a fraction\nof nodes behaves Byzantine, that is, deviates arbitrarily from the intended\nprotocol. Resilient, deterministic protocols rely on the detection of\nmajorities to avoid inconsistencies if there is a Byzantine minority, which\nrequires individual nodes to handle a communication load that is proportional\nto the size of the network -- an intolerable disadvantage in large networks.\n  Randomized protocols circumvent this by probing only small parts of the\nnetwork, thus allowing for consistent decisions quickly and with a high level\nof confidence with communication that is near-constant in the network size.\nHowever, such protocols usually come with the drawback of limiting the fault\ntolerance of the protocol. For instance, by severely restricting the number or\ntype of failures that the protocol can tolerate.\n  We present randomized protocols to reliably aggregate and broadcast\ninformation, form consensus and compute common coins that tolerate a constant\nfraction of Byzantine failures, do not require cryptographic methods and have a\nnear-constant time and message complexity per node. Our main technique is to\ncompute a system of witness committees as a pre-computation step almost\noptimally. This pre-computation step allows to solve the aforementioned\ndistributed tasks repeatedly and efficiently, but may have far reaching further\napplications, e.g., for sharding of distributed data structures.\n","authors":["Philipp Schneider"],"pdf_url":"https://arxiv.org/pdf/2501.05377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05374v1","updated":"2025-01-09T16:58:29Z","published":"2025-01-09T16:58:29Z","title":"Validation of GPU Computation in Decentralized, Trustless Networks","summary":"  Verifying computational processes in decentralized networks poses a\nfundamental challenge, particularly for Graphics Processing Unit (GPU)\ncomputations. Our investigation reveals significant limitations in existing\napproaches: exact recomputation fails due to computational non-determinism\nacross GPU nodes, Trusted Execution Environments (TEEs) require specialized\nhardware, and Fully Homomorphic Encryption (FHE) faces prohibitive\ncomputational costs. To address these challenges, we explore three verification\nmethodologies adapted from adjacent technical domains: model fingerprinting\ntechniques, semantic similarity analysis, and GPU profiling. Through systematic\nexploration of these approaches, we develop novel probabilistic verification\nframeworks, including a binary reference model with trusted node verification\nand a ternary consensus framework that eliminates trust requirements. These\nmethodologies establish a foundation for ensuring computational integrity\nacross untrusted networks while addressing the inherent challenges of\nnon-deterministic execution in GPU-accelerated workloads.\n","authors":["Eric Boniardi","Stanley Bishop","Alison Haire"],"pdf_url":"https://arxiv.org/pdf/2501.05374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05313v1","updated":"2025-01-09T15:29:33Z","published":"2025-01-09T15:29:33Z","title":"Optimizing Distributed Deployment of Mixture-of-Experts Model Inference\n  in Serverless Computing","summary":"  With the advancement of serverless computing, running machine learning (ML)\ninference services over a serverless platform has been advocated, given its\nlabor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models\nhave been a dominant type of model architectures to enable large models\nnowadays, with parallel expert networks. Serving large MoE models on serverless\ncomputing is potentially beneficial, but has been underexplored due to\nsubstantial challenges in handling the skewed expert popularity and\nscatter-gather communication bottleneck in MoE model execution, for\ncost-efficient serverless MoE deployment and performance guarantee. We study\noptimized MoE model deployment and distributed inference serving on a\nserverless platform, that effectively predict expert selection, pipeline\ncommunication with model execution, and minimize the overall billed cost of\nserving MoE models. Especially, we propose a Bayesian optimization framework\nwith multi-dimensional epsilon-greedy search to learn expert selections and\noptimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian\ndecision-making method for predicting expert popularity; 2) flexibly pipelined\nscatter-gather communication; and 3) an optimal model deployment algorithm for\ndistributed MoE serving. Extensive experiments on AWS Lambda show that our\ndesigns reduce the billed cost of all MoE layers by at least 75.67% compared to\nCPU clusters while maintaining satisfactory inference throughput. As compared\nto LambdaML in serverless computing, our designs achieves 43.41% lower cost\nwith a throughput decrease of at most 18.76%.\n","authors":["Mengfan Liu","Wei Wang","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2501.05313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05267v1","updated":"2025-01-09T14:23:49Z","published":"2025-01-09T14:23:49Z","title":"Distributed Graph Algorithms with Predictions","summary":"  We initiate the study of deterministic distributed graph algorithms with\npredictions in synchronous message passing systems. The process at each node in\nthe graph is given a prediction, which is some extra information about the\nproblem instance that may be incorrect. The processes may use the predictions\nto help them solve the problem. The overall goal is to develop algorithms that\nboth work faster when predictions are good and do not work much worse than\nalgorithms without predictions when predictions are bad. Concepts from the more\ngeneral area of algorithms with predictions, such as error measures,\nconsistency, robustness, and smoothness, are adapted to distributed graph\nalgorithms with predictions.\n  We consider algorithms with predictions for four distributed graph problems,\nMaximal Independent Set, Maximal Matching, $(\\Delta+1)$-Vertex Coloring, and\n$(2\\Delta-1)$-Edge Coloring, where $\\Delta$ denotes the degree of the graph.\nFor each, we define an appropriate error measure. We present generic templates\nthat can be used to design deterministic distributed graph algorithms with\npredictions from existing algorithms without predictions. Using these\ntemplates, we develop algorithms with predictions for Maximal Independent Set.\nAlternative error measures for the Maximal Independent Set problem are also\nconsidered. We obtain algorithms with predictions for general graphs and for\nrooted trees and analyze them using two of these error measures.\n","authors":["Joan Boyar","Faith Ellen","Kim S. Larsen"],"pdf_url":"https://arxiv.org/pdf/2501.05267v1.pdf","comment":"27 pages, 2 figures, 6 algorithms"},{"id":"http://arxiv.org/abs/2407.15879v2","updated":"2025-01-09T13:27:29Z","published":"2024-07-20T10:45:06Z","title":"Decentralized Federated Anomaly Detection in Smart Grids: A P2P Gossip\n  Approach","summary":"  The increasing security and privacy concerns in the Smart Grid sector have\nled to a significant demand for robust intrusion detection systems within\ncritical smart grid infrastructure. To address the challenges posed by privacy\npreservation and decentralized power system zones with distinct data ownership,\nFederated Learning (FL) has emerged as a promising privacy-preserving solution\nwhich facilitates collaborative training of attack detection models without\nnecessitating the sharing of raw data. However, FL presents several\nimplementation limitations in the power system domain due to its heavy reliance\non a centralized aggregator and the risks of privacy leakage during model\nupdate transmission. To overcome these technical bottlenecks, this paper\nintroduces a novel decentralized federated anomaly detection scheme based on\ntwo main gossip protocols namely Random Walk and Epidemic. Our findings\nindicate that the Random Walk protocol exhibits superior performance compared\nto the Epidemic protocol, highlighting its efficacy in decentralized federated\nlearning environments. Experimental validation of the proposed framework\nutilizing publicly available industrial control systems datasets demonstrates\nsuperior attack detection accuracy while safeguarding data confidentiality and\nmitigating the impact of communication latency and stragglers. Furthermore, our\napproach yields a notable 35% improvement in training time compared to\nconventional FL, underscoring the efficacy and robustness of our decentralized\nlearning method.\n","authors":["Muhammad Akbar Husnoo","Adnan Anwar","Md Enamul Haque","A. N. Mahmood"],"pdf_url":"https://arxiv.org/pdf/2407.15879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15166v3","updated":"2025-01-09T11:39:19Z","published":"2024-02-23T07:59:23Z","title":"Convergence Analysis of Split Federated Learning on Heterogeneous Data","summary":"  Split federated learning (SFL) is a recent distributed approach for\ncollaborative model training among multiple clients. In SFL, a global model is\ntypically split into two parts, where clients train one part in a parallel\nfederated manner, and a main server trains the other. Despite the recent\nresearch on SFL algorithm development, the convergence analysis of SFL is\nmissing in the literature, and this paper aims to fill this gap. The analysis\nof SFL can be more challenging than that of federated learning (FL), due to the\npotential dual-paced updates at the clients and the main server. We provide\nconvergence analysis of SFL for strongly convex and general convex objectives\non heterogeneous data. The convergence rates are $O(1/T)$ and\n$O(1/\\sqrt[3]{T})$, respectively, where $T$ denotes the total number of rounds\nfor SFL training. We further extend the analysis to non-convex objectives and\nthe scenario where some clients may be unavailable during training.\nExperimental experiments validate our theoretical results and show that SFL\noutperforms FL and split learning (SL) when data is highly heterogeneous across\na large number of clients.\n","authors":["Pengchao Han","Chao Huang","Geng Tian","Ming Tang","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2402.15166v3.pdf","comment":"Accepted by Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2407.17287v2","updated":"2025-01-09T10:13:51Z","published":"2024-07-24T13:56:56Z","title":"Deterministic and Reliable Software-Defined Vehicles: key building\n  blocks, challenges, and vision","summary":"  As vehicle systems become increasingly complex, with more features, services,\nsensors, actuators, and processing units, it is important to view vehicles not\njust as modes of transportation moving toward full autonomy, but also as\nadaptive systems that respond to the needs of their occupants. Vehicular\nservices can be developed to support these adaptations. However, the increasing\ncomplexity of vehicular service development, even with current\nstandardizations, best practices and guidelines, are insufficient to tackle the\nhigh complexity of development, with expectations of up to 1 (U.S.) billion\nlines of code for a fully (level 5) autonomous vehicle. Within this survey, the\nparadigm of Deterministic Software Defined Vehicles is explored, aiming to\nenhance the quality and ease of developing automotive services by focusing on\nservice-oriented architectures, virtualization techniques, and the necessary\ndeterministic intra- and inter-vehicular communications. Considering the main\nopen challenges for such verticals, a vision architecture towards improved\nservices development and orchestration is presented, focusing on: a) a\ndeterministic network configurator; b) a data layer configurator; c) a\nhypervisor configurator; d) the vehicle abstraction layer; and e) a software\norchestrator.\n","authors":["Pedro Veloso Teixeira","Duarte Raposo","Rui Lopes","Susana Sargento"],"pdf_url":"https://arxiv.org/pdf/2407.17287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05009v1","updated":"2025-01-09T07:11:51Z","published":"2025-01-09T07:11:51Z","title":"A Scalable System for Visual Analysis of Ocean Data","summary":"  Oceanographers rely on visual analysis to interpret model simulations,\nidentify events and phenomena, and track dynamic ocean processes. The ever\nincreasing resolution and complexity of ocean data due to its dynamic nature\nand multivariate relationships demands a scalable and adaptable visualization\ntool for interactive exploration. We introduce pyParaOcean, a scalable and\ninteractive visualization system designed specifically for ocean data analysis.\npyParaOcean offers specialized modules for common oceanographic analysis tasks,\nincluding eddy identification and salinity movement tracking. These modules\nseamlessly integrate with ParaView as filters, ensuring a user-friendly and\neasy-to-use system while leveraging the parallelization capabilities of\nParaView and a plethora of inbuilt general-purpose visualization\nfunctionalities. The creation of an auxiliary dataset stored as a Cinema\ndatabase helps address I/O and network bandwidth bottlenecks while supporting\nthe generation of quick overview visualizations. We present a case study on the\nBay of Bengal (BoB) to demonstrate the utility of the system and scaling\nstudies to evaluate the efficiency of the system.\n","authors":["Toshit Jain","Upkar Singh","Varun Singh","Vijay Kumar Boda","Ingrid Hotz","Sathish S. Vadhiyar","P. N. Vinayachandran","Vijay Natarajan"],"pdf_url":"https://arxiv.org/pdf/2501.05009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04956v1","updated":"2025-01-09T04:16:55Z","published":"2025-01-09T04:16:55Z","title":"Topology-aware Microservice Architecture in Edge Networks: Deployment\n  Optimization and Implementation","summary":"  As a ubiquitous deployment paradigm, integrating microservice architecture\n(MSA) into edge networks promises to enhance the flexibility and scalability of\nservices. However, it also presents significant challenges stemming from\ndispersed node locations and intricate network topologies. In this paper, we\nhave proposed a topology-aware MSA characterized by a three-tier network\ntraffic model encompassing the service, microservices, and edge node layers.\nThis model meticulously characterizes the complex dependencies between edge\nnetwork topologies and microservices, mapping microservice deployment onto link\ntraffic to accurately estimate communication delay. Building upon this model,\nwe have formulated a weighted sum communication delay optimization problem\nconsidering different types of services. Then, a novel topology-aware and\nindividual-adaptive microservices deployment (TAIA-MD) scheme is proposed to\nsolve the problem efficiently, which accurately senses the network topology and\nincorporates an individual-adaptive mechanism in a genetic algorithm to\naccelerate the convergence and avoid local optima. Extensive simulations show\nthat, compared to the existing deployment schemes, TAIA-MD improves the\ncommunication delay performance by approximately 30% to 60% and effectively\nenhances the overall network performance. Furthermore, we implement the TAIA-MD\nscheme on a practical microservice physical platform. The experimental results\ndemonstrate that TAIA-MD achieves superior robustness in withstanding link\nfailures and network fluctuations.\n","authors":["Yuang Chen","Chang Wu","Fangyu Zhang","Chengdi Lu","Yongsheng Huang","Hancheng Lu"],"pdf_url":"https://arxiv.org/pdf/2501.04956v1.pdf","comment":"15 pages, 17 figures, submitted to IEEE Transactions for potential\n  publication"}]},"2025-01-08T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2407.02994v2","updated":"2025-01-08T13:35:45Z","published":"2024-07-03T10:49:21Z","title":"MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced\n  AI Applications with Retrieval Augmented Generation and Knowledge Graphs","summary":"  The increasing interest in developing Artificial Intelligence applications in\nthe medical domain, suffers from the lack of high-quality data set, mainly due\nto privacy-related issues. In addition, the recent increase in large multimodal\nmodels (LMM) leads to the need for multimodal medical data sets, where clinical\nreports and findings are attached to the corresponding CT or MRI scans. This\npaper illustrates the entire workflow for building the MedPix 2.0 data set.\nStarting with the well-known multimodal data set\nMedPix\\textsuperscript{\\textregistered}, mainly used by physicians, nurses, and\nhealthcare students for Continuing Medical Education purposes, a semi-automatic\npipeline was developed to extract visual and textual data followed by a manual\ncuring procedure in which noisy samples were removed, thus creating a MongoDB\ndatabase. Along with the data set, we developed a GUI aimed at navigating\nefficiently the MongoDB instance and obtaining the raw data that can be easily\nused for training and/or fine-tuning LMMs. To enforce this point, in this work,\nwe first recall DR-Minerva, a RAG-based LMM trained using MedPix 2.0.\nDR-Minerva predicts the body part and the modality used to scan its input\nimage. We also propose the extension of DR-Minerva with a Knowledge Graph that\nuses Llama 3.1 Instruct 8B, and leverages MedPix 2.0. The resulting\narchitecture can be queried in a end-to-end manner, as a medical decision\nsupport system. MedPix 2.0 is available on GitHub.\n\\url{https://github.com/CHILab1/MedPix-2.0}\n","authors":["Irene Siragusa","Salvatore Contino","Massimo La Ciura","Rosario Alicata","Roberto Pirrone"],"pdf_url":"https://arxiv.org/pdf/2407.02994v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04467v1","updated":"2025-01-08T12:41:42Z","published":"2025-01-08T12:41:42Z","title":"A Histologic Dataset of Normal and Atypical Mitotic Figures on Human\n  Breast Cancer (AMi-Br)","summary":"  Assessment of the density of mitotic figures (MFs) in histologic tumor\nsections is an important prognostic marker for many tumor types, including\nbreast cancer. Recently, it has been reported in multiple works that the\nquantity of MFs with an atypical morphology (atypical MFs, AMFs) might be an\nindependent prognostic criterion for breast cancer. AMFs are an indicator of\nmutations in the genes regulating the cell cycle and can lead to aberrant\nchromosome constitution (aneuploidy) of the tumor cells. To facilitate further\nresearch on this topic using pattern recognition, we present the first ever\npublicly available dataset of atypical and normal MFs (AMi-Br). For this, we\nutilized two of the most popular MF datasets (MIDOG 2021 and TUPAC) and\nsubclassified all MFs using a three expert majority vote. Our final dataset\nconsists of 3,720 MFs, split into 832 AMFs (22.4%) and 2,888 normal MFs (77.6%)\nacross all 223 tumor cases in the combined set. We provide baseline\nclassification experiments to investigate the consistency of the dataset, using\na Monte Carlo cross-validation and different strategies to combat class\nimbalance. We found an averaged balanced accuracy of up to 0.806 when using a\npatch-level data set split, and up to 0.713 when using a patient-level split.\n","authors":["Christof A. Bertram","Viktoria Weiss","Taryn A. Donovan","Sweta Banerjee","Thomas Conrad","Jonas Ammeling","Robert Klopfleisch","Christopher Kaltenecker","Marc Aubreville"],"pdf_url":"https://arxiv.org/pdf/2501.04467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04347v1","updated":"2025-01-08T08:42:51Z","published":"2025-01-08T08:42:51Z","title":"Keyword Search in the Deep Web","summary":"  The Deep Web is constituted by data that are accessible through Web pages,\nbut not readily indexable by search engines as they are returned in dynamic\npages. In this paper we propose a conceptual framework for answering keyword\nqueries on Deep Web sources represented as relational tables with so-called\naccess limitations. We formalize the notion of optimal answer, characterize\nqueries for which an answer can be found, and present a method for query\nprocessing based on the construction of a query plan that minimizes the\naccesses to the data sources.\n","authors":["Andrea Calì","Davide Martinenghi","Riccardo Torlone"],"pdf_url":"https://arxiv.org/pdf/2501.04347v1.pdf","comment":"21 pages, 3 figures. A short version appeared in ER 2016"}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2403.16542v3","updated":"2025-01-08T21:05:26Z","published":"2024-03-25T08:35:19Z","title":"Differentially Private Online Federated Learning with Correlated Noise","summary":"  We introduce a novel differentially private algorithm for online federated\nlearning that employs temporally correlated noise to enhance utility while\nensuring privacy of continuously released models. To address challenges posed\nby DP noise and local updates with streaming non-iid data, we develop a\nperturbed iterate analysis to control the impact of the DP noise on the\nutility. Moreover, we demonstrate how the drift errors from local updates can\nbe effectively managed under a quasi-strong convexity condition. Subject to an\n$(\\epsilon, \\delta)$-DP budget, we establish a dynamic regret bound over the\nentire time horizon, quantifying the impact of key parameters and the intensity\nof changes in dynamic environments. Numerical experiments confirm the efficacy\nof the proposed algorithm.\n","authors":["Jiaojiao Zhang","Linglingzhi Zhu","Mikael Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.16542v3.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2411.18752v2","updated":"2025-01-08T20:51:17Z","published":"2024-11-27T20:56:43Z","title":"Locally Differentially Private Online Federated Learning With Correlated\n  Noise","summary":"  We introduce a locally differentially private (LDP) algorithm for online\nfederated learning that employs temporally correlated noise to improve utility\nwhile preserving privacy. To address challenges posed by the correlated noise\nand local updates with streaming non-IID data, we develop a perturbed iterate\nanalysis that controls the impact of the noise on the utility. Moreover, we\ndemonstrate how the drift errors from local updates can be effectively managed\nfor several classes of nonconvex loss functions. Subject to an\n$(\\epsilon,\\delta)$-LDP budget, we establish a dynamic regret bound that\nquantifies the impact of key parameters and the intensity of changes in the\ndynamic environment on the learning performance. Numerical experiments confirm\nthe efficacy of the proposed algorithm.\n","authors":["Jiaojiao Zhang","Linglingzhi Zhu","Dominik Fay","Mikael Johansson"],"pdf_url":"https://arxiv.org/pdf/2411.18752v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2403.16542"},{"id":"http://arxiv.org/abs/2501.04654v1","updated":"2025-01-08T18:07:12Z","published":"2025-01-08T18:07:12Z","title":"Recorder: Comprehensive Parallel I/O Tracing and Analysis","summary":"  This paper presents Recorder, a parallel I/O tracing tool designed to capture\ncomprehensive I/O information on HPC applications. Recorder traces I/O calls\nacross various I/O layers, storing all function parameters for each captured\ncall. The volume of stored information scales linearly the application's\nexecution scale. To address this, we present a sophisticated\npattern-recognition-based compression algorithm. This algorithm identifies and\ncompresses recurring I/O patterns both within individual processes and across\nmultiple processes, significantly reducing space and time overheads. We\nevaluate the proposed compression algorithm using I/O benchmarks and real-world\napplications, demonstrating that Recorder can store more information while\nrequiring approximately 12x less storage space compared to its predecessor.\nNotably, for applications with typical parallel I/O patterns, Recorder achieves\na constant trace size regardless of execution scale. Additionally, a comparison\nwith the profiling tool Darshan shows that Recorder captures detailed I/O\ninformation without incurring substantial overhead. The richer data collected\nby Recorder enables new insights and facilitates more in-depth I/O studies,\noffering valuable contributions to the I/O research community.\n","authors":["Chen Wang","Izzet Yildirim","Hariharan Devarajan","Kathryn Mohror","Marc Snir"],"pdf_url":"https://arxiv.org/pdf/2501.04654v1.pdf","comment":"29 pages. Under Review. Submitted to the Journal of Supercomputing"},{"id":"http://arxiv.org/abs/2501.06244v1","updated":"2025-01-08T16:55:04Z","published":"2025-01-08T16:55:04Z","title":"Microservice Deployment in Space Computing Power Networks via Robust\n  Reinforcement Learning","summary":"  With the growing demand for Earth observation, it is important to provide\nreliable real-time remote sensing inference services to meet the low-latency\nrequirements. The Space Computing Power Network (Space-CPN) offers a promising\nsolution by providing onboard computing and extensive coverage capabilities for\nreal-time inference. This paper presents a remote sensing artificial\nintelligence applications deployment framework designed for Low Earth Orbit\nsatellite constellations to achieve real-time inference performance. The\nframework employs the microservice architecture, decomposing monolithic\ninference tasks into reusable, independent modules to address high latency and\nresource heterogeneity. This distributed approach enables optimized\nmicroservice deployment, minimizing resource utilization while meeting quality\nof service and functional requirements. We introduce Robust Optimization to the\ndeployment problem to address data uncertainty. Additionally, we model the\nRobust Optimization problem as a Partially Observable Markov Decision Process\nand propose a robust reinforcement learning algorithm to handle the\nsemi-infinite Quality of Service constraints. Our approach yields sub-optimal\nsolutions that minimize accuracy loss while maintaining acceptable\ncomputational costs. Simulation results demonstrate the effectiveness of our\nframework.\n","authors":["Zhiyong Yu","Yuning Jiang","Xin Liu","Yuanming Shi","Chunxiao Jiang","Linling Kuang"],"pdf_url":"https://arxiv.org/pdf/2501.06244v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2501.04613v1","updated":"2025-01-08T16:53:17Z","published":"2025-01-08T16:53:17Z","title":"A Semantic Partitioning Method for Large-Scale Training of Knowledge\n  Graph Embeddings","summary":"  In recent years, knowledge graph embeddings have achieved great success. Many\nmethods have been proposed and achieved state-of-the-art results in various\ntasks. However, most of the current methods present one or more of the\nfollowing problems: (i) They only consider fact triplets, while ignoring the\nontology information of knowledge graphs. (ii) The obtained embeddings do not\ncontain much semantic information. Therefore, using these embeddings for\nsemantic tasks is problematic. (iii) They do not enable large-scale training.\nIn this paper, we propose a new algorithm that incorporates the ontology of\nknowledge graphs and partitions the knowledge graph based on classes to include\nmore semantic information for parallel training of large-scale knowledge graph\nembeddings. Our preliminary results show that our algorithm performs well on\nseveral popular benchmarks.\n","authors":["Yuhe Bai"],"pdf_url":"https://arxiv.org/pdf/2501.04613v1.pdf","comment":"Accepted at WWW '23 Companion: Companion Proceedings of the ACM Web\n  Conference 2023"},{"id":"http://arxiv.org/abs/2501.06242v1","updated":"2025-01-08T16:19:44Z","published":"2025-01-08T16:19:44Z","title":"Intelligent Task Offloading: Advanced MEC Task Offloading and Resource\n  Management in 5G Networks","summary":"  5G technology enhances industries with high-speed, reliable, low-latency\ncommunication, revolutionizing mobile broadband and supporting massive IoT\nconnectivity. With the increasing complexity of applications on User Equipment\n(UE), offloading resource-intensive tasks to robust servers is essential for\nimproving latency and speed. The 3GPP's Multi-access Edge Computing (MEC)\nframework addresses this challenge by processing tasks closer to the user,\nhighlighting the need for an intelligent controller to optimize task offloading\nand resource allocation. This paper introduces a novel methodology to\nefficiently allocate both communication and computational resources among\nindividual UEs. Our approach integrates two critical 5G service imperatives:\nUltra-Reliable Low Latency Communication (URLLC) and Massive Machine Type\nCommunication (mMTC), embedding them into the decision-making framework.\nCentral to this approach is the utilization of Proximal Policy Optimization,\nproviding a robust and efficient solution to the challenges posed by the\nevolving landscape of 5G technology. The proposed model is evaluated in a\nsimulated 5G MEC environment. The model significantly reduces processing time\nby 4% for URLLC users under strict latency constraints and decreases power\nconsumption by 26% for mMTC users, compared to existing baseline models based\non the reported simulation results. These improvements showcase the model's\nadaptability and superior performance in meeting diverse QoS requirements in 5G\nnetworks.\n","authors":["Alireza Ebrahimi","Fatemeh Afghah"],"pdf_url":"https://arxiv.org/pdf/2501.06242v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.04571v1","updated":"2025-01-08T15:40:22Z","published":"2025-01-08T15:40:22Z","title":"Scalable Data Notarization Leveraging Hybrid DLTs","summary":"  Notarization is a procedure that enhance data management by ensuring the\nauthentication of data during audits, thereby increasing trust in the audited\ndata. Blockchain is frequently used as a secure, immutable, and transparent\nstorage, contributing to make data notarization procedures more effective and\ntrustable. Several blockchain-based data notarization protocols have been\nproposed in literature and commercial solutions. However, these\nimplementations, whether on public or private blockchains, face inherent\nchallenges: high fees on public blockchains and trust issues on private\nplatforms, limiting the adoption of blockchains for data notarization or\nforcing several trade-offs. In this paper, we explore the use of hybrid\nblockchain architectures for data notarization, with a focus on scalability\nissues. Through the analysis of a real-world use case, the data notarization of\nproduct passports in supply chains, we propose a novel approach utilizing a\ndata structure designed to efficiently manage the trade-offs in terms of\nstorage occupation and costs involved in notarizing a large collection of data.\n","authors":["Domenico Tortola","Claudio Felicioli","Andrea Canciani","Fabio Severino"],"pdf_url":"https://arxiv.org/pdf/2501.04571v1.pdf","comment":"presented at WETICE 2024, pre-print"},{"id":"http://arxiv.org/abs/2501.04507v1","updated":"2025-01-08T13:52:55Z","published":"2025-01-08T13:52:55Z","title":"Effective Two-Stage Double Auction for Dynamic Resource Trading in Edge\n  Networks via Overbooking","summary":"  To facilitate responsive and cost-effective computing resource scheduling and\nservice delivery over edge-assisted mobile networks, this paper investigates a\nnovel two-stage double auction methodology via utilizing an interesting idea of\nresource overbooking to overcome dynamic and uncertain nature from edge servers\n(sellers) and demand from mobile devices (as buyers). The proposed auction\nintegrates multiple essential factors such as social welfare maximization and\ndecision-making latency (e.g., the time for determining winning seller-buyer\npairs) reduction, by introducing a stagewise strategy: an overbooking-driven\npre-double auction (OPDAuction) for determining long-term cooperations between\nsellers and buyers before practical resource transactions as Stage I, and a\nreal-time backup double auction (RBDAuction) for handling residual resource\ndemands during actual transactions. In particular, by applying a proper\noverbooking rate, OPDAuction helps with facilitating trading contracts between\nappropriate sellers and buyers as guidance for future transactions, by allowing\nthe booked resources to exceed supply. Then, since pre-auctions may cause\nrisks, our RBDAuction adjusts to real-time market changes, further enhancing\nthe overall social welfare. More importantly, we offer an interesting view to\nshow that our proposed two-stage auction can support significant design\nproperties such as truthfulness, individual rationality, and budget balance.\nThrough extensive experiments, we demonstrate good performance in social\nwelfare, time efficiency, and computational scalability, outstripping\nconventional methods in dynamic edge computing settings.\n","authors":["Sicheng Wu","Minghui Liwang","Deqing Wang","Xianbin Wang","Chao Wu","Junyi Tang","Li Li","Zhenzhen Jiao"],"pdf_url":"https://arxiv.org/pdf/2501.04507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04503v1","updated":"2025-01-08T13:42:54Z","published":"2025-01-08T13:42:54Z","title":"Developing a Modular Compiler for a Subset of a C-like Language","summary":"  The paper introduces the development of a modular compiler for a subset of a\nC-like language, which addresses the challenges in constructing a compiler for\nhigh-level languages. This modular approach will allow developers to modify a\nlanguage by adding or removing subsets as required, resulting in a minimal and\nmemory-efficient compiler. The development process is divided into small,\nincremental steps, where each step yields a fully functioning compiler for an\nexpanding subset of the language. The paper outlines the iterative\ndevelopmental phase of the compiler, emphasizing progressive enhancements in\ncapabilities and functionality. Adherence to industry best practices of modular\ndesign, code reusability, and documentation has enabled the resulting\ncompiler's functional efficiency, maintainability, and extensibility. The\ncompiler proved to be effective not only in managing the language structure but\nalso in developing optimized code, which demonstrates its practical usability.\nThis was also further assessed using the compiler on a tiny memory-deficient\nsingle-board computer, again showing the compiler's efficiency and suitability\nfor resource-constrained devices.\n","authors":["Debasish Dutta","Neeharika Sonowal","Irani Hazarika"],"pdf_url":"https://arxiv.org/pdf/2501.04503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04489v1","updated":"2025-01-08T13:16:22Z","published":"2025-01-08T13:16:22Z","title":"Collaborative Inference Acceleration with Non-Penetrative Tensor\n  Partitioning","summary":"  The inference of large-sized images on Internet of Things (IoT) devices is\ncommonly hindered by limited resources, while there are often stringent latency\nrequirements for Deep Neural Network (DNN) inference. Currently, this problem\nis generally addressed by collaborative inference, where the large-sized image\nis partitioned into multiple tiles, and each tile is assigned to an IoT device\nfor processing. However, since significant latency will be incurred due to the\ncommunication overhead caused by tile sharing, the existing collaborative\ninference strategy is inefficient for convolutional computation, which is\nindispensable for any DNN. To reduce it, we propose Non-Penetrative Tensor\nPartitioning (NPTP), a fine-grained tensor partitioning method that reduces the\ncommunication latency by minimizing the communication load of tiles shared,\nthereby reducing inference latency. We evaluate NPTP with four widely-adopted\nDNN models. Experimental results demonstrate that NPTP achieves a 1.44-1.68x\ninference speedup relative to CoEdge, a state-of-the-art (SOTA) collaborative\ninference algorithm.\n","authors":["Zhibang Liu","Chaonong Xu","Zhenjie Lv","Zhizhuo Liu","Suyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.04489v1.pdf","comment":"Accepted by the 2025 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2025)"},{"id":"http://arxiv.org/abs/2501.04483v1","updated":"2025-01-08T13:07:34Z","published":"2025-01-08T13:07:34Z","title":"Demystification and Near-perfect Estimation of Minimum Gas Limit and Gas\n  Used for Ethereum Smart Contracts","summary":"  The Ethereum blockchain has a \\emph{gas system} that associates operations\nwith a cost in gas units. Two central concepts of this system are the \\emph{gas\nlimit} assigned by the issuer of a transaction and the \\emph{gas used} by a\ntransaction. The former is a budget that must not be exhausted before the\ncompletion of the transaction execution; otherwise, the execution fails.\nTherefore, it seems rather essential to determine the \\emph{minimum gas limit}\nthat ensures the execution of a transaction will not abort due to the lack of\ngas. Despite its practical relevance, this concept has not been properly\naddressed. In the literature, gas used and minimum gas limit are conflated.\nThis paper proposes a precise notion of minimum gas limit and how it can differ\nfrom gas used by a transaction; this is also demonstrated with a quantitative\nstudy on real transactions of the Ethereum blockchain. Another significant\ncontribution is the proposition of a fairly precise estimator for each of the\ntwo metrics. Again, the confusion between these concepts has led to the\ncreation of estimators only for the gas used by a transaction. We demonstrate\nthat the minimum gas limit for the state of the Ethereum blockchain (after the\nblock) $t$ can serve as a near-perfect estimation for the execution of the\ntransaction at block $t + \\Delta$, where $\\Delta \\leq 11$; the same holds for\nestimating gas used. These precise estimators can be very valuable in helping\nthe users predict the gas budget of transactions and developers in optimising\ntheir smart contracts; over and underestimating gas used and minimum gas limit\ncan lead to a number of practical issues. Overall, this paper serves as an\nimportant reference for blockchain developers and users as to how the gas\nsystem really works.\n","authors":["Danilo Rafael de Lima Cabral","Pedro Antonino","Augusto Sampaio"],"pdf_url":"https://arxiv.org/pdf/2501.04483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.06300v3","updated":"2025-01-08T12:28:15Z","published":"2022-01-17T09:32:15Z","title":"Coded Distributed Computing with Pre-set Assignments of Data and Output\n  Functions","summary":"  Coded distributed computing can reduce the communication load for distributed\ncomputing systems by introducing redundant computation and creating\nmulticasting opportunities. However, the existing schemes require delicate data\nplacement and output function assignment, which is not feasible when\ndistributed nodes fetch data without the orchestration of a master node. In\nthis paper, we consider the general systems where the data placement and output\nfunction assignment are arbitrary but pre-set. We propose two coded computing\nschemes, One-shot Coded Transmission (OSCT) and Few-shot Coded Transmission\n(FSCT), to reduce the communication load. Both schemes first group the nodes\ninto clusters and divide the transmission of each cluster into multiple rounds,\nand then design coded transmission in each round to maximize the multicast\ngain. The key difference between OSCT and FSCT is that the former uses a\none-shot transmission where each encoded message can be decoded independently\nby the intended nodes, while the latter allows each node to jointly decode\nmultiple received symbols to achieve potentially larger multicast gains.\nFurthermore, based on the lower bound proposed by Yu et al., we derive\nsufficient conditions for the optimality of OSCT and FSCT, respectively. This\nnot only recovers the existing optimality results but also includes some cases\nwhere our schemes are optimal while others are not.\n","authors":["Yuhan Wang","Youlong Wu"],"pdf_url":"https://arxiv.org/pdf/2201.06300v3.pdf","comment":"Accepted by IEEE Transactions on Information Theory"},{"id":"http://arxiv.org/abs/2501.04443v1","updated":"2025-01-08T11:52:43Z","published":"2025-01-08T11:52:43Z","title":"Revisiting LocalSGD and SCAFFOLD: Improved Rates and Missing Analysis","summary":"  LocalSGD and SCAFFOLD are widely used methods in distributed stochastic\noptimization, with numerous applications in machine learning, large-scale data\nprocessing, and federated learning. However, rigorously establishing their\ntheoretical advantages over simpler methods, such as minibatch SGD (MbSGD), has\nproven challenging, as existing analyses often rely on strong assumptions,\nunrealistic premises, or overly restrictive scenarios.\n  In this work, we revisit the convergence properties of LocalSGD and SCAFFOLD\nunder a variety of existing or weaker conditions, including gradient\nsimilarity, Hessian similarity, weak convexity, and Lipschitz continuity of the\nHessian. Our analysis shows that (i) LocalSGD achieves faster convergence\ncompared to MbSGD for weakly convex functions without requiring stronger\ngradient similarity assumptions; (ii) LocalSGD benefits significantly from\nhigher-order similarity and smoothness; and (iii) SCAFFOLD demonstrates faster\nconvergence than MbSGD for a broader class of non-quadratic functions. These\ntheoretical insights provide a clearer understanding of the conditions under\nwhich LocalSGD and SCAFFOLD outperform MbSGD.\n","authors":["Ruichen Luo","Sebastian U Stich","Samuel Horváth","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2501.04443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03301v2","updated":"2025-01-08T11:47:25Z","published":"2025-01-06T15:19:26Z","title":"Rethinking Byzantine Robustness in Federated Recommendation from Sparse\n  Aggregation Perspective","summary":"  To preserve user privacy in recommender systems, federated recommendation\n(FR) based on federated learning (FL) emerges, keeping the personal data on the\nlocal client and updating a model collaboratively. Unlike FL, FR has a unique\nsparse aggregation mechanism, where the embedding of each item is updated by\nonly partial clients, instead of full clients in a dense aggregation of general\nFL. Recently, as an essential principle of FL, model security has received\nincreasing attention, especially for Byzantine attacks, where malicious clients\ncan send arbitrary updates. The problem of exploring the Byzantine robustness\nof FR is particularly critical since in the domains applying FR, e.g.,\ne-commerce, malicious clients can be injected easily by registering new\naccounts. However, existing Byzantine works neglect the unique sparse\naggregation of FR, making them unsuitable for our problem. Thus, we make the\nfirst effort to investigate Byzantine attacks on FR from the perspective of\nsparse aggregation, which is non-trivial: it is not clear how to define\nByzantine robustness under sparse aggregations and design Byzantine attacks\nunder limited knowledge/capability. In this paper, we reformulate the Byzantine\nrobustness under sparse aggregation by defining the aggregation for a single\nitem as the smallest execution unit. Then we propose a family of effective\nattack strategies, named Spattack, which exploit the vulnerability in sparse\naggregation and are categorized along the adversary's knowledge and capability.\nExtensive experimental results demonstrate that Spattack can effectively\nprevent convergence and even break down defenses under a few malicious clients,\nraising alarms for securing FR systems.\n","authors":["Zhongjian Zhang","Mengmei Zhang","Xiao Wang","Lingjuan Lyu","Bo Yan","Junping Du","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2501.03301v2.pdf","comment":"accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2309.14821v2","updated":"2025-01-08T10:39:19Z","published":"2023-09-26T10:39:59Z","title":"Shattering the Ephemeral Storage Cost Barrier for Data-Intensive\n  Serverless Workflows","summary":"  Serverless computing is a popular cloud deployment paradigm where developers\nimplement applications as workflows of functions that invoke each other. Cloud\nproviders automatically scale function instances on demand and forward workflow\nrequests to appropriate instances. However, current serverless clouds lack\nefficient cross-function data transfer, limiting the execution of\ndata-intensive applications. Functions often rely on third-party services like\nAWS S3, AWS ElastiCache, or multi-tier solutions for intermediate data\ntransfers, which introduces inefficiencies.\n  We demonstrate that such through-storage transfers make data-intensive\ndeployments economically impractical, with storage costs comprising more than\n24-99% of the total serverless bill. To address this, we introduce Zipline, a\nfast, API-preserving data communication method for serverless platforms.\nZipline enables direct function-to-function transfers, where the sender\nfunction buffers payloads in memory and sends a reference to the receiver. The\nreceiver retrieves the data directly from the sender's memory, guided by the\nload balancer and autoscaler. Zipline integrates seamlessly with existing\nautoscaling, maintains invocation semantics, and eliminates the costs and\noverheads of intermediate services. We prototype Zipline in vHive/Knative on\nAWS EC2 nodes, demonstrating significant improvements. Zipline reduces costs\nand enhances latency and bandwidth compared to AWS S3 (the lowest-cost\nsolution) and ElastiCache (the highest-performance solution). On real-world\napplications, Zipline lowers costs by 2-5x and reduces execution times by\n1.3-3.4x versus S3. Compared to ElastiCache, Zipline achieves 17-772x cost\nreductions while improving performance by 2-5%.\n","authors":["Dmitrii Ustiugov","Shyam Jesalpura","Mert Bora Alper","Michal Baczun","Rustem Feyzkhanov","Edouard Bugnion","Boris Grot","Marios Kogias"],"pdf_url":"https://arxiv.org/pdf/2309.14821v2.pdf","comment":"added cost reduction details"},{"id":"http://arxiv.org/abs/2501.04331v1","updated":"2025-01-08T08:05:18Z","published":"2025-01-08T08:05:18Z","title":"AutoDFL: A Scalable and Automated Reputation-Aware Decentralized\n  Federated Learning","summary":"  Blockchained federated learning (BFL) combines the concepts of federated\nlearning and blockchain technology to enhance privacy, security, and\ntransparency in collaborative machine learning models. However, implementing\nBFL frameworks poses challenges in terms of scalability and cost-effectiveness.\nReputation-aware BFL poses even more challenges, as blockchain validators are\ntasked with processing federated learning transactions along with the\ntransactions that evaluate FL tasks and aggregate reputations. This leads to\nfaster blockchain congestion and performance degradation. To improve BFL\nefficiency while increasing scalability and reducing on-chain reputation\nmanagement costs, this paper proposes AutoDFL, a scalable and automated\nreputation-aware decentralized federated learning framework. AutoDFL leverages\nzk-Rollups as a Layer-2 scaling solution to boost the performance while\nmaintaining the same level of security as the underlying Layer-1 blockchain.\nMoreover, AutoDFL introduces an automated and fair reputation model designed to\nincentivize federated learning actors. We develop a proof of concept for our\nframework for an accurate evaluation. Tested with various custom workloads,\nAutoDFL reaches an average throughput of over 3000 TPS with a gas reduction of\nup to 20X.\n","authors":["Meryem Malak Dif","Mouhamed Amine Bouchiha","Mourad Rabah","Yacine Ghamri-Doudane"],"pdf_url":"https://arxiv.org/pdf/2501.04331v1.pdf","comment":"Paper accepted at NOMS'2025 (pages 9, figures 5)"},{"id":"http://arxiv.org/abs/2501.04319v1","updated":"2025-01-08T07:32:54Z","published":"2025-01-08T07:32:54Z","title":"VerifBFL: Leveraging zk-SNARKs for A Verifiable Blockchained Federated\n  Learning","summary":"  Blockchain-based Federated Learning (FL) is an emerging decentralized machine\nlearning paradigm that enables model training without relying on a central\nserver. Although some BFL frameworks are considered privacy-preserving, they\nare still vulnerable to various attacks, including inference and model\npoisoning. Additionally, most of these solutions employ strong trust\nassumptions among all participating entities or introduce incentive mechanisms\nto encourage collaboration, making them susceptible to multiple security flaws.\nThis work presents VerifBFL, a trustless, privacy-preserving, and verifiable\nfederated learning framework that integrates blockchain technology and\ncryptographic protocols. By employing zero-knowledge Succinct Non-Interactive\nArgument of Knowledge (zk-SNARKs) and incrementally verifiable computation\n(IVC), VerifBFL ensures the verifiability of both local training and\naggregation processes. The proofs of training and aggregation are verified\non-chain, guaranteeing the integrity and auditability of each participant's\ncontributions. To protect training data from inference attacks, VerifBFL\nleverages differential privacy. Finally, to demonstrate the efficiency of the\nproposed protocols, we built a proof of concept using emerging tools. The\nresults show that generating proofs for local training and aggregation in\nVerifBFL takes less than 81s and 2s, respectively, while verifying them\non-chain takes less than 0.6s.\n","authors":["Ahmed Ayoub Bellachia","Mouhamed Amine Bouchiha","Yacine Ghamri-Doudane","Mourad Rabah"],"pdf_url":"https://arxiv.org/pdf/2501.04319v1.pdf","comment":"Paper accepted at NOMS'25 (9 pages, 6 Figures)"},{"id":"http://arxiv.org/abs/2501.04266v1","updated":"2025-01-08T04:19:57Z","published":"2025-01-08T04:19:57Z","title":"Scaling Large Language Model Training on Frontier with Low-Bandwidth\n  Partitioning","summary":"  Scaling up Large Language Model(LLM) training involves fitting a tremendous\namount of training parameters across a limited number of workers. However,\nmethods like ZeRO-3 that drastically reduce GPU memory pressure often incur\nheavy communication to ensure global synchronization and consistency.\nEstablished efforts such as ZeRO++ use secondary partitions to avoid inter-node\ncommunications, given that intra-node GPU-GPU transfer generally has more\nbandwidth and lower latency than inter-node connections. However, as more\ncapable infrastructure like Frontier, equipped with AMD GPUs, emerged with\nimpressive computing capability, there is a need for investigations on the\nhardware topology and to develop targeted strategies to improve training\nefficiency. In this work, we propose a collection of communication and\noptimization strategies for ZeRO++ to reduce communication costs and improve\nmemory utilization. In this paper, we propose a 3-level hierarchical\npartitioning specifically for the current Top-1 supercomputing cluster,\nFrontier, which aims at leveraging various bandwidths across layers of\ncommunications (GCD-GCD, GPU-GPU, and inter-node) to reduce communication\noverhead. For a 20B GPT model, we observe a 1.71x increase in TFLOPS per GPU\nwhen compared with ZeRO++ up to 384 GCDs and a scaling efficiency of 0.94 for\nup to 384 GCDs. To the best of our knowledge, our work is also the first effort\nto efficiently optimize LLM workloads on Frontier AMD GPUs.\n","authors":["Lang Xu","Quentin Anthony","Jacob Hatef","Aamir Shafi","Hari Subramoni","Dhabaleswar K."," Panda"],"pdf_url":"https://arxiv.org/pdf/2501.04266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04265v1","updated":"2025-01-08T04:19:53Z","published":"2025-01-08T04:19:53Z","title":"HiCoCS: High Concurrency Cross-Sharding on Permissioned Blockchains","summary":"  As the foundation of the Web3 trust system, blockchain technology faces\nincreasing demands for scalability. Sharding emerges as a promising solution,\nbut it struggles to handle highly concurrent cross-shard transactions\n(\\textsf{CSTx}s), primarily due to simultaneous ledger operations on the same\naccount. Hyperledger Fabric, a permissioned blockchain, employs multi-version\nconcurrency control for parallel processing. Existing solutions use channels\nand intermediaries to achieve cross-sharding in Hyperledger Fabric. However,\nthe conflict problem caused by highly concurrent \\textsf{CSTx}s has not been\nadequately resolved. To fill this gap, we propose HiCoCS, a high concurrency\ncross-shard scheme for permissioned blockchains. HiCoCS creates a unique\nvirtual sub-broker for each \\textsf{CSTx} by introducing a composite key\nstructure, enabling conflict-free concurrent transaction processing while\nreducing resource overhead. The challenge lies in managing large numbers of\ncomposite keys and mitigating intermediary privacy risks. HiCoCS utilizes\nvirtual sub-brokers to receive and process \\textsf{CSTx}s concurrently while\nmaintaining a transaction pool. Batch processing is employed to merge multiple\n\\textsf{CSTx}s in the pool, improving efficiency. We explore composite key\nreuse to reduce the number of virtual sub-brokers and lower system overhead.\nPrivacy preservation is enhanced using homomorphic encryption. Evaluations show\nthat HiCoCS improves cross-shard transaction throughput by 3.5-20.2 times\ncompared to the baselines.\n","authors":["Lingxiao Yang","Xuewen Dong","Zhiguo Wan","Di Lu","Yushu Zhang","Yulong Shen"],"pdf_url":"https://arxiv.org/pdf/2501.04265v1.pdf","comment":"Major Revised in IEEE TC"},{"id":"http://arxiv.org/abs/2501.04250v1","updated":"2025-01-08T03:18:41Z","published":"2025-01-08T03:18:41Z","title":"Publish on Ping: A Better Way to Publish Reservations in Memory\n  Reclamation for Concurrent Data Structures","summary":"  Safe memory reclamation techniques that utilize per read reservations, such\nas hazard pointers, often cause significant overhead in traversals of linked\nconcurrent data structures. This is primarily due to the need to announce a\nreservation, and fence to enforce appropriate ordering, before each read. In\nread-intensive workloads, this overhead is amplified because, even if\nrelatively little memory reclamation actually occurs, the full overhead of\nreserving records is still incurred while traversing data structures.\n  In this paper, we propose a novel memory reclamation technique by combining\nPOSIX signals and delayed reclamation, introducing a publish-on-ping approach.\nThis method eliminates the need to make reservations globally visible before\nuse. Instead, threads privately track which records they are accessing, and\nshare this information on demand with threads that intend to reclaim memory.\nThe approach can serve as a drop-in replacement for hazard pointers and hazard\neras. Furthermore, the capability to retain reservations during traversals in\ndata structure operations and publish them on demand facilitates the\nconstruction of a variant of hazard pointers (EpochPOP). This variant uses\nepochs to approach the performance of epoch-based reclamation in the common\ncase where threads are not frequently delayed (while retaining the robustness\nof hazard pointers).\n  Our publish-on-ping implementations based on hazard pointers (HP) and hazard\neras, when applied to various data structures, exhibit significant performance\nimprovements. The improvements across various workloads and data structures\nrange from 1.2X to 4X over the original HP, up to 20% compared to a heavily\noptimized HP implementation similar to the one in the Folly open-source\nlibrary, and up to 3X faster than hazard eras. EpochPOP delivers performance\nsimilar to epoch-based reclamation while providing stronger guarantees.\n","authors":["Ajay Singh","Trevor Brown"],"pdf_url":"https://arxiv.org/pdf/2501.04250v1.pdf","comment":"Extended version of full paper accepted at PPoPP '25: The 30th ACM\n  SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming\n  Proceedings"},{"id":"http://arxiv.org/abs/2501.04236v1","updated":"2025-01-08T02:30:26Z","published":"2025-01-08T02:30:26Z","title":"Splicer$^{+}$: Secure Hub Placement and Deadlock-Free Routing for\n  Payment Channel Network Scalability","summary":"  Payment channel hub (PCH) is a promising approach for payment channel\nnetworks (PCNs) to improve efficiency by deploying robust hubs to steadily\nprocess off-chain transactions. However, existing PCHs, often preplaced without\nconsidering payment request distribution across PCNs, can lead to load\nimbalance. PCNs' reliance on source routing, which makes decisions based solely\non individual sender requests, can degrade performance by overlooking other\nrequests, thus further impairing scalability. In this paper, we introduce\nSplicer$^{+}$, a highly scalable multi-PCH solution based on the trusted\nexecution environment (TEE). We study tradeoffs in communication overhead\nbetween participants, transform the original NP-hard PCH placement problem by\nmixed-integer linear programming, and propose optimal/approximate solutions\nwith load balancing for different PCN scales using supermodular techniques.\nConsidering global PCN states and local directly connected sender requests, we\ndesign a deadlock-free routing protocol for PCHs. It dynamically adjusts the\npayment processing rate across multiple channels and, combined with TEE,\nensures high-performance routing with confidential computation. We provide a\nformal security proof for the Splicer$^{+}$ protocol in the UC-framework.\nExtensive evaluations demonstrate the effectiveness of Splicer$^{+}$, with\ntransaction success ratio ($\\uparrow$51.1%), throughput ($\\uparrow$181.5%), and\nlatency outperforming state-of-the-art PCNs.\n","authors":["Lingxiao Yang","Xuewen Dong","Wei Wang","Sheng Gao","Qiang Qu","Wensheng Tian","Yulong Shen"],"pdf_url":"https://arxiv.org/pdf/2501.04236v1.pdf","comment":"Extended version of ICDCS 2023 (arXiv:2305.19182)"}]},"2025-01-07T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2501.03892v1","updated":"2025-01-07T16:00:40Z","published":"2025-01-07T16:00:40Z","title":"LEAP: LLM-powered End-to-end Automatic Library for Processing Social\n  Science Queries on Unstructured Data","summary":"  Social scientists are increasingly interested in analyzing the semantic\ninformation (e.g., emotion) of unstructured data (e.g., Tweets), where the\nsemantic information is not natively present. Performing this analysis in a\ncost-efficient manner requires using machine learning (ML) models to extract\nthe semantic information and subsequently analyze the now structured data.\nHowever, this process remains challenging for domain experts.\n  To demonstrate the challenges in social science analytics, we collect a\ndataset, QUIET-ML, of 120 real-world social science queries in natural language\nand their ground truth answers. Existing systems struggle with these queries\nsince (1) they require selecting and applying ML models, and (2) more than a\nquarter of these queries are vague, making standard tools like natural language\nto SQL systems unsuited. To address these issues, we develop LEAP, an\nend-to-end library that answers social science queries in natural language with\nML. LEAP filters vague queries to ensure that the answers are deterministic and\nselects from internally supported and user-defined ML functions to extend the\nunstructured data to structured tables with necessary annotations. LEAP further\ngenerates and executes code to respond to these natural language queries. LEAP\nachieves a 100% pass @ 3 and 92% pass @ 1 on QUIET-ML, with a \\$1.06 average\nend-to-end cost, of which code generation costs \\$0.02.\n","authors":["Chuxuan Hu","Austin Peters","Daniel Kang"],"pdf_url":"https://arxiv.org/pdf/2501.03892v1.pdf","comment":"Accepted to VLDB 2025"},{"id":"http://arxiv.org/abs/2501.03850v1","updated":"2025-01-07T15:07:07Z","published":"2025-01-07T15:07:07Z","title":"Partitioning Strategies for Parallel Computation of Flexible Skylines","summary":"  While classical skyline queries identify interesting data within large\ndatasets, flexible skylines introduce preferences through constraints on\nattribute weights, and further reduce the data returned. However, computing\nthese queries can be time-consuming for large datasets. We propose and\nimplement a parallel computation scheme consisting of a parallel phase followed\nby a sequential phase, and apply it to flexible skylines. We assess the\nadditional effect of an initial filtering phase to reduce dataset size before\nparallel processing, and the elimination of the sequential part (the most\ntime-consuming) altogether. All our experiments are executed in the PySpark\nframework for a number of different datasets of varying sizes and dimensions.\n","authors":["Emilio De Lorenzis","Davide Martinenghi"],"pdf_url":"https://arxiv.org/pdf/2501.03850v1.pdf","comment":"21 pages, 11 figures"},{"id":"http://arxiv.org/abs/2501.03647v1","updated":"2025-01-07T09:27:25Z","published":"2025-01-07T09:27:25Z","title":"Hierarchical Datacubes","summary":"  Many approaches have been proposed to pre-compute data cubes in order to\nefficiently respond to OLAP queries in data warehouses. However, few have\nproposed solutions integrating all of the possible outcomes, and it is this\nidea that leads the integration of hierarchical dimensions into these\nresponses. To meet this need, we propose, in this paper, a complete\nredefinition of the framework and the formal definition of traditional database\nanalysis through the prism of hierarchical dimensions. After characterizing the\nhierarchical data cube lattice, we introduce the hierarchical data cube and its\nmost concise reduced representation, the closed hierarchical data cube. It\noffers compact replication so as to optimize storage space by removing\nredundancies of strongly correlated data. Such data are typical of data\nwarehouses, and in particular in video games, our field of study and\nexperimentation, where hierarchical dimension attributes are widely\nrepresented.\n","authors":["Mickaël Martin Nevot","Sébastien Nedjar","Lotfi Lakhal"],"pdf_url":"https://arxiv.org/pdf/2501.03647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03639v1","updated":"2025-01-07T09:15:25Z","published":"2025-01-07T09:15:25Z","title":"A case study on the transformative potential of AI in software\n  engineering on LeetCode and ChatGPT","summary":"  The recent surge in the field of generative artificial intelligence (GenAI)\nhas the potential to bring about transformative changes across a range of\nsectors, including software engineering and education. As GenAI tools, such as\nOpenAI's ChatGPT, are increasingly utilised in software engineering, it becomes\nimperative to understand the impact of these technologies on the software\nproduct. This study employs a methodological approach, comprising web scraping\nand data mining from LeetCode, with the objective of comparing the software\nquality of Python programs produced by LeetCode users with that generated by\nGPT-4o. In order to gain insight into these matters, this study addresses the\nquestion whether GPT-4o produces software of superior quality to that produced\nby humans.\n  The findings indicate that GPT-4o does not present a considerable impediment\nto code quality, understandability, or runtime when generating code on a\nlimited scale. Indeed, the generated code even exhibits significantly lower\nvalues across all three metrics in comparison to the user-written code.\nHowever, no significantly superior values were observed for the generated code\nin terms of memory usage in comparison to the user code, which contravened the\nexpectations. Furthermore, it will be demonstrated that GPT-4o encountered\nchallenges in generalising to problems that were not included in the training\ndata set.\n  This contribution presents a first large-scale study comparing generated code\nwith human-written code based on LeetCode platform based on multiple measures\nincluding code quality, code understandability, time behaviour and resource\nutilisation. All data is publicly available for further research.\n","authors":["Manuel Merkel","Jens Dörpinghaus"],"pdf_url":"https://arxiv.org/pdf/2501.03639v1.pdf","comment":null}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2408.13510v2","updated":"2025-01-07T18:16:17Z","published":"2024-08-24T08:12:22Z","title":"Intelligent Router for LLM Workloads: Improving Performance Through\n  Workload-Aware Load Balancing","summary":"  Large Language Model (LLM) workloads have distinct prefill and decode phases\nwith different compute and memory requirements which should ideally be\naccounted for when scheduling input queries across different LLM instances in a\ncluster. However existing scheduling algorithms treat LLM workloads as\nmonolithic jobs without considering the distinct characteristics of the two\nphases in each workload. This leads to sub-optimal scheduling and increased\nresponse latency. In this work, we start by characterizing factors affecting\nthe response latency during LLM inference serving. We establish that better\nload balancing of inference requests across the available LLM instances can\nimprove the end-to-end latency to a larger extent than merely focusing on\noptimizing the instance-level scheduler. Motivated by our findings, we propose\na heuristic-guided reinforcement learning-based intelligent router for\ndata-driven and workload-aware scheduling. Our router schedules queries across\nLLM instances by leveraging a trainable response-length predictor, and a novel\nformulation for estimating the impact of mixing different workloads and\nachieves over 11% lower end-to-end latency than existing approaches on a mix of\npublic datasets and 7.8% lower end-to-end latency on real workload data with\ndiverse input and output trends from Cloud Provider X. Additionally, the\nproposed framework can also serve as a standard for benchmarking different LLM\ninference schedulers since it provides the best latency for a given model,\nhardware, and instance-level scheduler combination.\n","authors":["Kunal Jain","Anjaly Parayil","Ankur Mallick","Esha Choukse","Xiaoting Qin","Jue Zhang","Íñigo Goiri","Rujia Wang","Chetan Bansal","Victor Rühle","Anoop Kulkarni","Steve Kofsky","Saravan Rajmohan"],"pdf_url":"https://arxiv.org/pdf/2408.13510v2.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2501.03695v1","updated":"2025-01-07T10:50:15Z","published":"2025-01-07T10:50:15Z","title":"Unraveling Responsiveness of Chained BFT Consensus with Network Delay","summary":"  With the advancement of blockchain technology, chained Byzantine Fault\nTolerant (BFT) protocols have been increasingly adopted in practical systems,\nmaking their performance a crucial aspect of the study. In this paper, we\nintroduce a unified framework utilizing Markov Decision Processes (MDP) to\nmodel and assess the performance of three prominent chained BFT protocols. Our\nframework effectively captures complex adversarial behaviors, focusing on two\nkey performance metrics: chain growth and commitment rate. We implement the\noptimal attack strategies obtained from MDP analysis on an existing evaluation\nplatform for chained BFT protocols and conduct extensive experiments under\nvarious settings to validate our theoretical results. Through rigorous\ntheoretical analysis and thorough practical experiments, we provide an in-depth\nevaluation of chained BFT protocols under diverse attack scenarios, uncovering\noptimal attack strategies. Contrary to conventional belief, our findings reveal\nthat while responsiveness can enhance performance, it is not universally\nbeneficial across all scenarios. This work not only deepens our understanding\nof chained BFT protocols, but also offers valuable insights and analytical\ntools that can inform the design of more robust and efficient protocols.\n","authors":["Yining Tang","Qihang Luo","Runchao Han","Jianyu Niu","Chen Feng","Yinqian Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.03695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03649v1","updated":"2025-01-07T09:31:32Z","published":"2025-01-07T09:31:32Z","title":"On the Locality of Hall's Theorem","summary":"  The last five years of research on distributed graph algorithms have seen\nhuge leaps of progress, both regarding algorithmic improvements and\nimpossibility results: new strong lower bounds have emerged for many central\nproblems and exponential improvements over the state of the art have been\nachieved for the runtimes of many algorithms. Nevertheless, there are still\nlarge gaps between the best known upper and lower bounds for many important\nproblems. The current lower bound techniques for deterministic algorithms are\noften tailored to obtaining a logarithmic bound and essentially cannot be used\nto prove lower bounds beyond $\\Omega(\\log n)$. In contrast, the best\ndeterministic upper bounds are often polylogarithmic, raising the fundamental\nquestion of how to resolve the gap between logarithmic lower and\npolylogarithmic upper bounds and finally obtain tight bounds. We develop a\nnovel algorithm design technique aimed at closing this gap. In essence, each\nnode finds a carefully chosen local solution in $O(\\log n)$ rounds and we\nguarantee that this solution is consistent with the other nodes' solutions\nwithout coordination. The local solutions are based on a distributed version of\nHall's theorem that may be of independent interest and motivates the title of\nthis work. We showcase our framework by improving on the state of the art for\nthe following fundamental problems: edge coloring, bipartite saturating\nmatchings and hypergraph sinkless orientation. In particular, we obtain an\nasymptotically optimal $O(\\log n)$-round algorithm for $3\\Delta/2$-edge\ncoloring in bounded degree graphs. The previously best bound for the problem\nwas $O(\\log^4 n)$ rounds, obtained by plugging in the state-of-the-art maximal\nindependent set algorithm from arXiv:2303.16043 into the $3\\Delta/2$-edge\ncoloring algorithm from arXiv:1711.05469 .\n","authors":["Sebastian Brandt","Yannic Maus","Ananth Narayanan","Florian Schager","Jara Uitto"],"pdf_url":"https://arxiv.org/pdf/2501.03649v1.pdf","comment":"To appear at SODA'25"},{"id":"http://arxiv.org/abs/2501.04058v1","updated":"2025-01-07T07:42:41Z","published":"2025-01-07T07:42:41Z","title":"Homomorphic Encryption in Healthcare Industry Applications for\n  Protecting Data Privacy","summary":"  Focussing on two different use cases-Quality Control methods in industrial\ncontexts and Neural Network algorithms for healthcare diagnostics-this research\ninvestigates the inclusion of Fully Homomorphic Encryption into real-world\napplications in the healthcare sector. We evaluate the performance, resource\nrequirements, and viability of deploying FHE in these settings through\nextensive testing and analysis, highlighting the progress made in FHE tooling\nand the obstacles still facing addressing the gap between conceptual research\nand practical applications. We start our research by describing the specific\ncase study and trust model were working with. Choosing the two FHE frameworks\nmost appropriate for industry development, we assess the resources and\nperformance requirements for implementing each of the two FHE frameworks in the\nfirst scenario, Quality Control algorithms. In conclusion, our findings\ndemonstrate the effectiveness and resource consumption of the two use\ncases-complex NN models and simple QC algorithms-when implemented in an FHE\nsetting.\n","authors":["J. S. Rauthan"],"pdf_url":"https://arxiv.org/pdf/2501.04058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15320v2","updated":"2025-01-07T06:39:29Z","published":"2024-07-07T09:25:52Z","title":"Edge Graph Intelligence: Reciprocally Empowering Edge Networks with\n  Graph Intelligence","summary":"  Recent years have witnessed a thriving growth of computing facilities\nconnected at the network edge, cultivating edge networks as a fundamental\ninfrastructure for supporting miscellaneous intelligent services.Meanwhile,\nArtificial Intelligence (AI) frontiers have extrapolated to the graph domain\nand promoted Graph Intelligence (GI). Given the inherent relation between\ngraphs and networks, the interdiscipline of graph learning and edge networks,\ni.e., Edge GI or EGI, has revealed a novel interplay between them -- GI aids in\noptimizing edge networks, while edge networks facilitate GI model deployment.\nDriven by this delicate closed-loop, EGI is recognized as a promising solution\nto fully unleash the potential of edge computing power and is garnering growing\nattention. Nevertheless, research on EGI remains nascent, and there is a\nsoaring demand within both the communications and AI communities for a\ndedicated venue to share recent advancements. To this end, this paper promotes\nthe concept of EGI, explores its scope and core principles, and conducts a\ncomprehensive survey concerning recent research efforts on this emerging field.\nSpecifically, this paper introduces and discusses: 1) fundamentals of edge\ncomputing and graph learning,2) emerging techniques centering on the closed\nloop between graph intelligence and edge networks, and 3) open challenges and\nresearch opportunities of future EGI. By bridging the gap across communication,\nnetworking, and graph learning areas, we believe that this survey can garner\nincreased attention, foster meaningful discussions, and inspire further\nresearch ideas in EGI.\n","authors":["Liekang Zeng","Shengyuan Ye","Xu Chen","Xiaoxi Zhang","Ju Ren","Jian Tang","Yang Yang"," Xuemin"," Shen"],"pdf_url":"https://arxiv.org/pdf/2407.15320v2.pdf","comment":"Accepted by IEEE Communications Surveys & Tutorials"},{"id":"http://arxiv.org/abs/2501.03547v1","updated":"2025-01-07T05:47:35Z","published":"2025-01-07T05:47:35Z","title":"Metric Criticality Identification for Cloud Microservices","summary":"  For Site Reliability Engineers, alerts are typically the first and often the\nprimary indications that a system may not be performing as expected. Once\nalerts are triggered, Site Reliability Engineers delve into detailed data\nacross various modalities such as metrics, logs, and traces - to diagnose\nsystem issues. However, defining an optimal set of alerts is increasingly\nchallenging due to the sheer volume of multi-modal observability data points in\nlarge cloud-native systems. Typically, alerts are manually curated, primarily\ndefined on the metrics modality, and heavily reliant on subject matter experts\nmanually navigating through the large state-space of intricate relationships in\nmulti-modal observability data. Such a process renders defining alerts prone to\ninsufficient coverage, potentially missing critical events. Defining alerts is\neven more challenging with the shift from traditional monolithic architectures\nto microservice based architectures due to the intricate interplay between\nmicroservices governed by the application topology in an ever stochastic\nenvironment. To tackle this issue, we take a data driven approach wherein we\npropose KIMetrix, a system that relies only on historical metric data and\nlightweight microservice traces to identify microservice metric criticality.\nKIMetrix significantly aids Subject Matter Experts by identifying a critical\nset of metrics to define alerts, averting the necessity of weaving through the\nvast multi-modal observability sphere. KIMetrix delves deep into the\nmetric-trace coupling and leverages information theoretic measures to recommend\nmicroservice-metric mappings in a microservice topology-aware manner.\nExperimental evaluation on state-of-the-art microservice based applications\ndemonstrates the effectiveness of our approach.\n","authors":["Akanksha Singal","Divya Pathak","Kaustabha Ray","Felix George","Mudit Verma","Pratibha Moogi"],"pdf_url":"https://arxiv.org/pdf/2501.03547v1.pdf","comment":null}]}}