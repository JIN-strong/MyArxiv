<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-14T00:00:00Z">2025-01-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Dataframe <span class="highlight-title">System</span>s: Lazy Fat Pandas on a Diet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhushan Pal Singh, Priyesh Kumar, Chiranmoy Bhattacharya, S. Sudarshan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pandas is widely used for data science applications, but users often run into
problems when datasets are larger than memory. There are several frameworks
based on lazy evaluation that handle large datasets, but the programs have to
be rewritten to suit the framework, and the presence of multiple frameworks
complicates the life of a programmer. In this paper we present a framework that
allows programmers to code in plain Pandas; with just two lines of code changed
by the user, our system optimizes the program using a combination of
just-in-time static analysis, and runtime optimization based on a lazy
dataframe wrapper framework. Moreover, our system allows the programmer to
choose the backend. It works seamlessly with Pandas, Dask, and Modin, allowing
the choice of the best-suited backend for an application based on factors such
as data size. Performance results on a variety of programs show the benefits of
our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Evaluation of Serverless Cloud Infrastructure for
  Large-Scale Data Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Bodner, Theo Radig, David Justen, Daniel Ritter, Tilmann Rabl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data processing systems are increasingly deployed in the cloud. While
monolithic systems run fully on virtual servers, recent systems embrace cloud
infrastructure and utilize the disaggregation of compute and storage to scale
them independently. The introduction of serverless compute services, such as
AWS Lambda, enables finer-grained and elastic scalability within these systems.
Prior work shows the viability of serverless infrastructure for scalable data
processing yet also sees limitations due to variable performance and cost
overhead, in particular for networking and storage.
  In this paper, we perform a detailed analysis of the performance and cost
characteristics of serverless infrastructure in the data processing context. We
base our analysis on a large series of micro-benchmarks across different
compute and storage services, as well as end-to-end workloads. To enable our
analysis, we propose the Skyrise serverless evaluation platform. For the widely
used serverless infrastructure of AWS, our analysis reveals distinct boundaries
for performance variability in serverless networks and storage. We further
present cost break-even points for serverless compute and storage. These
insights provide guidance on when and how serverless infrastructure can be
efficiently used for data processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-variable Quantification of BDDs in External Memory using Nested
  Sweeping (Extended Paper) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steffan Christ Sølvsten, Jaco van de Pol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research on the Adiar BDD package has been successful at designing
algorithms capable of handling large Binary Decision Diagrams (BDDs) stored in
external memory. To do so, it uses consecutive sweeps through the BDDs to
resolve computations. Yet, this approach has kept algorithms for multi-variable
quantification, the relational product, and variable reordering out of its
scope.
  In this work, we address this by introducing the nested sweeping framework.
Here, multiple concurrent sweeps pass information between eachother to compute
the result. We have implemented the framework in Adiar and used it to create a
new external memory multi-variable quantification algorithm. Compared to
conventional depth-first implementations, Adiar with nested sweeping is able to
solve more instances of our benchmarks and/or solve them faster.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 14 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QMDB: Quick Merkle Database 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Zhang, Ryan Zarick, Daniel Wong, Thomas Kim, Bryan Pellegrino, Mignon Li, Kelvin Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain
state management by integrating key-value (KV) and Merkle tree storage into a
single unified architecture. QMDB delivers a significant throughput improvement
over existing architectures, achieving up to 6X over the widely used RocksDB
and 8X over NOMT, a leading verifiable database. Its novel append-only
twig-based design enables one SSD read per state access, O(1) IOs for updates,
and in-memory Merkleization on a memory footprint as small as 2.3 bytes per
entry, enabling it to run on even modest consumer-grade PCs. QMDB scales
seamlessly across both commodity and enterprise hardware, achieving up to 2.28
million state updates per second. This performance enables support for 1
million token transfers per second (TPS), marking QMDB as the first solution
achieving such a milestone. QMDB has been benchmarked with workloads exceeding
15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to
scale to 280 billion entries on a single server. Furthermore, QMDB introduces
historical proofs, unlocking the ability to query its blockchain's historical
state at the latest block. QMDB not only meets the demands of current
blockchains but also provides a robust foundation for building scalable,
efficient, and verifiable decentralized applications across diverse use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A GPU-Accelerated Distributed Algorithm for Optimal Power Flow in
  Distribution <span class="highlight-title">System</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minseok Ryu, Geunyeong Byeon, Kibaek Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a GPU-accelerated distributed optimization algorithm for
controlling multi-phase optimal power flow in active distribution systems with
dynamically changing topologies. To handle varying network configurations and
enable adaptable decomposition, we advocate a componentwise decomposition
strategy. However, this approach can lead to a prolonged computation time
mainly due to the excessive iterations required for achieving consensus among a
large number of fine-grained components. To overcome this, we introduce a
technique that segregates equality constraints from inequality constraints,
enabling GPU parallelism to reduce per-iteration time by orders of magnitude,
thereby significantly accelerating the overall computation. Numerical
experiments on IEEE test systems ranging from 13 to 8500 buses demonstrate the
superior scalability of the proposed approach compared to its CPU-based
counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM
  Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Caner Yüzügüler, Jiawei Zhuang, Lukas Cavigelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are widely used across various applications, but
their substantial computational requirements pose significant challenges,
particularly in terms of HBM bandwidth bottlenecks and inter-device
communication overhead. In this paper, we present PRESERVE, a novel prefetching
framework designed to optimize LLM inference by overlapping memory reads for
model weights and KV-cache with collective communication operations. Through
extensive experiments conducted on commercial AI accelerators, we demonstrate
up to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.
Additionally, we perform a design space exploration that identifies the optimal
hardware configuration for the proposed method, showing a further 1.25x
improvement in performance per cost by selecting the optimal L2 cache size. Our
results show that PRESERVE has the potential to mitigate the memory bottlenecks
and communication overheads, offering a solution to improve the performance and
scalability of the LLM inference systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Autoscaling for Large Language Model Serving with Chiron 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archit Patke, Dhemath Reddy, Saurabh Jha, Chandra Narayanaswami, Zbigniew Kalbarczyk, Ravishankar Iyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) serving is becoming an increasingly important
workload for cloud providers. Based on performance SLO requirements, LLM
inference requests can be divided into (a) interactive requests that have tight
SLOs in the order of seconds, and (b) batch requests that have relaxed SLO in
the order of minutes to hours. These SLOs can degrade based on the arrival
rates, multiplexing, and configuration parameters, thus necessitating the use
of resource autoscaling on serving instances and their batch sizes. However,
previous autoscalers for LLM serving do not consider request SLOs leading to
unnecessary scaling and resource under-utilization. To address these
limitations, we introduce Chiron, an autoscaler that uses the idea of
hierarchical backpressure estimated using queue size, utilization, and SLOs.
Our experiments show that Chiron achieves up to 90% higher SLO attainment and
improves GPU efficiency by up to 70% compared to existing solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Technical Report: Exploring Automatic Model-Checking of the Ethereum
  specification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Konnov, Jure Kukovec, Thomas Pani, Roberto Saltini, Thanh Hai Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate automated model-checking of the Ethereum specification,
focusing on the Accountable Safety property of the 3SF consensus protocol. We
select 3SF due to its relevance and the unique challenges it poses for formal
verification. Our primary tools are TLA+ for specification and the Apalache
model checker for verification.
  Our formalization builds on the executable Python specification of 3SF. To
begin, we manually translate this specification into TLA+, revealing
significant combinatorial complexity in the definition of Accountable Safety.
To address these challenges, we introduce several layers of manual abstraction:
(1) replacing recursion with folds, (2) substituting abstract graphs with
integers, and (3) decomposing chain configurations. To cross-validate our
results, we develop alternative encodings in SMT (CVC5) and Alloy.
  Despite the inherent complexity, our results demonstrate that exhaustive
verification of Accountable Safety is feasible for small instances - supporting
up to 7 checkpoints and 24 validator votes. Moreover, no violations of
Accountable Safety are observed, even in slightly larger configurations. Beyond
these findings, our study highlights the importance of manual abstraction and
domain expertise in enhancing model-checking efficiency and showcases the
flexibility of TLA+ for managing intricate specifications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HgPCN: A Heterogeneous Architecture for E2E Embedded Point Cloud
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Gao, Chao Jiang, Wesley Piard, Xiangru Chen, Bhavesh Patel, Herman Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud is an important type of geometric data structure for many
embedded applications such as autonomous driving and augmented reality. Current
Point Cloud Networks (PCNs) have proven to achieve great success in using
inference to perform point cloud analysis, including object part segmentation,
shape classification, and so on. However, point cloud applications on the
computing edge require more than just the inference step. They require an
end-to-end (E2E) processing of the point cloud workloads: pre-processing of raw
data, input preparation, and inference to perform point cloud analysis. Current
PCN approaches to support end-to-end processing of point cloud workload cannot
meet the real-time latency requirement on the edge, i.e., the ability of the AI
service to keep up with the speed of raw data generation by 3D sensors. Latency
for end-to-end processing of the point cloud workloads stems from two reasons:
memory-intensive down-sampling in the pre-processing phase and the data
structuring step for input preparation in the inference phase. In this paper,
we present HgPCN, an end-to-end heterogeneous architecture for real-time
embedded point cloud applications. In HgPCN, we introduce two novel
methodologies based on spatial indexing to address the two identified
bottlenecks. In the Pre-processing Engine of HgPCN, an Octree-Indexed-Sampling
method is used to optimize the memory-intensive down-sampling bottleneck of the
pre-processing phase. In the Inference Engine, HgPCN extends a commercial DLA
with a customized Data Structuring Unit which is based on a Voxel-Expanded
Gathering method to fundamentally reduce the workload of the data structuring
step in the inference phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICRO2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Union: A Trust-minimized Bridge for Rootstock 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramon Amela, Shreemoy Mishra, Sergio Demian Lerner, Javier Álvarez Cid-Fuentes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Union, a trust-minimized bridge protocol that enables secure
transfer of BTC between Bitcoin and a secondary blockchain. The growing
ecosystem of blockchain systems built around Bitcoin has created a pressing
need for secure and efficient bridges to transfer BTC between networks while
preserving Bitcoin's security guarantees. Union employs a multi-party variant
of BitVMX, an optimistic proving system on Bitcoin, to create a bridge that
operates securely under the assumption that at least one participant remains
honest. This 1-of-n honest approach is strikingly different from the
conventional honest-majority assumption adopted by practically all federated
systems. The protocol introduces several innovations: a packet-based
architecture that allows security bonds to be reused for multiple bridge
operations, improving capital efficiency; a system of enablers to manage
functionaries participation and to enforce penalties; a flexible light client
framework adaptable to various blockchain architectures; and an efficient stop
watch mechanism to optimize time-lock management. Union is a practical and
scalable solution for Bitcoin interoperability that maintains strong security
guarantees and minimizes trust assumptions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoK: Design, Vulnerabilities, and Security Measures of Cryptocurrency
  Wallets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12874v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12874v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimika Erinle, Yathin Kethepalli, Yebo Feng, Jiahua Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of decentralised digital currencies powered by blockchain
technology, a new era of peer-to-peer transactions has commenced. The rapid
growth of the cryptocurrency economy has led to increased use of
transaction-enabling wallets, making them a focal point for security risks. As
the frequency of wallet-related incidents rises, there is a critical need for a
systematic approach to measure and evaluate these attacks, drawing lessons from
past incidents to enhance wallet security. In response, we introduce a
multi-dimensional design taxonomy for existing and novel wallets with various
design decisions. We classify existing industry wallets based on this taxonomy,
identify previously occurring vulnerabilities and discuss the security
implications of design decisions. We also systematise threats to the wallet
mechanism and analyse the adversary's goals, capabilities and required
knowledge. We present a multi-layered attack framework and investigate 84
incidents between 2012 and 2024, accounting for $5.4B. Following this, we
classify defence implementations for these attacks on the precautionary and
remedial axes. We map the mechanism and design decisions to vulnerabilities,
attacks, and possible defence methods to discuss various insights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Securing Distributed Network Digital Twin <span class="highlight-title">System</span>s Against Model
  Poisoning Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Zhang, Minghong Fang, Mingzhe Chen, Gaolei Li, Xi Lin, Yuchen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of 5G and beyond, the increasing complexity of wireless networks
necessitates innovative frameworks for efficient management and deployment.
Digital twins (DTs), embodying real-time monitoring, predictive configurations,
and enhanced decision-making capabilities, stand out as a promising solution in
this context. Within a time-series data-driven framework that effectively maps
wireless networks into digital counterparts, encapsulated by integrated
vertical and horizontal twinning phases, this study investigates the security
challenges in distributed network DT systems, which potentially undermine the
reliability of subsequent network applications such as wireless traffic
forecasting. Specifically, we consider a minimal-knowledge scenario for all
attackers, in that they do not have access to network data and other
specialized knowledge, yet can interact with previous iterations of
server-level models. In this context, we spotlight a novel fake traffic
injection attack designed to compromise a distributed network DT system for
wireless traffic prediction. In response, we then propose a defense mechanism,
termed global-local inconsistency detection (GLID), to counteract various model
poisoning threats. GLID strategically removes abnormal model parameters that
deviate beyond a particular percentile range, thereby fortifying the security
of network twinning process. Through extensive experiments on real-world
wireless traffic datasets, our experimental evaluations show that both our
attack and defense strategies significantly outperform existing baselines,
highlighting the importance of security measures in the design and
implementation of DTs for 5G and beyond network systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Internet of Things Journal (IoT-J). arXiv admin note:
  substantial text overlap with arXiv:2404.14389</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-Efficient Split Learning for Fine-Tuning Large Language Models in
  Edge Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuguang Li, Shaohua Wu, Liang Li, Songge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, we propose an energy-efficient split learning (SL) framework
for fine-tuning large language models (LLMs) using geo-distributed personal
data at the network edge, where LLMs are split and alternately across massive
mobile devices and an edge server. Considering the device heterogeneity and
channel dynamics in edge networks, a \underline{C}ut l\underline{A}yer and
computing \underline{R}esource \underline{D}ecision (CARD) algorithm is
developed to minimize training delay and energy consumption. Simulation results
demonstrate that the proposed approach reduces the average training delay and
server's energy consumption by 70.8% and 53.1%, compared to the benchmarks,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient Equilibrium in Online Learning: Theory and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasios N. Angelopoulos, Michael I. Jordan, Ryan J. Tibshirani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new perspective on online learning that we refer to as gradient
equilibrium: a sequence of iterates achieves gradient equilibrium if the
average of gradients of losses along the sequence converges to zero. In
general, this condition is not implied by nor implies sublinear regret. It
turns out that gradient equilibrium is achievable by standard online learning
methods such as gradient descent and mirror descent with constant step sizes
(rather than decaying step sizes, as is usually required for no regret).
Further, as we show through examples, gradient equilibrium translates into an
interpretable and meaningful property in online prediction problems spanning
regression, classification, quantile estimation, and others. Notably, we show
that the gradient equilibrium framework can be used to develop a debiasing
scheme for black-box predictions under arbitrary distribution shift, based on
simple post hoc online descent updates. We also show that post hoc gradient
updates can be used to calibrate predicted quantiles under distribution shift,
and that the framework leads to unbiased Elo scores for pairwise preference
prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at
  https://github.com/aangelopoulos/gradient-equilibrium/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Similarity Measure Between Functions with Applications to Statistical
  Learning and Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengpiao Huang, Kaizheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this note, we present a novel measure of similarity between two functions.
It quantifies how the sub-optimality gaps of two functions convert to each
other, and unifies several existing notions of functional similarity. We show
that it has convenient operation rules, and illustrate its use in empirical
risk minimization and non-stationary online optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Adversarial Post-Training for One-Step Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, Lu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diffusion models are widely used for image and video generation, but
their iterative generation process is slow and expansive. While existing
distillation approaches have demonstrated the potential for one-step generation
in the image domain, they still suffer from significant quality degradation. In
this work, we propose Adversarial Post-Training (APT) against real data
following diffusion pre-training for one-step video generation. To improve the
training stability and quality, we introduce several improvements to the model
architecture and training procedures, along with an approximated R1
regularization objective. Empirically, our experiments show that our
adversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,
24fps videos in real time using a single forward evaluation step. Additionally,
our model is capable of generating 1024px images in a single step, achieving
quality comparable to state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Path Loss Prediction Using Machine Learning with Extended Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Ethier, Mathieu Chateauvert, Ryan G. Dempsey, Alexis Bose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wireless communications rely on path loss modeling, which is most effective
when it includes the physical details of the propagation environment. Acquiring
this data has historically been challenging, but geographic information system
data is becoming increasingly available with higher resolution and accuracy.
Access to such details enables propagation models to more accurately predict
coverage and minimize interference in wireless deployments. Machine
learning-based modeling can significantly support this effort, with
feature-based approaches allowing for accurate, efficient, and scalable
propagation modeling. Building on previous work, we introduce an extended set
of features that improves prediction accuracy while, most importantly,
maintaining model generalization across a broad range of environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures, conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Graph Representations and Graph Neural Networks for
  Multivariate Time Series Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wennuo Yang, Shiling Wu, Yuzhi Zhou, Weicheng Xie, Linlin Shen, Siyang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate Time Series Classification (MTSC) enables the analysis if
complex temporal data, and thus serves as a cornerstone in various real-world
applications, ranging from healthcare to finance. Since the relationship among
variables in MTS usually contain crucial cues, a large number of graph-based
MTSC approaches have been proposed, as the graph topology and edges can
explicitly represent relationships among variables (channels), where not only
various MTS graph representation learning strategies but also different Graph
Neural Networks (GNNs) have been explored. Despite such progresses, there is no
comprehensive study that fairly benchmarks and investigates the performances of
existing widely-used graph representation learning strategies/GNN classifiers
in the application of different MTSC tasks. In this paper, we present the first
benchmark which systematically investigates the effectiveness of the
widely-used three node feature definition strategies, four edge feature
learning strategies and five GNN architecture, resulting in 60 different
variants for graph-based MTSC. These variants are developed and evaluated with
a standardized data pipeline and training/validation/testing strategy on 26
widely-used suspensor MTSC datasets. Our experiments highlight that node
features significantly influence MTSC performance, while the visualization of
edge features illustrates why adaptive edge learning outperforms other edge
feature learning methods. The code of the proposed benchmark is publicly
available at
\url{https://github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Polynomial Threshold Functions of Bounded Tree-Width: Some
  Explainability and Complexity Aspects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karine Chubarian, Johnny Joyce, Gyorgy Turan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tree-width of a multivariate polynomial is the tree-width of the
hypergraph with hyperedges corresponding to its terms. Multivariate polynomials
of bounded tree-width have been studied by Makowsky and Meer as a new sparsity
condition that allows for polynomial solvability of problems which are
intractable in general. We consider a variation on this theme for Boolean
variables. A representation of a Boolean function as the sign of a polynomial
is called a polynomial threshold representation. We discuss Boolean functions
representable as polynomial threshold functions of bounded tree-width and
present two applications to Bayesian network classifiers, a probabilistic
graphical model. Both applications are in Explainable Artificial Intelligence
(XAI), the research area dealing with the black-box nature of many recent
machine learning models. We also give a separation result between the
representational power of positive and general polynomial threshold functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 3 figures. To be published in Festschrift in honor of
  Johann A. Makowsky</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Avoiding subtraction and division of stochastic signals using
  normalizing flows: NFdeconvolve 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Pessoa, Max Schweiger, Lance W. Q. Xu, Tristan Manha, Ayush Saurabh, Julian Antolin Camarena, Steve Pressé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Across the scientific realm, we find ourselves subtracting or dividing
stochastic signals. For instance, consider a stochastic realization, $x$,
generated from the addition or multiplication of two stochastic signals $a$ and
$b$, namely $x=a+b$ or $x = ab$. For the $x=a+b$ example, $a$ can be
fluorescence background and $b$ the signal of interest whose statistics are to
be learned from the measured $x$. Similarly, when writing $x=ab$, $a$ can be
thought of as the illumination intensity and $b$ the density of fluorescent
molecules of interest. Yet dividing or subtracting stochastic signals amplifies
noise, and we ask instead whether, using the statistics of $a$ and the
measurement of $x$ as input, we can recover the statistics of $b$. Here, we
show how normalizing flows can generate an approximation of the probability
distribution over $b$, thereby avoiding subtraction or division altogether.
This method is implemented in our software package, NFdeconvolve, available on
GitHub with a tutorial linked in the main text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Bayesian Neural Networks Explicitly Model Input Uncertainty? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Valdenegro-Toro, Marco Zullich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inputs to machine learning models can have associated noise or uncertainties,
but they are often ignored and not modelled. It is unknown if Bayesian Neural
Networks and their approximations are able to consider uncertainty in their
inputs. In this paper we build a two input Bayesian Neural Network (mean and
standard deviation) and evaluate its capabilities for input uncertainty
estimation across different methods like Ensembles, MC-Dropout, and Flipout.
Our results indicate that only some uncertainty estimation methods for
approximate Bayesian NNs can model input uncertainty, in particular Ensembles
and Flipout.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11 figures, VISAPP 2025 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding Interpretable Logic Rules from Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuqin Geng, Xiaojie Xu, Zhaoyue Wang, Ziyu Zhao, Xujie Si
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep neural networks continue to excel across various domains, their
black-box nature has raised concerns about transparency and trust. In
particular, interpretability has become increasingly essential for applications
that demand high safety and knowledge rigor, such as drug discovery, autonomous
driving, and genomics. However, progress in understanding even the simplest
deep neural networks - such as fully connected networks - has been limited,
despite their role as foundational elements in state-of-the-art models like
ResNet and Transformer. In this paper, we address this challenge by introducing
NeuroLogic, a novel approach for decoding interpretable logic rules from neural
networks. NeuroLogic leverages neural activation patterns to capture the
model's critical decision-making processes, translating them into logical rules
represented by hidden predicates. Thanks to its flexible design in the
grounding phase, NeuroLogic can be adapted to a wide range of neural networks.
For simple fully connected neural networks, hidden predicates can be grounded
in certain split patterns of original input features to derive
decision-tree-like rules. For large, complex vision neural networks, NeuroLogic
grounds hidden predicates into high-level visual concepts that are
understandable to humans. Our empirical study demonstrates that NeuroLogic can
extract global and interpretable rules from state-of-the-art models such as
ResNet, a task at which existing work struggles. We believe NeuroLogic can help
pave the way for understanding the black-box nature of neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Driven Water Segmentation with deep learning models for Enhanced
  Flood Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjida Afrin Mou, Tasfia Noor Chowdhury, Adib Ibn Mannan, Sadia Nourin Mim, Lubana Tarannum, Tasrin Noman, Jamal Uddin Ahamed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flooding is a major natural hazard causing significant fatalities and
economic losses annually, with increasing frequency due to climate change.
Rapid and accurate flood detection and monitoring are crucial for mitigating
these impacts. This study compares the performance of three deep learning
models UNet, ResNet, and DeepLabv3 for pixelwise water segmentation to aid in
flood detection, utilizing images from drones, in field observations, and
social media. This study involves creating a new dataset that augments
wellknown benchmark datasets with flood-specific images, enhancing the
robustness of the models. The UNet, ResNet, and DeepLab v3 architectures are
tested to determine their effectiveness in various environmental conditions and
geographical locations, and the strengths and limitations of each model are
also discussed here, providing insights into their applicability in different
scenarios by predicting image segmentation masks. This fully automated approach
allows these models to isolate flooded areas in images, significantly reducing
processing time compared to traditional semi-automated methods. The outcome of
this study is to predict segmented masks for each image effected by a flood
disaster and the validation accuracy of these models. This methodology
facilitates timely and continuous flood monitoring, providing vital data for
emergency response teams to reduce loss of life and economic damages. It offers
a significant reduction in the time required to generate flood maps, cutting
down the manual processing time. Additionally, we present avenues for future
research, including the integration of multimodal data sources and the
development of robust deep learning architectures tailored specifically for
flood detection tasks. Overall, our work contributes to the advancement of
flood management strategies through innovative use of deep learning
technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiplayer Federated Learning: Reaching Equilibrium with Less
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        TaeHo Yoon, Sayantan Choudhury, Nicolas Loizou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Federated Learning (FL) approaches assume collaborative clients
with aligned objectives working towards a shared global model. However, in many
real-world scenarios, clients act as rational players with individual
objectives and strategic behaviors, a concept that existing FL frameworks are
not equipped to adequately address. To bridge this gap, we introduce
Multiplayer Federated Learning (MpFL), a novel framework that models the
clients in the FL environment as players in a game-theoretic context, aiming to
reach an equilibrium. In this scenario, each player tries to optimize their own
utility function, which may not align with the collective goal. Within MpFL, we
propose Per-Player Local Stochastic Gradient Descent (PEARL-SGD), an algorithm
in which each player/client performs local updates independently and
periodically communicates with other players. We theoretically analyze
PEARL-SGD and prove that it reaches a neighborhood of equilibrium with less
communication in the stochastic setup compared to its non-local counterpart.
Finally, we verify our theoretical findings through numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FDPP: Fine-tune Diffusion Policy with Human Preference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Chen, Devesh K. Jha, Masayoshi Tomizuka, Diego Romeres
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning from human demonstrations enables robots to perform
complex manipulation tasks and has recently witnessed huge success. However,
these techniques often struggle to adapt behavior to new preferences or changes
in the environment. To address these limitations, we propose Fine-tuning
Diffusion Policy with Human Preference (FDPP). FDPP learns a reward function
through preference-based learning. This reward is then used to fine-tune the
pre-trained policy with reinforcement learning (RL), resulting in alignment of
pre-trained policy with new human preferences while still solving the original
task. Our experiments across various robotic tasks and preferences demonstrate
that FDPP effectively customizes policy behavior without compromising
performance. Additionally, we show that incorporating Kullback-Leibler (KL)
regularization during fine-tuning prevents over-fitting and helps maintain the
competencies of the initial policy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eliciting In-context Retrieval and Reasoning for Long-context Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Qiu, Varun Embar, Yizhe Zhang, Navdeep Jaitly, Shay B. Cohen, Benjamin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in long-context language models (LCLMs) promise to
transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With
their expanded context windows, LCLMs can process entire knowledge bases and
perform retrieval and reasoning directly -- a capability we define as
In-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like
LOFT often overestimate LCLM performance by providing overly simplified
contexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs
in more realistic scenarios by including confounding passages retrieved with
strong retrievers. We then propose three methods to enhance LCLM performance:
(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which
uses attention heads to filter and de-noise long contexts during decoding, and
(3) joint retrieval head training alongside the generation head. Our evaluation
of five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with
our best approach applied to Mistral-7B: +17 and +15 points by Exact Match on
LOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised
fine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks
despite being a much smaller model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful
  Behaviors with Proximity Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Nöther, Adish Singla, Goran Radanović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has proposed automated red-teaming methods for testing the
vulnerabilities of a given target large language model (LLM). These methods use
red-teaming LLMs to uncover inputs that induce harmful behavior in a target
LLM. In this paper, we study red-teaming strategies that enable a targeted
security assessment. We propose an optimization framework for red-teaming with
proximity constraints, where the discovered prompts must be similar to
reference prompts from a given dataset. This dataset serves as a template for
the discovered prompts, anchoring the search for test-cases to specific topics,
writing styles, or types of harmful behavior. We show that established
auto-regressive model architectures do not perform well in this setting. We
therefore introduce a black-box red-teaming method inspired by text-diffusion
models: Diffusion for Auditing and Red-Teaming (DART). DART modifies the
reference prompt by perturbing it in the embedding space, directly controlling
the amount of change introduced. We systematically evaluate our method by
comparing its effectiveness with established methods based on model fine-tuning
and zero- and few-shot prompting. Our results show that DART is significantly
more effective at discovering harmful inputs in close proximity to the
reference prompt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an extended version of a paper published at AAAI 25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Deep Active Learning for Medical Imaging: Replay-Base
  Architecture for Context Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Daniel, M. Rita Verdelho, Catarina Barata, Carlos Santiago
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning for medical imaging faces challenges in adapting and
generalizing to new contexts. Additionally, it often lacks sufficient labeled
data for specific tasks requiring significant annotation effort. Continual
Learning (CL) tackles adaptability and generalizability by enabling lifelong
learning from a data stream while mitigating forgetting of previously learned
knowledge. Active Learning (AL) reduces the number of required annotations for
effective training. This work explores both approaches (CAL) to develop a novel
framework for robust medical image analysis. Based on the automatic recognition
of shifts in image characteristics, Replay-Base Architecture for Context
Adaptation (RBACA) employs a CL rehearsal method to continually learn from
diverse contexts, and an AL component to select the most informative instances
for annotation. A novel approach to evaluate CAL methods is established using a
defined metric denominated IL-Score, which allows for the simultaneous
assessment of transfer learning, forgetting, and final model performance. We
show that RBACA works in domain and class-incremental learning scenarios, by
assessing its IL-Score on the segmentation and diagnosis of cardiac images. The
results show that RBACA outperforms a baseline framework without CAL, and a
state-of-the-art CAL method across various memory sizes and annotation budgets.
Our code is available in https://github.com/RuiDaniel/RBACA .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kannan Parthasarathy, Karthik Vaidhyanathan, Rudra Dhar, Venkat Krishnamachari, Basil Muhammed, Adyansh Kakran, Sreemaee Akshathala, Shrikara Arun, Sumant Dubey, Mohan Veerubhotla, Amey Karan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud Operations (CloudOps) is a rapidly growing field focused on the
automated management and optimization of cloud infrastructure which is
essential for organizations navigating increasingly complex cloud environments.
MontyCloud Inc. is one of the major companies in the CloudOps domain that
leverages autonomous bots to manage cloud compliance, security, and continuous
operations. To make the platform more accessible and effective to the
customers, we leveraged the use of GenAI.
  Developing a GenAI-based solution for autonomous CloudOps for the existing
MontyCloud system presented us with various challenges such as i) diverse data
sources; ii) orchestration of multiple processes; and iii) handling complex
workflows to automate routine tasks. To this end, we developed MOYA, a
multi-agent framework that leverages GenAI and balances autonomy with the
necessary human control. This framework integrates various internal and
external systems and is optimized for factors like task orchestration,
security, and error mitigation while producing accurate, reliable, and relevant
insights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our
multi-agent system with the help of practitioners as well as using automated
checks demonstrate enhanced accuracy, responsiveness, and effectiveness over
non-agentic approaches across complex workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted as full paper to CAIN 2025
  (https://conf.researchr.org/home/cain-2025), co-located with ICSE 2025
  (https://conf.researchr.org/home/icse-2025). The paper was submitted to CAIN
  for review on 9 November 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images
  using Choquet Integral and Differential Evolution Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Reza Takhsha, Maryam Rastgarpour, Mozhgan Naderi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has profoundly impacted billions globally. It
challenges public health and healthcare systems due to its rapid spread and
severe respiratory effects. An effective strategy to mitigate the COVID-19
pandemic involves integrating testing to identify infected individuals. While
RT-PCR is considered the gold standard for diagnosing COVID-19, it has some
limitations such as the risk of false negatives. To address this problem, this
paper introduces a novel Deep Learning Diagnosis System that integrates
pre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemble
learning framework to achieve precise identification of COVID-19 cases from
Chest X-ray (CXR) images. We combine feature vectors from the final hidden
layers of pre-trained DCNNs using the Choquet integral to capture interactions
between different DCNNs that a linear approach cannot. We employed
Sugeno-$\lambda$ measure theory to derive fuzzy measures for subsets of
networks to enable aggregation. We utilized Differential Evolution to estimate
fuzzy densities. We developed a TensorFlow-based layer for Choquet operation to
facilitate efficient aggregation, due to the intricacies involved in
aggregating feature vectors. Experimental results on the COVIDx dataset show
that our ensemble model achieved 98\% accuracy in three-class classification
and 99.50\% in binary classification, outperforming its components-DenseNet-201
(97\% for three-class, 98.75\% for binary), Inception-v3 (96.25\% for
three-class, 98.50\% for binary), and Xception (94.50\% for three-class, 98\%
for binary)-and surpassing many previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Model and Preprocessing Verification for Machine
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbiao Li, Anisa Halimi, Xiaoqian Jiang, Jaideep Vaidya, Erman Ayday
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a framework for privacy-preserving verification of
machine learning models, focusing on models trained on sensitive data.
Integrating Local Differential Privacy (LDP) with model explanations from LIME
and SHAP, our framework enables robust verification without compromising
individual privacy. It addresses two key tasks: binary classification, to
verify if a target model was trained correctly by applying the appropriate
preprocessing steps, and multi-class classification, to identify specific
preprocessing errors. Evaluations on three real-world datasets-Diabetes, Adult,
and Student Record-demonstrate that while the ML-based approach is particularly
effective in binary tasks, the threshold-based method performs comparably in
multi-class tasks. Results indicate that although verification accuracy varies
across datasets and noise levels, the framework provides effective detection of
preprocessing errors, strong privacy guarantees, and practical applicability
for safeguarding sensitive data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrique Adrian Villarrubia-Martin, Luis Rodriguez-Benitez, David Muñoz-Valero, Giovanni Montana, Luis Jimenez-Linares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses a critical challenge in the high-speed passenger railway
industry: designing effective dynamic pricing strategies in the context of
competing and cooperating operators. To address this, a multi-agent
reinforcement learning (MARL) framework based on a non-zero-sum Markov game is
proposed, incorporating random utility models to capture passenger decision
making. Unlike prior studies in areas such as energy, airlines, and mobile
networks, dynamic pricing for railway systems using deep reinforcement learning
has received limited attention. A key contribution of this paper is a
parametrisable and versatile reinforcement learning simulator designed to model
a variety of railway network configurations and demand patterns while enabling
realistic, microscopic modelling of user behaviour, called RailPricing-RL. This
environment supports the proposed MARL framework, which models heterogeneous
agents competing to maximise individual profits while fostering cooperative
behaviour to synchronise connecting services. Experimental results validate the
framework, demonstrating how user preferences affect MARL performance and how
pricing policies influence passenger choices, utility, and overall system
dynamics. This study provides a foundation for advancing dynamic pricing
strategies in railway systems, aligning profitability with system-wide
efficiency, and supporting future research on optimising pricing policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeineb Haouari, Jonas Weidner, Ivan Ezhov, Aswathi Varma, Daniel Rueckert, Bjoern Menze, Benedikt Wiestler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Glioblastoma, a highly aggressive brain tumor, poses major challenges due to
its poor prognosis and high morbidity rates. Partial differential
equation-based models offer promising potential to enhance therapeutic outcomes
by simulating patient-specific tumor behavior for improved radiotherapy
planning. However, model calibration remains a bottleneck due to the high
computational demands of optimization methods like Monte Carlo sampling and
evolutionary algorithms. To address this, we recently introduced an approach
leveraging a neural forward solver with gradient-based optimization to
significantly reduce calibration time. This approach requires a highly accurate
and fully differentiable forward model. We investigate multiple architectures,
including (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a
3D Vision Transformer (ViT). The optimized TumorSurrogate achieved the best
overall results, excelling in both tumor outline matching and voxel-level
prediction of tumor cell concentration. It halved the MSE relative to the
baseline model and achieved the highest Dice score across all tumor cell
concentration thresholds. Our study demonstrates significant enhancement in
forward solver performance and outlines important future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Big Batch Bayesian Active Learning by Considering Predictive
  Probabilities <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian W. Ober, Samuel Power, Tom Diethe, Henry B. Moss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We observe that BatchBALD, a popular acquisition function for batch Bayesian
active learning for classification, can conflate epistemic and aleatoric
uncertainty, leading to suboptimal performance. Motivated by this observation,
we propose to focus on the predictive probabilities, which only exhibit
epistemic uncertainty. The result is an acquisition function that not only
performs better, but is also faster to evaluate, allowing for larger batches
than before.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures; presented as a lightning talk at the NeurIPS
  Workshop on Bayesian Decision-making and Uncertainty (BDU; 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Energy Efficiency and Performance Trade-offs in LLM
  Inference Across Tasks and DVFS Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown significant improvements in many
natural language processing (NLP) tasks, accelerating their rapid adoption
across many industries. These models are resource-intensive, requiring
extensive computational resources both during training and inference, leading
to increased energy consumption and negative environmental impact. As their
adoption accelerates, the sustainability of LLMs has become a critical issue,
necessitating strategies to optimize their runtime efficiency without
compromising performance. Hence, it is imperative to identify the parameters
that significantly influence the performance and energy efficiency of LLMs. To
that end, in this work, we investigate the effect of important parameters on
the performance and energy efficiency of LLMs during inference and examine
their trade-offs.
  First, we analyze how different types of models with varying numbers of
parameters and architectures perform on tasks like text generation, question
answering, and summarization by benchmarking LLMs such as Falcon-7B,
Mistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study
input and output sequence characteristics such as sequence length concerning
energy consumption, performance, and throughput. Finally, we explore the impact
of hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency
Scaling (DVFS), on the models' latency and energy efficiency. Our extensive
benchmarking and statistical analysis reveal many interesting findings,
uncovering how specific optimizations can reduce energy consumption while
maintaining throughput and accuracy. This study provides actionable insights
for researchers and practitioners to design energy-efficient LLM inference
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Feature Maps for Quantum Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navneet Singh, Shiva Raj Pokhrel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Machine Learning (QML) offers significant potential for complex tasks
like genome sequence classification, but quantum noise on Noisy
Intermediate-Scale Quantum (NISQ) devices poses practical challenges. This
study systematically evaluates how various quantum noise models including
dephasing, amplitude damping, depolarizing, thermal noise, bit-flip, and
phase-flip affect key QML algorithms (QSVC, Peg-QSVC, QNN, VQC) and feature
mapping techniques (ZFeatureMap, ZZFeatureMap, and PauliFeatureMap). Results
indicate that QSVC is notably robust under noise, whereas Peg-QSVC and QNN are
more sensitive, particularly to depolarizing and amplitude-damping noise. The
PauliFeatureMap is especially vulnerable, highlighting difficulties in
maintaining accurate classification under noisy conditions. These findings
underscore the critical importance of feature map selection and noise
mitigation strategies in optimizing QML for genomic classification, with
promising implications for personalized medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-driven <span class="highlight-title">system</span> identification using quadratic embeddings of
  nonlinear dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Klus, Joel-Pascal N'Konzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel data-driven method called QENDy (Quadratic Embedding of
Nonlinear Dynamics) that not only allows us to learn quadratic representations
of highly nonlinear dynamical systems, but also to identify the governing
equations. The approach is based on an embedding of the system into a
higher-dimensional feature space in which the dynamics become quadratic. Just
like SINDy (Sparse Identification of Nonlinear Dynamics), our method requires
trajectory data, time derivatives for the training data points, which can also
be estimated using finite difference approximations, and a set of preselected
basis functions, called dictionary. We illustrate the efficacy and accuracy of
QENDy with the aid of various benchmark problems and compare its performance
with SINDy and a deep learning method for identifying quadratic embeddings.
Furthermore, we analyze the convergence of QENDy and SINDy in the infinite data
limit, highlight their similarities and main differences, and compare the
quadratic embedding with linearization techniques based on the Koopman
operator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Globally Convergent Variational Inference <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Declan McNamara, Jackson Loper, Jeffrey Regier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In variational inference (VI), an approximation of the posterior distribution
is selected from a family of distributions through numerical optimization. With
the most common variational objective function, known as the evidence lower
bound (ELBO), only convergence to a local optimum can be guaranteed. In this
work, we instead establish the global convergence of a particular VI method.
This VI method, which may be considered an instance of neural posterior
estimation (NPE), minimizes an expectation of the inclusive (forward) KL
divergence to fit a variational distribution that is parameterized by a neural
network. Our convergence result relies on the neural tangent kernel (NTK) to
characterize the gradient dynamics that arise from considering the variational
objective in function space. In the asymptotic regime of a fixed,
positive-definite neural tangent kernel, we establish conditions under which
the variational objective admits a unique solution in a reproducing kernel
Hilbert space (RKHS). Then, we show that the gradient descent dynamics in
function space converge to this unique function. In ablation studies and
practical problems, we demonstrate that our results explain the behavior of NPE
in non-asymptotic finite-neuron settings, and show that NPE outperforms
ELBO-based optimization, which often converges to shallow local optima.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CWEval: Outcome-driven Evaluation on Functionality and Security of LLM
  Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjun Peng, Leyi Cui, Kele Huang, Junfeng Yang, Baishakhi Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have significantly aided developers by
generating or assisting in code writing, enhancing productivity across various
tasks. While identifying incorrect code is often straightforward, detecting
vulnerabilities in functionally correct code is more challenging, especially
for developers with limited security knowledge, which poses considerable
security risks of using LLM-generated code and underscores the need for robust
evaluation benchmarks that assess both functional correctness and security.
Current benchmarks like CyberSecEval and SecurityEval attempt to solve it but
are hindered by unclear and impractical specifications, failing to assess both
functionality and security accurately. To tackle these deficiencies, we
introduce CWEval, a novel outcome-driven evaluation framework designed to
enhance the evaluation of secure code generation by LLMs. This framework not
only assesses code functionality but also its security simultaneously with
high-quality task specifications and outcome-driven test oracles which provides
high accuracy. Coupled with CWEval-bench, a multilingual, security-critical
coding benchmark, CWEval provides a rigorous empirical security evaluation on
LLM-generated code, overcoming previous benchmarks' shortcomings. Through our
evaluations, CWEval reveals a notable portion of functional but insecure code
produced by LLMs, and shows a serious inaccuracy of previous evaluations,
ultimately contributing significantly to the field of secure code generation.
We open-source our artifact at: https://github.com/Co1lin/CWEval .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in LLM4Code 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-supervised Deep Hyperspectral Inpainting with the Plug and Play and
  Deep Image Prior Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Li, Mehrdad Yaghoobi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral images are typically composed of hundreds of narrow and
contiguous spectral bands, each containing information regarding the material
composition of the imaged scene. However, these images can be affected by
various sources of noise, distortions, or data loss, which can significantly
degrade their quality and usefulness. This paper introduces a convergent
guaranteed algorithm, LRS-PnP-DIP(1-Lip), which successfully addresses the
instability issue of DHP that has been reported before. The proposed algorithm
extends the successful joint low-rank and sparse model to further exploit the
underlying data structures beyond the conventional and sometimes restrictive
unions of subspace models. A stability analysis guarantees the convergence of
the proposed algorithm under mild assumptions , which is crucial for its
application in real-world scenarios. Extensive experiments demonstrate that the
proposed solution consistently delivers visually and quantitatively superior
inpainting results, establishing state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 9 Figures, 7 Tables. arXiv admin note: text overlap with
  arXiv:2306.08128</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Quantum Machine Learning for Genomic Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navneet Singh, Shiva Raj Pokhrel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Machine Learning (QML) continues to evolve, unlocking new
opportunities for diverse applications. In this study, we investigate and
evaluate the applicability of QML models for binary classification of genome
sequence data by employing various feature mapping techniques. We present an
open-source, independent Qiskit-based implementation to conduct experiments on
a benchmark genomic dataset. Our simulations reveal that the interplay between
feature mapping techniques and QML algorithms significantly influences
performance. Notably, the Pegasos Quantum Support Vector Classifier
(Pegasos-QSVC) exhibits high sensitivity, particularly excelling in recall
metrics, while Quantum Neural Networks (QNN) achieve the highest training
accuracy across all feature maps. However, the pronounced variability in
classifier performance, dependent on feature mapping, highlights the risk of
overfitting to localized output distributions in certain scenarios. This work
underscores the transformative potential of QML for genomic data classification
while emphasizing the need for continued advancements to enhance the robustness
and accuracy of these methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Critical Synthesis of Uncertainty Quantification and Foundation Models
  in Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Landgraf, Rongjun Qin, Markus Ulrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent foundation models have enabled significant breakthroughs in
monocular depth estimation, a clear path towards safe and reliable deployment
in the real-world remains elusive. Metric depth estimation, which involves
predicting absolute distances, poses particular challenges, as even the most
advanced foundation models remain prone to critical errors. Since quantifying
the uncertainty has emerged as a promising endeavor to address these
limitations and enable trustworthy deployment, we fuse five different
uncertainty quantification methods with the current state-of-the-art
DepthAnythingV2 foundation model. To cover a wide range of metric depth
domains, we evaluate their performance on four diverse datasets. Our findings
identify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a
particularly promising approach, offering reliable uncertainty estimates while
maintaining predictive performance and computational efficiency on par with the
baseline, encompassing both training and inference time. By fusing uncertainty
quantification and foundation models within the context of monocular depth
estimation, this paper lays a critical foundation for future research aimed at
improving not only model performance but also its explainability. Extending
this critical synthesis of uncertainty quantification and foundation models
into other crucial tasks, such as semantic segmentation and pose estimation,
presents exciting opportunities for safer and more reliable machine vision
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction
  Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Fang, Xinle Deng, Kangwei Liu, Ningyu Zhang, Jingyang Qian, Penghui Yang, Xiaohui Fan, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models excel at interpreting complex natural language
instructions, enabling them to perform a wide range of tasks. In the life
sciences, single-cell RNA sequencing (scRNA-seq) data serves as the "language
of cellular biology", capturing intricate gene expression patterns at the
single-cell level. However, interacting with this "language" through
conventional tools is often inefficient and unintuitive, posing challenges for
researchers. To address these limitations, we present InstructCell, a
multi-modal AI copilot that leverages natural language as a medium for more
direct and flexible single-cell analysis. We construct a comprehensive
multi-modal instruction dataset that pairs text-based instructions with
scRNA-seq profiles from diverse tissues and species. Building on this, we
develop a multi-modal cell language architecture capable of simultaneously
interpreting and processing both modalities. InstructCell empowers researchers
to accomplish critical tasks-such as cell type annotation, conditional
pseudo-cell generation, and drug sensitivity prediction-using straightforward
natural language commands. Extensive evaluations demonstrate that InstructCell
consistently meets or exceeds the performance of existing single-cell
foundation models, while adapting to diverse experimental conditions. More
importantly, InstructCell provides an accessible and intuitive tool for
exploring complex single-cell data, lowering technical barriers and enabling
deeper biological insights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages; 13 figures; Code: https://github.com/zjunlp/Instructcell;
  Models: https://huggingface.co/zjunlp/Instructcell-chat,
  https://huggingface.co/zjunlp/InstructCell-instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Zeng, Jie Song, Han Zheng, Hao Jiang, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved cutting-edge performance in image generation.
However, their lengthy denoising process and computationally intensive score
estimation network impede their scalability in low-latency and
resource-constrained scenarios. Post-training quantization (PTQ) compresses and
accelerates diffusion models without retraining, but it inevitably introduces
additional quantization noise, resulting in mean and variance deviations. In
this work, we propose D2-DPM, a dual denoising mechanism aimed at precisely
mitigating the adverse effects of quantization noise on the noise estimation
network. Specifically, we first unravel the impact of quantization noise on the
sampling equation into two components: the mean deviation and the variance
deviation. The mean deviation alters the drift coefficient of the sampling
equation, influencing the trajectory trend, while the variance deviation
magnifies the diffusion coefficient, impacting the convergence of the sampling
trajectory. The proposed D2-DPM is thus devised to denoise the quantization
noise at each time step, and then denoise the noisy sample through the inverse
diffusion iterations. Experimental results demonstrate that D2-DPM achieves
superior generation quality, yielding a 1.42 lower FID than the full-precision
model while achieving 3.99x compression and 11.67x bit-operation acceleration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, acceptted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revolutionizing Communication with Deep Learning and XAI for Enhanced
  Arabic Sign Language Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mazen Balat, Rewaa Awaad, Ahmed B. Zaky, Salah A. Aly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces an integrated approach to recognizing Arabic Sign
Language (ArSL) using state-of-the-art deep learning models such as
MobileNetV3, ResNet50, and EfficientNet-B2. These models are further enhanced
by explainable AI (XAI) techniques to boost interpretability. The ArSL2018 and
RGB Arabic Alphabets Sign Language (AASL) datasets are employed, with
EfficientNet-B2 achieving peak accuracies of 99.48\% and 98.99\%, respectively.
Key innovations include sophisticated data augmentation methods to mitigate
class imbalance, implementation of stratified 5-fold cross-validation for
better generalization, and the use of Grad-CAM for clear model decision
transparency. The proposed system not only sets new benchmarks in recognition
accuracy but also emphasizes interpretability, making it suitable for
applications in healthcare, education, and inclusive communication
technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 25 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference-Time-Compute: More Faithful? A Research Note 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Chua, Owain Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models trained specifically to generate long Chains of Thought (CoTs) have
recently achieved impressive results. We refer to these models as
Inference-Time-Compute (ITC) models. Are the CoTs of ITC models more faithful
compared to traditional non-ITC models? We evaluate two ITC models (based on
Qwen-2.5 and Gemini-2) on an existing test of faithful CoT To measure
faithfulness, we test if models articulate cues in their prompt that influence
their answers to MMLU questions. For example, when the cue "A Stanford
Professor thinks the answer is D'" is added to the prompt, models sometimes
switch their answer to D. In such cases, the Gemini ITC model articulates the
cue 54% of the time, compared to 14% for the non-ITC Gemini.
  We evaluate 7 types of cue, such as misleading few-shot examples and
anchoring on past responses. ITC models articulate cues that influence them
much more reliably than all the 6 non-ITC models tested, such as
Claude-3.5-Sonnet and GPT-4o, which often articulate close to 0% of the time.
  However, our study has important limitations. We evaluate only two ITC models
-- we cannot evaluate OpenAI's SOTA o1 model. We also lack details about the
training of these ITC models, making it hard to attribute our findings to
specific processes.
  We think faithfulness of CoT is an important property for AI Safety. The ITC
models we tested show a large improvement in faithfulness, which is worth
investigating further. To speed up this investigation, we release these early
results as a research note.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FairTTTS: A Tree Test Time Simulation Method for Fairness-Aware
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurit Cohen-Inger, Lior Rokach, Bracha Shapira, Seffi Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithmic decision-making has become deeply ingrained in many domains, yet
biases in machine learning models can still produce discriminatory outcomes,
often harming unprivileged groups. Achieving fair classification is inherently
challenging, requiring a careful balance between predictive performance and
ethical considerations. We present FairTTTS, a novel post-processing bias
mitigation method inspired by the Tree Test Time Simulation (TTTS) method.
Originally developed to enhance accuracy and robustness against adversarial
inputs through probabilistic decision-path adjustments, TTTS serves as the
foundation for FairTTTS. By building on this accuracy-enhancing technique,
FairTTTS mitigates bias and improves predictive performance. FairTTTS uses a
distance-based heuristic to adjust decisions at protected attribute nodes,
ensuring fairness for unprivileged samples. This fairness-oriented adjustment
occurs as a post-processing step, allowing FairTTTS to be applied to
pre-trained models, diverse datasets, and various fairness metrics without
retraining. Extensive evaluation on seven benchmark datasets shows that
FairTTTS outperforms traditional methods in fairness improvement, achieving a
20.96% average increase over the baseline compared to 18.78% for related work,
and further enhances accuracy by 0.55%. In contrast, competing methods
typically reduce accuracy by 0.42%. These results confirm that FairTTTS
effectively promotes more equitable decision-making while simultaneously
improving predictive performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple-Input Variational Auto-Encoder for Anomaly Detection in
  Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phai Vu Dinh, Diep N. Nguyen, Dinh Thai Hoang, Quang Uy Nguyen, Eryk Dutkiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection (AD) plays a pivotal role in AI applications, e.g., in
classification, and intrusion/threat detection in cybersecurity. However, most
existing methods face challenges of heterogeneity amongst feature subsets posed
by non-independent and identically distributed (non-IID) data. We propose a
novel neural network model called Multiple-Input Auto-Encoder for AD (MIAEAD)
to address this. MIAEAD assigns an anomaly score to each feature subset of a
data sample to indicate its likelihood of being an anomaly. This is done by
using the reconstruction error of its sub-encoder as the anomaly score. All
sub-encoders are then simultaneously trained using unsupervised learning to
determine the anomaly scores of feature subsets. The final AUC of MIAEAD is
calculated for each sub-dataset, and the maximum AUC obtained among the
sub-datasets is selected. To leverage the modelling of the distribution of
normal data to identify anomalies of the generative models, we develop a novel
neural network architecture/model called Multiple-Input Variational
Auto-Encoder (MIVAE). MIVAE can process feature subsets through its
sub-encoders before learning distribution of normal data in the latent space.
This allows MIVAE to identify anomalies that deviate from the learned
distribution. We theoretically prove that the difference in the average anomaly
score between normal samples and anomalies obtained by the proposed MIVAE is
greater than that of the Variational Auto-Encoder (VAEAD), resulting in a
higher AUC for MIVAE. Extensive experiments on eight real-world anomaly
datasets demonstrate the superior performance of MIAEAD and MIVAE over
conventional methods and the state-of-the-art unsupervised models, by up to 6%
in terms of AUC score. Alternatively, MIAEAD and MIVAE have a high AUC when
applied to feature subsets with low heterogeneity based on the coefficient of
variation (CV) score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bootstrapping Corner Cases: High-Resolution Inpainting for Safety
  Critical Detect and Avoid for Automated Flying 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Lyhs, Lars Hinneburg, Michael Fischer, Florian Ölsner, Stefan Milz, Jeremy Tschirner, Patrick Mäder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern machine learning techniques have shown tremendous potential,
especially for object detection on camera images. For this reason, they are
also used to enable safety-critical automated processes such as autonomous
drone flights. We present a study on object detection for Detect and Avoid, a
safety critical function for drones that detects air traffic during automated
flights for safety reasons. An ill-posed problem is the generation of good and
especially large data sets, since detection itself is the corner case. Most
models suffer from limited ground truth in raw data, \eg recorded air traffic
or frontal flight with a small aircraft. It often leads to poor and critical
detection rates. We overcome this problem by using inpainting methods to
bootstrap the dataset such that it explicitly contains the corner cases of the
raw data. We provide an overview of inpainting methods and generative models
and present an example pipeline given a small annotated dataset. We validate
our method by generating a high-resolution dataset, which we make publicly
available and present it to an independent object detector that was fully
trained on real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EEG-ReMinD: Enhancing Neurodegenerative EEG Decoding through
  Self-Supervised State Reconstruction-Primed Riemannian Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Wang, Zhenxi Song, Yi Guo, Yuxin Liu, Guoyang Xu, Min Zhang, Zhiguo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of EEG decoding algorithms confronts challenges such as data
sparsity, subject variability, and the need for precise annotations, all of
which are vital for advancing brain-computer interfaces and enhancing the
diagnosis of diseases. To address these issues, we propose a novel two-stage
approach named Self-Supervised State Reconstruction-Primed Riemannian Dynamics
(EEG-ReMinD) , which mitigates reliance on supervised learning and integrates
inherent geometric features. This approach efficiently handles EEG data
corruptions and reduces the dependency on labels. EEG-ReMinD utilizes
self-supervised and geometric learning techniques, along with an attention
mechanism, to analyze the temporal dynamics of EEG features within the
framework of Riemannian geometry, referred to as Riemannian dynamics.
Comparative analyses on both intact and corrupted datasets from two different
neurodegenerative disorders underscore the enhanced performance of EEG-ReMinD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Wall-Pressure Spectrum Model for Aeroacoustic Predictions
  Based on Symbolic Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Botero Bolívar, David Huergo, Fernanda L. dos Santos, Cornelis H. Venner, Leandro D. de Santana, Esteban Ferrer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast-turn around methods to predict airfoil trailing-edge noise are crucial
for incorporating noise limitations into design optimization loops of several
applications. Among these aeroacoustic predictive models, Amiet's theory offers
the best balance between accuracy and simplicity. The accuracy of the model
relies heavily on precise wall-pressure spectrum predictions, which are often
based on single-equation formulations with adjustable parameters. These
parameters are calibrated for particular airfoils and flow conditions and
consequently tend to fail when applied outside their calibration range. This
paper introduces a new wall-pressure spectrum empirical model designed to
enhance the robustness and accuracy of current state-of-the-art predictions
while widening the range of applicability of the model to different airfoils
and flow conditions. The model is developed using AI-based symbolic regression
via a genetic-algorithm-based approach, and applied to a dataset of
wall-pressure fluctuations measured on NACA 0008 and NACA 63018 airfoils at
multiple angles of attack and inflow velocities, covering turbulent boundary
layers with both adverse and favorable pressure gradients. Validation against
experimental data (outside the training dataset) demonstrates the robustness of
the model compared to well-accepted semi-empirical models. Finally, the model
is integrated with Amiet's theory to predict the aeroacoustic noise of a
full-scale wind turbine, showing good agreement with experimental measurements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoHan: Robust Hand Detection in Operation Room 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roi Papo, Sapir Gershov, Tom Friedman, Itay Or, Gil Bolotin, Shlomi Laufer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand-specific localization has garnered significant interest within the
computer vision community. Although there are numerous datasets with hand
annotations from various angles and settings, domain transfer techniques
frequently struggle in surgical environments. This is mainly due to the limited
availability of gloved hand instances and the unique challenges of operating
rooms (ORs). Thus, hand-detection models tailored to OR settings require
extensive training and expensive annotation processes. To overcome these
challenges, we present "RoHan" - a novel approach for robust hand detection in
the OR, leveraging advanced semi-supervised domain adaptation techniques to
tackle the challenges of varying recording conditions, diverse glove colors,
and occlusions common in surgical settings. Our methodology encompasses two
main stages: (1) data augmentation strategy that utilizes "Artificial Gloves,"
a method for augmenting publicly available hand datasets with synthetic images
of hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that
improves detection performance in real-world OR settings through iterative
prediction refinement and efficient frame filtering. We evaluate our method
using two datasets: simulated enterotomy repair and saphenous vein graft
harvesting. "RoHan" substantially reduces the need for extensive labeling and
model training, paving the way for the practical implementation of hand
detection technologies in medical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-driven inventory management for new products: A warm-start and
  adjusted Dyna-$Q$ approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Qu, Longxiao Liu, Wenjie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel reinforcement learning algorithm for
inventory management of newly launched products with no or limited historical
demand information. The algorithm follows the classic Dyna-$Q$ structure,
balancing the model-based and model-free approaches, while accelerating the
training process of Dyna-$Q$ and mitigating the model discrepancy generated by
the model-based feedback. Warm-start information from the demand data of
existing similar products can be incorporated into the algorithm to further
stabilize the early-stage training and reduce the variance of the estimated
optimal policy. Our approach is validated through a case study of bakery
inventory management with real data. The adjusted Dyna-$Q$ shows up to a 23.7\%
reduction in average daily cost compared with $Q$-learning, and up to a 77.5\%
reduction in training time within the same horizon compared with classic
Dyna-$Q$. By incorporating the warm-start information, it can be found that the
adjusted Dyna-$Q$ has the lowest total cost, lowest variance in total cost, and
relatively low shortage percentages among all the algorithms under a 30-day
testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Smooth Handovers via Smoothed Online Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michail Kalntis, Andra Lutu, Jesús Omaña Iglesias, Fernando A. Kuipers, George Iosifidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With users demanding seamless connectivity, handovers (HOs) have become a
fundamental element of cellular networks. However, optimizing HOs is a
challenging problem, further exacerbated by the growing complexity of mobile
networks. This paper presents the first countrywide study of HO optimization,
through the prism of Smoothed Online Learning (SOL). We first analyze an
extensive dataset from a commercial mobile network operator (MNO) in Europe
with more than 40M users, to understand and reveal important features and
performance impacts on HOs. Our findings highlight a correlation between HO
failures/delays, and the characteristics of radio cells and end-user devices,
showcasing the impact of heterogeneity in mobile networks nowadays. We
subsequently model UE-cell associations as dynamic decisions and propose a
realistic system model for smooth and accurate HOs that extends existing
approaches by (i) incorporating device and cell features on HO optimization,
and (ii) eliminating (prior) strong assumptions about requiring future signal
measurements and knowledge of end-user mobility. Our algorithm, aligned with
the O-RAN paradigm, provides robust dynamic regret guarantees, even in
challenging environments, and shows superior performance in multiple scenarios
with real-world and synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Action Based Reinforcement Learning for Multi-Objective
  Compatible Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guizhe Jin, Zhuoren Li, Bo Leng, Wei Han, Lu Xiong, Chen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has shown excellent performance in solving
decision-making and control problems of autonomous driving, which is
increasingly applied in diverse driving scenarios. However, driving is a
multi-attribute problem, leading to challenges in achieving multi-objective
compatibility for current RL methods, especially in both policy execution and
policy iteration. On the one hand, the common action space structure with
single action type limits driving flexibility or results in large behavior
fluctuations during policy execution. On the other hand, the multi-attribute
weighted single reward function result in the agent's disproportionate
attention to certain objectives during policy iterations. To this end, we
propose a Multi-objective Ensemble-Critic reinforcement learning method with
Hybrid Parametrized Action for multi-objective compatible autonomous driving.
Specifically, a parameterized action space is constructed to generate hybrid
driving actions, combining both abstract guidance and concrete control
commands. A multi-objective critics architecture is constructed considering
multiple attribute rewards, to ensure simultaneously focusing on different
driving objectives. Additionally, uncertainty-based exploration strategy is
introduced to help the agent faster approach viable driving policy. The
experimental results in both the simulated traffic environment and the HighD
dataset demonstrate that our method can achieve multi-objective compatible
autonomous driving in terms of driving efficiency, action consistency, and
safety. It enhances the general performance of the driving while significantly
increasing training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Multimodal Sentiment Analysis: Leveraging Cross-Modal Attention
  for Enabled Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Lee, Singh Suniljit, Yong Siang Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the development of a multimodal sentiment analysis model
that integrates text, audio, and visual data to enhance sentiment
classification. The goal is to improve emotion detection by capturing the
complex interactions between these modalities, thereby enabling more accurate
and nuanced sentiment interpretation. The study evaluates three feature fusion
strategies -- late stage fusion, early stage fusion, and multi-headed attention
-- within a transformer-based architecture. Experiments were conducted using
the CMU-MOSEI dataset, which includes synchronized text, audio, and visual
inputs labeled with sentiment scores. Results show that early stage fusion
significantly outperforms late stage fusion, achieving an accuracy of 71.87\%,
while the multi-headed attention approach offers marginal improvement, reaching
72.39\%. The findings suggest that integrating modalities early in the process
enhances sentiment classification, while attention mechanisms may have limited
impact within the current framework. Future work will focus on refining feature
fusion techniques, incorporating temporal data, and exploring dynamic feature
weighting to further improve model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CuAsmRL: Optimizing GPU SASS Schedules via Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoliang He, Eiko Yoneki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are remarked by their substantial computational
requirements. To mitigate the cost, researchers develop specialized CUDA
kernels, which often fuse several tensor operations to maximize the utilization
of GPUs as much as possible. However, those specialized kernels may still leave
performance on the table as CUDA assembly experts show that manual optimization
of GPU SASS schedules can lead to better performance, and trial-and-error is
largely employed to manually find the best GPU SASS schedules.
  In this work, we employ an automatic approach to optimize GPU SASS schedules,
which thus can be integrated into existing compiler frameworks. The key to
automatic optimization is training an RL agent to mimic how human experts
perform manual scheduling. To this end, we formulate an assembly game, where RL
agents can play to find the best GPU SASS schedules. The assembly game starts
from a \textit{-O3} optimized SASS schedule, and the RL agents can iteratively
apply actions to mutate the current schedules. Positive rewards are generated
if the mutated schedules get higher throughput by executing on GPUs.
Experiments show that CuAsmRL can further improve the performance of existing
specialized CUDA kernels transparently by up to $26\%$, and on average $9\%$.
Moreover, it is used as a tool to reveal potential optimization moves learned
automatically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>cgo 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Policy Adaptation under Covariate Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueqing Liu, Qinwei Yang, Zhaoqing Tian, Ruocheng Guo, Peng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning of prediction models has been extensively studied, while
the corresponding policy learning approaches are rarely discussed. In this
paper, we propose principled approaches for learning the optimal policy in the
target domain by leveraging two datasets: one with full information from the
source domain and the other from the target domain with only covariates. First,
under the setting of covariate shift, we formulate the problem from a
perspective of causality and present the identifiability assumptions for the
reward induced by a given policy. Then, we derive the efficient influence
function and the semiparametric efficiency bound for the reward. Based on this,
we construct a doubly robust and semiparametric efficient estimator for the
reward and then learn the optimal policy by optimizing the estimated reward.
Moreover, we theoretically analyze the bias and the generalization error bound
for the learned policy. Furthermore, in the presence of both covariate and
concept shifts, we propose a novel sensitivity analysis method to evaluate the
robustness of the proposed policy learning approach. Extensive experiments
demonstrate that the approach not only estimates the reward more accurately but
also yields a policy that closely approximates the theoretically optimal
policy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the use of Statistical Learning Theory for model selection in
  Structural Health Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        C. A. Lindley, N. Dervilis, K. Worden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whenever data-based systems are employed in engineering applications,
defining an optimal statistical representation is subject to the problem of
model selection. This paper focusses on how well models can generalise in
Structural Health Monitoring (SHM). Although statistical model validation in
this field is often performed heuristically, it is possible to estimate
generalisation more rigorously using the bounds provided by Statistical
Learning Theory (SLT). Therefore, this paper explores the selection process of
a kernel smoother for modelling the impulse response of a linear oscillator
from the perspective of SLT. It is demonstrated that incorporating domain
knowledge into the regression problem yields a lower guaranteed risk, thereby
enhancing generalisation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Attentive Spatio-Temporal Calibration for Precise Intermediate
  Layer Matching in ANN-to-SNN Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Hong, Yueming Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) are promising for low-power computation due to
their event-driven mechanism but often suffer from lower accuracy compared to
Artificial Neural Networks (ANNs). ANN-to-SNN knowledge distillation can
improve SNN performance, but previous methods either focus solely on label
information, missing valuable intermediate layer features, or use a layer-wise
approach that neglects spatial and temporal semantic inconsistencies, leading
to performance degradation.To address these limitations, we propose a novel
method called self-attentive spatio-temporal calibration (SASTC). SASTC uses
self-attention to identify semantically aligned layer pairs between ANN and
SNN, both spatially and temporally. This enables the autonomous transfer of
relevant semantic information. Extensive experiments show that SASTC
outperforms existing methods, effectively solving the mismatching problem.
Superior accuracy results include 95.12% on CIFAR-10, 79.40% on CIFAR-100 with
2 time steps, and 68.69% on ImageNet with 4 time steps for static datasets, and
97.92% on DVS-Gesture and 83.60% on DVS-CIFAR10 for neuromorphic datasets. This
marks the first time SNNs have outperformed ANNs on both CIFAR-10 and
CIFAR-100, shedding the new light on the potential applications of SNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gen-A: Generalizing Ambisonics Neural Encoding to Unseen Microphone
  Arrays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikko Heikkinen, Archontis Politis, Konstantinos Drossos, Tuomas Virtanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using deep neural networks (DNNs) for encoding of microphone array (MA)
signals to the Ambisonics spatial audio format can surpass certain limitations
of established conventional methods, but existing DNN-based methods need to be
trained separately for each MA. This paper proposes a DNN-based method for
Ambisonics encoding that can generalize to arbitrary MA geometries unseen
during training. The method takes as inputs the MA geometry and MA signals and
uses a multi-level encoder consisting of separate paths for geometry and signal
data, where geometry features inform the signal encoder at each level. The
method is validated in simulated anechoic and reverberant conditions with one
and two sources. The results indicate improvement over conventional encoding
across the whole frequency range for dry scenes, while for reverberant scenes
the improvement is frequency-dependent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Proceedings of the 2025 IEEE
  International Conference on Acoustics, Speech and Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UFGraphFR: An attempt at a federated recommendation <span class="highlight-title">system</span> based on user
  text characteristics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has become an important research area in 'private
computing' due to the 'useable invisibility' of data during training. Inspired
by Federated learning, the federated recommendation system has gradually become
a new recommendation service architecture that can protect users' privacy. The
use of user diagrams to enhance federated recommendations is a promising topic.
How to use user diagrams to enhance federated recommendations is a promising
research topic. However, it's a great challenge to construct a user diagram
without compromising privacy in a federated learning scenario. Inspired by the
simple idea that similar users often have the same attribute characteristics,
we propose a personalized federated recommendation algorithm based on the user
relationship graph constructed by the user text characteristics(Graph
Federation Recommendation System based on User Text description Features,
UFGraphFR). The method uses the embedding layer weight of the user's text
feature description to construct the user relationship graph. It introduces the
Transformer mechanism to capture the sequence modeling of the user's historical
interaction sequence. Without access to user history interactions and specific
user attributes, the federal learning privacy protection of data 'useable
invisibility' is embodied. Preliminary experiments on some benchmark datasets
demonstrate the superior performance of UFGraphFR. Our experiments show that
this model can protect user privacy to some extent without affecting the
performance of the recommendation system. The code will be easily available on
https://github.com/trueWangSyutung/UFGraphFR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware
  Structured Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Andronic, Jiawen Li, George A. Constantinides
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard deep neural network inference involves the computation of
interleaved linear maps and nonlinear activation functions. Prior work for
ultra-low latency implementations has hardcoded these operations inside FPGA
lookup tables (LUTs). However, FPGA LUTs can implement a much greater variety
of functions. In this paper, we propose a novel approach to training DNNs for
FPGA deployment using multivariate polynomials as the basic building block. Our
method takes advantage of the flexibility offered by the soft logic, hiding the
polynomial evaluation inside the LUTs with minimal overhead. By using
polynomial building blocks, we achieve the same accuracy using considerably
fewer layers of soft logic than by using linear functions, leading to
significant latency and area improvements. LUT-based implementations also face
a significant challenge: the LUT size grows exponentially with the number of
inputs. Prior work relies on a priori fixed sparsity, with results heavily
dependent on seed selection. To address this, we propose a structured pruning
strategy using a bespoke hardware-aware group regularizer that encourages a
particular sparsity pattern that leads to a small number of inputs per neuron.
We demonstrate the effectiveness of PolyLUT on three tasks: network intrusion
detection, jet identification at the CERN Large Hadron Collider, and MNIST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2309.02334</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence Analysis of Real-time Recurrent Learning (RTRL) for a class
  of Recurrent Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Chun-Hei Lam, Justin Sirignano, Konstantinos Spiliopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent neural networks (RNNs) are commonly trained with the truncated
backpropagation-through-time (TBPTT) algorithm. For the purposes of
computational tractability, the TBPTT algorithm truncates the chain rule and
calculates the gradient on a finite block of the overall data sequence. Such
approximation could lead to significant inaccuracies, as the block length for
the truncated backpropagation is typically limited to be much smaller than the
overall sequence length. In contrast, Real-time recurrent learning (RTRL) is an
online optimization algorithm which asymptotically follows the true gradient of
the loss on the data sequence as the number of sequence time steps $t
\rightarrow \infty$. RTRL forward propagates the derivatives of the RNN
hidden/memory units with respect to the parameters and, using the forward
derivatives, performs online updates of the parameters at each time step in the
data sequence. RTRL's online forward propagation allows for exact optimization
over extremely long data sequences, although it can be computationally costly
for models with large numbers of parameters. We prove convergence of the RTRL
algorithm for a class of RNNs. The convergence analysis establishes a fixed
point for the joint distribution of the data sequence, RNN hidden layer, and
the RNN hidden layer forward derivatives as the number of data samples from the
sequence and the number of training steps tend to infinity. We prove
convergence of the RTRL algorithm to a stationary point of the loss. Numerical
studies illustrate our theoretical results. One potential application area for
RTRL is the analysis of financial data, which typically involve long time
series and models with small to medium numbers of parameters. This makes RTRL
computationally tractable and a potentially appealing optimization method for
training models. Thus, we include an example of RTRL applied to limit order
book data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced SPS Velocity-adaptive Scheme: Access Fariness in 5G NR V2I
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Qiong Wu, Pingyi Fan, Kezhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle-to-Infrastructure (V2I) technology enables information exchange
between vehicles and road infrastructure. Specifically, when a vehicle
approaches a roadside unit (RSU), it can exchange information with the RSU to
obtain accurate data that assists in driving. With the release of the 3rd
Generation Partnership Project (3GPP) Release 16, which includes the 5G New
Radio (NR) Vehicle-to-Everything (V2X) standards, vehicles typically adopt
mode-2 communication using sensing-based semi-persistent scheduling (SPS) for
resource allocation. In this approach, vehicles identify candidate resources
within a selection window and exclude ineligible resources based on information
from a sensing window. However, vehicles often drive at different speeds,
resulting in varying amounts of data transmission with RSUs as they pass by,
which leads to unfair access. Therefore, it is essential to design an access
scheme that accounts for different vehicle speeds to achieve fair access across
the network. This paper formulates an optimization problem for vehicular
networks and proposes a multi-objective optimization scheme to address it by
adjusting the selection window in the SPS mechanism of 5G NR V2I mode-2.
Simulation results demonstrate the effectiveness of the proposed scheme
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to IEEE Journal. The source code has
  been released at:
  https://github.com/qiongwu86/Enhanced-SPS-Velocity-adaptiveScheme-Access-Fariness-in-5G-NR-V2I-Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An AI-driven framework for rapid and localized optimizations of urban
  open spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pegah Eshraghi, Arman Nikkhah Dehnavi, Maedeh Mirdamadi, Riccardo Talami, Zahra-Sadat Zomorodian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As urbanization accelerates, open spaces are increasingly recognized for
their role in enhancing sustainability and well-being, yet they remain
underexplored compared to built spaces. This study introduces an AI-driven
framework that integrates machine learning models (MLMs) and explainable AI
techniques to optimize Sky View Factor (SVF) and visibility, key spatial
metrics influencing thermal comfort and perceived safety in urban spaces.
Unlike global optimization methods, which are computationally intensive and
impractical for localized adjustments, this framework supports incremental
design improvements with lower computational costs and greater flexibility. The
framework employs SHapley Adaptive Explanations (SHAP) to analyze feature
importance and Counterfactual Explanations (CFXs) to propose minimal design
changes. Simulations tested five MLMs, identifying XGBoost as the most
accurate, with building width, park area, and heights of surrounding buildings
as critical for SVF, and distances from southern buildings as key for
visibility. Compared to Genetic Algorithms, which required approximately 15/30
minutes across 3/4 generations to converge, the tested CFX approach achieved
optimized results in 1 minute with a 5% RMSE error, demonstrating significantly
faster performance and suitability for scalable retrofitting strategies. This
interpretable and computationally efficient framework advances urban
performance optimization, providing data-driven insights and practical
retrofitting solutions for enhancing usability and environmental quality across
diverse urban contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximizing Uncertainty for Federated learning via Bayesian
  Optimisation-based Model Poisoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marios Aristodemou, Xiaolan Liu, Yuan Wang, Konstantinos G. Kyriakopoulos, Sangarapillai Lambotharan, Qingsong Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we transition from Narrow Artificial Intelligence towards Artificial Super
Intelligence, users are increasingly concerned about their privacy and the
trustworthiness of machine learning (ML) technology. A common denominator for
the metrics of trustworthiness is the quantification of uncertainty inherent in
DL algorithms, and specifically in the model parameters, input data, and model
predictions. One of the common approaches to address privacy-related issues in
DL is to adopt distributed learning such as federated learning (FL), where
private raw data is not shared among users. Despite the privacy-preserving
mechanisms in FL, it still faces challenges in trustworthiness. Specifically,
the malicious users, during training, can systematically create malicious model
parameters to compromise the models predictive and generative capabilities,
resulting in high uncertainty about their reliability. To demonstrate malicious
behaviour, we propose a novel model poisoning attack method named Delphi which
aims to maximise the uncertainty of the global model output. We achieve this by
taking advantage of the relationship between the uncertainty and the model
parameters of the first hidden layer of the local model. Delphi employs two
types of optimisation , Bayesian Optimisation and Least Squares Trust Region,
to search for the optimal poisoned model parameters, named as Delphi-BO and
Delphi-LSTR. We quantify the uncertainty using the KL Divergence to minimise
the distance of the predictive probability distribution towards an uncertain
distribution of model output. Furthermore, we establish a mathematical proof
for the attack effectiveness demonstrated in FL. Numerical results demonstrate
that Delphi-BO induces a higher amount of uncertainty than Delphi-LSTR
highlighting vulnerability of FL systems to model poisoning attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Feature Construction for Anomaly Detection in Time Series
  -- An Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marine Hamon, Vincent Lemaire, Nour Eddine Yassine Nair-Benrekia, Samuel Berlemont, Julien Cumin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To detect anomalies with precision and without prior knowledge in time
series, is it better to build a detector from the initial temporal
representation, or to compute a new (tabular) representation using an existing
automatic variable construction library? In this article, we address this
question by conducting an in-depth experimental study for two popular detectors
(Isolation Forest and Local Outlier Factor). The obtained results, for 5
different datasets, show that the new representation, computed using the
tsfresh library, allows Isolation Forest to significantly improve its
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reward Compatibility: A Framework for Inverse RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Lazzati, Mirco Mutti, Alberto Metelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide an original theoretical study of Inverse Reinforcement Learning
(IRL) through the lens of reward compatibility, a novel framework to quantify
the compatibility of a reward with the given expert's demonstrations.
Intuitively, a reward is more compatible with the demonstrations the closer the
performance of the expert's policy computed with that reward is to the optimal
performance for that reward. This generalizes the notion of feasible reward
set, the most common framework in the theoretical IRL literature, for which a
reward is either compatible or not compatible. The grayscale introduced by the
reward compatibility is the key to extend the realm of provably efficient IRL
far beyond what is attainable with the feasible reward set: from tabular to
large-scale MDPs. We analyze the IRL problem across various settings, including
optimal and suboptimal expert's demonstrations and both online and offline data
collection. For all of these dimensions, we provide a tractable algorithm and
corresponding sample complexity analysis, as well as various insights on reward
compatibility and how the framework can pave the way to yet more general
problem settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining imaging and shape features for prediction tasks of Alzheimer's
  disease classification and brain age regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nairouz Shehata, Carolina Piçarra, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate combining imaging and shape features extracted from MRI for
the clinically relevant tasks of brain age prediction and Alzheimer's disease
classification. Our proposed model fuses ResNet-extracted image embeddings with
shape embeddings from a bespoke graph neural network. The shape embeddings are
derived from surface meshes of 15 brain structures, capturing detailed
geometric information. Combined with the appearance features from T1-weighted
images, we observe improvements in the prediction performance on both tasks,
with substantial gains for classification. We evaluate the model using public
datasets, including CamCAN, IXI, and OASIS3, demonstrating the effectiveness of
fusing imaging and shape features for brain analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHEQ-ing the Box: Safe Variable Impedance Learning for Robotic Polishing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma Cramer, Lukas Jäschke, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic systems are increasingly employed for industrial automation, with
contact-rich tasks like polishing requiring dexterity and compliant behaviour.
These tasks are difficult to model, making classical control challenging. Deep
reinforcement learning (RL) offers a promising solution by enabling the
learning of models and control policies directly from data. However, its
application to real-world problems is limited by data inefficiency and unsafe
exploration. Adaptive hybrid RL methods blend classical control and RL
adaptively, combining the strengths of both: structure from control and
learning from RL. This has led to improvements in data efficiency and
exploration safety. However, their potential for hardware applications remains
underexplored, with no evaluations on physical systems to date. Such
evaluations are critical to fully assess the practicality and effectiveness of
these methods in real-world settings. This work presents an experimental
demonstration of the hybrid RL algorithm CHEQ for robotic polishing with
variable impedance, a task requiring precise force and velocity tracking. In
simulation, we show that variable impedance enhances polishing performance. We
compare standalone RL with adaptive hybrid RL, demonstrating that CHEQ achieves
effective learning while adhering to safety constraints. On hardware, CHEQ
achieves effective polishing behaviour, requiring only eight hours of training
and incurring just five failures. These results highlight the potential of
adaptive hybrid RL for real-world, contact-rich tasks trained directly on
hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Derivation of Output Correlation Inferences for Multi-Output (aka
  Multi-Task) Gaussian Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhei Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian process (GP) is arguably one of the most widely used machine
learning algorithms in practice. One of its prominent applications is Bayesian
optimization (BO). Although the vanilla GP itself is already a powerful tool
for BO, it is often beneficial to be able to consider the dependencies of
multiple outputs. To do so, Multi-task GP (MTGP) is formulated, but it is not
trivial to fully understand the derivations of its formulations and their
gradients from the previous literature. This paper serves friendly derivations
of the MTGP formulations and their gradients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Guide Dog: Egocentric Path Prediction on Smartphone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Jadhav, Jeffery Cao, Abhishree Shetty, Urvashi Priyam Kumar, Aditi Sharma, Ben Sukboontip, Jayant Sravan Tamarapalli, Jingyi Zhang, Anirudh Koul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces AI Guide Dog (AIGD), a lightweight egocentric
navigation assistance system for visually impaired individuals, designed for
real-time deployment on smartphones. AIGD addresses key challenges in blind
navigation by employing a vision-only, multi-label classification approach to
predict directional commands, ensuring safe traversal across diverse
environments. We propose a novel technique to enable goal-based outdoor
navigation by integrating GPS signals and high-level directions, while also
addressing uncertain multi-path predictions for destination-free indoor
navigation. Our generalized model is the first navigation assistance system to
handle both goal-oriented and exploratory navigation scenarios across indoor
and outdoor settings, establishing a new state-of-the-art in blind navigation.
We present methods, datasets, evaluations, and deployment insights to encourage
further innovations in assistive navigation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gandalf the Red: Adaptive Security for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Pfister, Václav Volhejn, Manuel Knott, Santiago Arias, Julia Bazińska, Mykhailo Bichurin, Alan Commike, Janet Darling, Peter Dienes, Matthew Fiedler, David Haber, Matthias Kraft, Marco Lancini, Max Mathys, Damián Pascual-Ortiz, Jakub Podolak, Adrià Romero-López, Kyriacos Shiarlis, Andreas Signer, Zsolt Terek, Athanasios Theocharis, Daniel Timbrell, Samuel Trautwein, Samuel Watts, Natalie Wu, Mateo Rojas-Carulla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluations of defenses against prompt attacks in large language
model (LLM) applications often overlook two critical factors: the dynamic
nature of adversarial behavior and the usability penalties imposed on
legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security
Utility Threat Model), which explicitly separates attackers from legitimate
users, models multi-step interactions, and rigorously expresses the
security-utility in an optimizable form. We further address the shortcomings in
existing evaluations by introducing Gandalf, a crowd-sourced, gamified
red-teaming platform designed to generate realistic, adaptive attack datasets.
Using Gandalf, we collect and release a dataset of 279k prompt attacks.
Complemented by benign user data, our analysis reveals the interplay between
security and utility, showing that defenses integrated in the LLM (e.g., system
prompts) can degrade usability even without blocking requests. We demonstrate
that restricted application domains, defense-in-depth, and adaptive defenses
are effective strategies for building secure and useful LLM applications. Code
is available at
\href{https://github.com/lakeraai/dsec-gandalf}{\texttt{https://github.com/lakeraai/dsec-gandalf}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Niklas Pfister, V\'aclav Volhejn and Manuel Knott contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phase of Flight Classification in Aviation Safety using LSTM, GRU, and
  BiLSTM: A Case Study with ASN <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aziida Nanyonga, Hassan Wasswa, Graham Wild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety is the main concern in the aviation industry, where even minor
operational issues can lead to serious consequences. This study addresses the
need for comprehensive aviation accident analysis by leveraging natural
language processing (NLP) and advanced AI models to classify the phase of
flight from unstructured aviation accident analysis narratives. The research
aims to determine whether the phase of flight can be inferred from narratives
of post-accident events using NLP techniques. The classification performance of
various deep learning models was evaluated. For single RNN-based models, LSTM
achieved an accuracy of 63%, precision 60%, and recall 61%. BiLSTM recorded an
accuracy of 64%, precision 63%, and a recall of 64%. GRU exhibited balanced
performance with an accuracy and recall of 60% and a precision of 63%. Joint
RNN-based models further enhanced predictive capabilities. GRU-LSTM,
LSTM-BiLSTM, and GRU-BiLSTM demonstrated accuracy rates of 62%, 67%, and 60%,
respectively, showcasing the benefits of combining these architectures. To
provide a comprehensive overview of model performance, single and combined
models were compared in terms of the various metrics. These results underscore
the models' capacity to classify the phase of flight from raw text narratives,
equipping aviation industry stakeholders with valuable insights for proactive
decision-making. Therefore, this research signifies a substantial advancement
in the application of NLP and deep learning models to enhance aviation safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Aviation Safety, Deep learning algorithms, Flight phase, NLP, ASN,
  and Classification</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aviation Safety Enhancement via NLP & Deep Learning: Classifying Flight
  Phases in ATSB Safety Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aziida Nanyonga, Hassan Wasswa, Graham Wild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aviation safety is paramount, demanding precise analysis of safety
occurrences during different flight phases. This study employs Natural Language
Processing (NLP) and Deep Learning models, including LSTM, CNN, Bidirectional
LSTM (BLSTM), and simple Recurrent Neural Networks (sRNN), to classify flight
phases in safety reports from the Australian Transport Safety Bureau (ATSB).
The models exhibited high accuracy, precision, recall, and F1 scores, with LSTM
achieving the highest performance of 87%, 88%, 87%, and 88%, respectively. This
performance highlights their effectiveness in automating safety occurrence
analysis. The integration of NLP and Deep Learning technologies promises
transformative enhancements in aviation safety analysis, enabling targeted
safety measures and streamlined report handling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NLP, Aviation Safety, ATSB, Deep learning, Flight phase. arXiv admin
  note: substantial text overlap with arXiv:2501.01694</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence
  Modeling for Resource-Constrained Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed A. Taha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-range sequence modeling is a crucial aspect of natural language
processing and time series analysis. However, traditional models like Recurrent
Neural Networks (RNNs) and Transformers suffer from computational and memory
inefficiencies, especially when dealing with long sequences. This paper
introduces Logarithmic Memory Networks (LMNs), a novel architecture that
leverages a hierarchical logarithmic tree structure to efficiently store and
retrieve past information. LMNs dynamically summarize historical context,
significantly reducing the memory footprint and computational complexity of
attention mechanisms from O(n2) to O(log(n)). The model employs a
single-vector, targeted attention mechanism to access stored information, and
the memory block construction worker (summarizer) layer operates in two modes:
a parallel execution mode during training for efficient processing of
hierarchical tree structures and a sequential execution mode during inference,
which acts as a memory management system. It also implicitly encodes positional
information, eliminating the need for explicit positional encodings. These
features make LMNs a robust and scalable solution for processing long-range
sequences in resource-constrained environments, offering practical improvements
in efficiency and scalability. The code is publicly available under the MIT
License on GitHub: https://github.com/AhmedBoin/LogarithmicMemory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Classification Trees for Continuous Feature Data Using Dynamic
  Programming with Branch-and-Bound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Catalin E. Brita, Jacobus G. M. van der Linden, Emir Demirović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computing an optimal classification tree that provably maximizes training
performance within a given size limit, is NP-hard, and in practice, most
state-of-the-art methods do not scale beyond computing optimal trees of depth
three. Therefore, most methods rely on a coarse binarization of continuous
features to maintain scalability. We propose a novel algorithm that optimizes
trees directly on the continuous feature data using dynamic programming with
branch-and-bound. We develop new pruning techniques that eliminate many
sub-optimal splits in the search when similar to previously computed splits and
we provide an efficient subroutine for computing optimal depth-two trees. Our
experiments demonstrate that these techniques improve runtime by one or more
orders of magnitude over state-of-the-art optimal methods and improve test
accuracy by 5% over greedy heuristics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In the proceedings of AAAI-25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Label Refinement Matters More than Preference Optimization
  under Weak Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaowen Ye, Cassidy Laidlaw, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LM) post-training relies on two stages of human supervision:
task demonstrations for supervised finetuning (SFT), followed by preference
comparisons for reinforcement learning from human feedback (RLHF). As LMs
become more capable, the tasks they are given become harder to supervise. Will
post-training remain effective under unreliable supervision? To test this, we
simulate unreliable demonstrations and comparison feedback using small LMs and
time-constrained humans. We find that in the presence of unreliable
supervision, SFT still retains some effectiveness, but DPO (a common RLHF
algorithm) fails to improve the model beyond SFT. To address this, we propose
iterative label refinement (ILR) as an alternative to RLHF. ILR improves the
SFT data by using comparison feedback to decide whether human demonstrations
should be replaced by model-generated alternatives, then retrains the model via
SFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with
unreliable supervision (math, coding, and safe instruction-following). Our
findings suggest that as LMs are used for complex tasks where human supervision
is unreliable, RLHF may no longer be the best use of human comparison feedback;
instead, it is better to direct feedback towards improving the training data
rather than continually training the model. Our code and data are available at
https://github.com/helloelwin/iterative-label-refinement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Algorithmic Bias in Multiclass CNN Classifications Using
  Causal Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Sik Byun, Wendy Wan Yee Hui, Wai Kwong Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study describes a procedure for applying causal modeling to detect and
mitigate algorithmic bias in a multiclass classification problem. The dataset
was derived from the FairFace dataset, supplemented with emotional labels
generated by the DeepFace pre-trained model. A custom Convolutional Neural
Network (CNN) was developed, consisting of four convolutional blocks, followed
by fully connected layers and dropout layers to mitigate overfitting. Gender
bias was identified in the CNN model's classifications: Females were more
likely to be classified as "happy" or "sad," while males were more likely to be
classified as "neutral." To address this, the one-vs-all (OvA) technique was
applied. A causal model was constructed for each emotion class to adjust the
CNN model's predicted class probabilities. The adjusted probabilities for the
various classes were then aggregated by selecting the class with the highest
probability. The resulting debiased classifications demonstrated enhanced
gender fairness across all classes, with negligible impact--or even a slight
improvement--on overall accuracy. This study highlights that algorithmic
fairness and accuracy are not necessarily trade-offs. All data and code for
this study are publicly available for download.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages; 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MD-Syn: Synergistic drug combination prediction based on the
  multidimensional feature fusion method and attention mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        XinXin Ge, Yi-Ting Lee, Shan-Ju Yeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug combination therapies have shown promising therapeutic efficacy in
complex diseases and have demonstrated the potential to reduce drug resistance.
However, the huge number of possible drug combinations makes it difficult to
screen them all in traditional experiments. In this study, we proposed MD-Syn,
a computational framework, which is based on the multidimensional feature
fusion method and multi-head attention mechanisms. Given drug pair-cell line
triplets, MD-Syn considers one-dimensional and two-dimensional feature spaces
simultaneously. It consists of a one-dimensional feature embedding module
(1D-FEM), a two-dimensional feature embedding module (2D-FEM), and a deep
neural network-based classifier for synergistic drug combination prediction.
MD-Syn achieved the AUROC of 0.919 in 5-fold cross-validation, outperforming
the state-of-the-art methods. Further, MD-Syn showed comparable results over
two independent datasets. In addition, the multi-head attention mechanisms not
only learn embeddings from different feature aspects but also focus on
essential interactive feature elements, improving the interpretability of
MD-Syn. In summary, MD-Syn is an interpretable framework to prioritize
synergistic drug combination pairs with chemicals and cancer cell line gene
expression profiles. To facilitate broader community access to this model, we
have developed a web portal (https://labyeh104-2.life.nthu.edu.tw/) that
enables customized predictions of drug combination synergy effects based on
user-specified compounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Nonparametric Estimation: from Sparse to Dense Samples per
  Terminal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deheng Yuan, Tao Guo, Zhongyi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider the communication-constrained problem of nonparametric function
estimation, in which each distributed terminal holds multiple i.i.d. samples.
Under certain regularity assumptions, we characterize the minimax optimal rates
for all regimes, and identify phase transitions of the optimal rates as the
samples per terminal vary from sparse to dense. This fully solves the problem
left open by previous works, whose scopes are limited to regimes with either
dense samples or a single sample per terminal. To achieve the optimal rates, we
design a layered estimation protocol by exploiting protocols for the parametric
density estimation problem. We show the optimality of the protocol using
information-theoretic methods and strong data processing inequalities, and
incorporating the classic balls and bins model. The optimal rates are immediate
for various special cases such as density estimation, Gaussian, binary, Poisson
and heteroskedastic regression models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ deepTerra -- AI Land Classification Made Easy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Keith Wilkinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  deepTerra is a comprehensive platform designed to facilitate the
classification of land surface features using machine learning and satellite
imagery. The platform includes modules for data collection, image augmentation,
training, testing, and prediction, streamlining the entire workflow for image
classification tasks. This paper presents a detailed overview of the
capabilities of deepTerra, shows how it has been applied to various research
areas, and discusses the future directions it might take.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State-of-the-Art Transformer Models for Image Super-Resolution:
  Techniques, Challenges, and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debasish Dutta, Deepjyoti Chetia, Neeharika Sonowal, Sanjib Kr Kalita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Super-Resolution (SR) aims to recover a high-resolution image from its
low-resolution counterpart, which has been affected by a specific degradation
process. This is achieved by enhancing detail and visual quality. Recent
advancements in transformer-based methods have remolded image super-resolution
by enabling high-quality reconstructions surpassing previous deep-learning
approaches like CNN and GAN-based. This effectively addresses the limitations
of previous methods, such as limited receptive fields, poor global context
capture, and challenges in high-frequency detail recovery. Additionally, the
paper reviews recent trends and advancements in transformer-based SR models,
exploring various innovative techniques and architectures that combine
transformers with traditional networks to balance global and local contexts.
These neoteric methods are critically analyzed, revealing promising yet
unexplored gaps and potential directions for future research. Several
visualizations of models and techniques are included to foster a holistic
understanding of recent trends. This work seeks to offer a structured roadmap
for researchers at the forefront of deep learning, specifically exploring the
impact of transformers on super-resolution techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Intra- and Cross-frame Topological Consistency Scheme for
  Semi-supervised Atherosclerotic Coronary Plaque Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Zhang, Zihan Li, Dandan Shan, Yuehui Qiu, Qingqi Hong, Qingqiang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing the precision of segmenting coronary atherosclerotic plaques from
CT Angiography (CTA) images is pivotal for advanced Coronary Atherosclerosis
Analysis (CAA), which distinctively relies on the analysis of vessel
cross-section images reconstructed via Curved Planar Reformation. This task
presents significant challenges due to the indistinct boundaries and structures
of plaques and blood vessels, leading to the inadequate performance of current
deep learning models, compounded by the inherent difficulty in annotating such
complex data. To address these issues, we propose a novel dual-consistency
semi-supervised framework that integrates Intra-frame Topological Consistency
(ITC) and Cross-frame Topological Consistency (CTC) to leverage labeled and
unlabeled data. ITC employs a dual-task network for simultaneous segmentation
mask and Skeleton-aware Distance Transform (SDT) prediction, achieving similar
prediction of topology structure through consistency constraint without
additional annotations. Meanwhile, CTC utilizes an unsupervised estimator for
analyzing pixel flow between skeletons and boundaries of adjacent frames,
ensuring spatial continuity. Experiments on two CTA datasets show that our
method surpasses existing semi-supervised methods and approaches the
performance of supervised methods on CAA. In addition, our method also performs
better than other methods on the ACDC dataset, demonstrating its
generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flow: A Modular Approach to Automated Agentic Workflow Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boye Niu, Yiliao Song, Kai Lian, Yifan Shen, Yu Yao, Kun Zhang, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent frameworks powered by large language models (LLMs) have
demonstrated great success in automated planning and task execution. However,
the effective adjustment of Agentic workflows during execution has not been
well-studied. A effective workflow adjustment is crucial, as in many real-world
scenarios, the initial plan must adjust to unforeseen challenges and changing
conditions in real-time to ensure the efficient execution of complex tasks. In
this paper, we define workflows as an activity-on-vertex (AOV) graphs. We
continuously refine the workflow by dynamically adjusting task allocations
based on historical performance and previous AOV with LLM agents. To further
enhance system performance, we emphasize modularity in workflow design based on
measuring parallelism and dependence complexity. Our proposed multi-agent
framework achieved efficient sub-task concurrent execution, goal achievement,
and error tolerance. Empirical results across different practical tasks
demonstrate dramatic improvements in the efficiency of multi-agent frameworks
through dynamic workflow updating and modularization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction Interval Construction Method for Electricity Prices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction of electricity prices plays an essential role in the
electricity market. To reflect the uncertainty of electricity prices, price
intervals are predicted. This paper proposes a novel prediction interval
construction method. A conditional generative adversarial network is first
presented to generate electricity price scenarios, with which the prediction
intervals can be constructed. Then, different generated scenarios are stacked
to obtain the probability densities, which can be applied to accurately reflect
the uncertainty of electricity prices. Furthermore, a reinforced prediction
mechanism based on the volatility level of weather factors is introduced to
address the spikes or volatile prices. A case study is conducted to verify the
effectiveness of the proposed novel prediction interval construction method.
The method can also provide the probability density of each price scenario
within the prediction interval and has the superiority to address the volatile
prices and price spikes with a reinforced prediction mechanism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Verification and Refinement of Language Model Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonho Ko, Jinheon Baek, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance across a wide
range of natural language tasks. However, a critical challenge remains in that
they sometimes generate factually incorrect answers. To address this, while
many previous work has focused on identifying errors in their generation and
further refining them, they are slow in deployment since they are designed to
verify the response from LLMs only after their entire generation (from the
first to last tokens) is done. Further, we observe that once LLMs generate
incorrect tokens early on, there is a higher likelihood that subsequent tokens
will also be factually incorrect. To this end, in this work, we propose
Streaming-VR (Streaming Verification and Refinement), a novel approach designed
to enhance the efficiency of verification and refinement of LLM outputs.
Specifically, the proposed Streaming-VR enables on-the-fly verification and
correction of tokens as they are being generated, similar to a streaming
process, ensuring that each subset of tokens is checked and refined in
real-time by another LLM as the LLM constructs its response. Through
comprehensive evaluations on multiple datasets, we demonstrate that our
approach not only enhances the factual accuracy of LLMs, but also offers a more
efficient solution compared to prior refinement methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh D. Dhole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Among parameter-efficient fine-tuning methods, freezing has emerged as a
popular strategy for speeding up training, reducing catastrophic forgetting,
and improving downstream performance. We investigate the impact of freezing the
decoder in a multi-task setup comprising diverse natural language tasks, aiming
to reduce deployment overhead and enhance portability to novel tasks. Our
experiments, conducted by fine-tuning both individual and multi-task setups on
the AlexaTM model, reveal that freezing decoders is highly effective for tasks
with natural language outputs and mitigates catastrophic forgetting in
multilingual tasks. However, we find that pairing frozen decoders with a larger
model can effectively maintain or even enhance performance in structured and QA
tasks, making it a viable strategy for a broader range of task types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STTS-EAD: Improving Spatio-Temporal Learning Based Time Series
  Prediction via 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyuan Liang, Tianhao Zhang, Tingyu Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handling anomalies is a critical preprocessing step in multivariate time
series prediction. However, existing approaches that separate anomaly
preprocessing from model training for multivariate time series prediction
encounter significant limitations. Specifically, these methods fail to utilize
auxiliary information crucial for identifying latent anomalies associated with
spatiotemporal factors during the preprocessing stage. Instead, they rely
solely on data distribution for anomaly detection, which can result in the
incorrect processing of numerous samples that could otherwise contribute
positively to model training. To address this, we propose STTS-EAD, an
end-to-end method that seamlessly integrates anomaly detection into the
training process of multivariate time series forecasting and aims to improve
Spatio-Temporal learning based Time Series prediction via Embedded Anomaly
Detection. Our proposed STTS-EAD leverages spatio-temporal information for
forecasting and anomaly detection, with the two parts alternately executed and
optimized for each other. To the best of our knowledge, STTS-EAD is the first
to integrate anomaly detection and forecasting tasks in the training phase for
improving the accuracy of multivariate time series forecasting. Extensive
experiments on a public stock dataset and two real-world sales datasets from a
renowned coffee chain enterprise show that our proposed method can effectively
process detected anomalies in the training stage to improve forecasting
performance in the inference stage and significantly outperform baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal mapping Coordinates Physics-Informed Neural Networks
  (CoCo-PINNs): learning neural networks for designing neutral inclusions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daehee Cho, Hyeonmin Yun, Jaeyong Lee, Mikyoung Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We focus on designing and solving the neutral inclusion problem via neural
networks. The neutral inclusion problem has a long history in the theory of
composite materials, and it is exceedingly challenging to identify the precise
condition that precipitates a general-shaped inclusion into a neutral
inclusion. Physics-informed neural networks (PINNs) have recently become a
highly successful approach to addressing both forward and inverse problems
associated with partial differential equations. We found that traditional PINNs
perform inadequately when applied to the inverse problem of designing neutral
inclusions with arbitrary shapes. In this study, we introduce a novel approach,
Conformal mapping Coordinates Physics-Informed Neural Networks (CoCo-PINNs),
which integrates complex analysis techniques into PINNs. This method exhibits
strong performance in solving forward-inverse problems to construct neutral
inclusions of arbitrary shapes in two dimensions, where the imperfect interface
condition on the inclusion's boundary is modeled by training neural networks.
Notably, we mathematically prove that training with a single linear field is
sufficient to achieve neutrality for untrained linear fields in arbitrary
directions, given a minor assumption. We demonstrate that CoCo-PINNs offer
enhanced performances in terms of credibility, consistency, and stability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioPose: Biomechanically-accurate 3D Pose Estimation from Monocular
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnoosh Koleini, Muhammad Usama Saleem, Pu Wang, Hongfei Xue, Ahmed Helmy, Abbey Fenwick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D human pose estimation from single-camera images and
videos have relied on parametric models, like SMPL. However, these models
oversimplify anatomical structures, limiting their accuracy in capturing true
joint locations and movements, which reduces their applicability in
biomechanics, healthcare, and robotics. Biomechanically accurate pose
estimation, on the other hand, typically requires costly marker-based motion
capture systems and optimization techniques in specialized labs. To bridge this
gap, we propose BioPose, a novel learning-based framework for predicting
biomechanically accurate 3D human pose directly from monocular videos. BioPose
includes three key components: a Multi-Query Human Mesh Recovery model
(MQ-HMR), a Neural Inverse Kinematics (NeurIK) model, and a 2D-informed pose
refinement technique. MQ-HMR leverages a multi-query deformable transformer to
extract multi-scale fine-grained image features, enabling precise human mesh
recovery. NeurIK treats the mesh vertices as virtual markers, applying a
spatial-temporal network to regress biomechanically accurate 3D poses under
anatomical constraints. To further improve 3D pose estimations, a 2D-informed
refinement step optimizes the query tokens during inference by aligning the 3D
structure with 2D pose observations. Experiments on benchmark datasets
demonstrate that BioPose significantly outperforms state-of-the-art methods.
Project website:
\url{https://m-usamasaleem.github.io/publication/BioPose/BioPose.html}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linearly Convergent Mixup Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gakuto Obi, Ayato Saito, Yuto Sasaki, Tsuyoshi Kato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning in the reproducing kernel Hilbert space (RKHS) such as the support
vector machine has been recognized as a promising technique. It continues to be
highly effective and competitive in numerous prediction tasks, particularly in
settings where there is a shortage of training data or computational
limitations exist. These methods are especially valued for their ability to
work with small datasets and their interpretability. To address the issue of
limited training data, mixup data augmentation, widely used in deep learning,
has remained challenging to apply to learning in RKHS due to the generation of
intermediate class labels. Although gradient descent methods handle these
labels effectively, dual optimization approaches are typically not directly
applicable. In this study, we present two novel algorithms that extend to a
broader range of binary classification models. Unlike gradient-based
approaches, our algorithms do not require hyperparameters like learning rates,
simplifying their implementation and optimization. Both the number of
iterations to converge and the computational cost per iteration scale linearly
with respect to the dataset size. The numerical experiments demonstrate that
our algorithms achieve faster convergence to the optimal solution compared to
gradient descent approaches, and that mixup data augmentation consistently
improves the predictive performance across various loss functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>none</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transforming Indoor Localization: Advanced Transformer Architecture for
  NLOS Dominated Wireless Environments with Distributed Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saad Masrur,  Jung-Fu,  Cheng, Atieh R. Khamesi, Ismail Guvenc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indoor localization in challenging non-line-of-sight (NLOS) environments
often leads to mediocre accuracy with traditional approaches. Deep learning
(DL) has been applied to tackle these challenges; however, many DL approaches
overlook computational complexity, especially for floating-point operations
(FLOPs), making them unsuitable for resource-limited devices. Transformer-based
models have achieved remarkable success in natural language processing (NLP)
and computer vision (CV) tasks, motivating their use in wireless applications.
However, their use in indoor localization remains nascent, and directly
applying Transformers for indoor localization can be both computationally
intensive and exhibit limitations in accuracy. To address these challenges, in
this work, we introduce a novel tokenization approach, referred to as Sensor
Snapshot Tokenization (SST), which preserves variable-specific representations
of power delay profile (PDP) and enhances attention mechanisms by effectively
capturing multi-variate correlation. Complementing this, we propose a
lightweight Swish-Gated Linear Unit-based Transformer (L-SwiGLU Transformer)
model, designed to reduce computational complexity without compromising
localization accuracy. Together, these contributions mitigate the computational
burden and dependency on large datasets, making Transformer models more
efficient and suitable for resource-constrained scenarios. The proposed
tokenization method enables the Vanilla Transformer to achieve a 90th
percentile positioning error of 0.388 m in a highly NLOS indoor factory,
surpassing conventional tokenization methods. The L-SwiGLU ViT further reduces
the error to 0.355 m, achieving an 8.51% improvement. Additionally, the
proposed model outperforms a 14.1 times larger model with a 46.13% improvement,
underscoring its computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been submitted to IEEE Transactions on Machine Learning
  in Communications and Networking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symmetry-Aware Generative Modeling through Learned Canonicalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kusha Sareen, Daniel Levy, Arnab Kumar Mondal, Sékou-Oumar Kaba, Tara Akhound-Sadegh, Siamak Ravanbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative modeling of symmetric densities has a range of applications in AI
for science, from drug discovery to physics simulations. The existing
generative modeling paradigm for invariant densities combines an invariant
prior with an equivariant generative process. However, we observe that this
technique is not necessary and has several drawbacks resulting from the
limitations of equivariant networks. Instead, we propose to model a learned
slice of the density so that only one representative element per orbit is
learned. To accomplish this, we learn a group-equivariant canonicalization
network that maps training samples to a canonical pose and train a
non-equivariant generative model over these canonicalized samples. We implement
this idea in the context of diffusion models. Our preliminary experimental
results on molecular modeling are promising, demonstrating improved sample
quality and faster inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurReps 2024 Workshop Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BMIP: Bi-directional Modality Interaction Prompt Learning for VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song-Lin Lv, Yu-Yang Chen, Zhi Zhou, Ming Yang, Lan-Zhe Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have exhibited remarkable generalization
capabilities, and prompt learning for VLMs has attracted great attention for
the ability to adapt pre-trained VLMs to specific downstream tasks. However,
existing studies mainly focus on single-modal prompts or uni-directional
modality interaction, overlooking the powerful alignment effects resulting from
the interaction between the vision and language modalities. To this end, we
propose a novel prompt learning method called
$\underline{\textbf{B}}i-directional \underline{\textbf{M}}odality
\underline{\textbf{I}}nteraction \underline{\textbf{P}}rompt (BMIP)$, which
dynamically weights bi-modal information through learning the information of
the attention layer, enhancing trainability and inter-modal consistency
compared to simple information aggregation methods. To evaluate the
effectiveness of prompt learning methods, we propose a more realistic
evaluation paradigm called open-world generalization complementing the widely
adopted cross-dataset transfer and domain generalization tasks. Comprehensive
experiments on various datasets reveal that BMIP not only outperforms current
state-of-the-art methods across all three evaluation paradigms but is also
flexible enough to be combined with other prompt-based methods for consistent
performance enhancement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rate-In: Information-Driven Adaptive Dropout Rates for Improved
  Inference-Time Uncertainty Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tal Zeevi, Ravid Shwartz-Ziv, Yann LeCun, Lawrence H. Staib, John A. Onofrey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty estimation is crucial for deploying neural networks in
risk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a
widely used technique for approximating predictive uncertainty by performing
stochastic forward passes with dropout during inference. However, using static
dropout rates across all layers and inputs can lead to suboptimal uncertainty
estimates, as it fails to adapt to the varying characteristics of individual
inputs and network layers. Existing approaches optimize dropout rates during
training using labeled data, resulting in fixed inference-time parameters that
cannot adjust to new data distributions, compromising uncertainty estimates in
Monte Carlo simulations.
  In this paper, we propose Rate-In, an algorithm that dynamically adjusts
dropout rates during inference by quantifying the information loss induced by
dropout in each layer's feature maps. By treating dropout as controlled noise
injection and leveraging information-theoretic principles, Rate-In adapts
dropout rates per layer and per input instance without requiring ground truth
labels. By quantifying the functional information loss in feature maps, we
adaptively tune dropout rates to maintain perceptual quality across diverse
medical imaging tasks and architectural configurations. Our extensive empirical
study on synthetic data and real-world medical imaging tasks demonstrates that
Rate-In improves calibration and sharpens uncertainty estimates compared to
fixed or heuristic dropout rates without compromising predictive performance.
Rate-In offers a practical, unsupervised, inference-time approach to optimizing
dropout for more reliable predictive uncertainty estimation in critical
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated author affiliation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Distribution Matching of Representations via Noise-Injected
  Deep InfoMax 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Butakov, Alexander Semenenko, Alexander Tolmachev, Andrey Gladkov, Marina Munkhoeva, Alexey Frolov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep InfoMax (DIM) is a well-established method for self-supervised
representation learning (SSRL) based on maximization of the mutual information
between the input and the output of a deep neural network encoder. Despite the
DIM and contrastive SSRL in general being well-explored, the task of learning
representations conforming to a specific distribution (i.e., distribution
matching, DM) is still under-addressed. Motivated by the importance of DM to
several downstream tasks (including generative modeling, disentanglement,
outliers detection and other), we enhance DIM to enable automatic matching of
learned representations to a selected prior distribution. To achieve this, we
propose injecting an independent noise into the normalized outputs of the
encoder, while keeping the same InfoMax training objective. We show that such
modification allows for learning uniformly and normally distributed
representations, as well as representations of other absolutely continuous
distributions. Our approach is tested on various downstream tasks. The results
indicate a moderate trade-off between the performance on the downstream tasks
and quality of DM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 7 fugures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt
  Optimization for Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02748v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02748v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han He, Qianchu Liu, Lei Xu, Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing automatic prompt engineering methods are typically designed for
discriminative tasks, where new task prompts are iteratively refined with
limited feedback from a single metric reflecting a single aspect. However,
these approaches are suboptimal for generative tasks, which require more
nuanced guidance beyond a single numeric metric to improve the prompt and
optimize multiple aspects of the generated text. To address these challenges,
we propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt
Optimization (CriSPO) approach. CriSPO introduces a critique-suggestion module
as its core component. This module spontaneously discovers aspects, and
compares generated and reference texts across these aspects, providing specific
suggestions for prompt modification. These clear critiques and actionable
suggestions guide a receptive optimizer module to make more substantial
changes, exploring a broader and more effective search space. To further
improve CriSPO with multi-metric optimization, we introduce an Automatic Suffix
Tuning (AST) extension to enhance the performance of task prompts across
multiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4
summarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score
improvement on summarization and substantial improvement of various metrics on
QA. Code available at https://github.com/amazon-science/crispo
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Detection and Analysis of Minor Deformations in Flat Walls Due
  to Railway Vibrations Using LiDAR and Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Surjo Dey, Ankit Sharma, Hritu Raj, Susham Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces an advanced methodology for automatically identifying
minor deformations in flat walls caused by vibrations from nearby railway
tracks. It leverages high-density Terrestrial Laser Scanner (TLS) LiDAR surveys
and AI/ML techniques to collect and analyze data. The scan data is processed
into a detailed point cloud, which is segmented to distinguish ground points,
trees, buildings, and other objects. The analysis focuses on identifying
sections along flat walls and estimating their deformations relative to the
ground orientation.
  Findings from the study, conducted at the RGIPT campus, reveal significant
deformations in walls close to the railway corridor, with the highest
deformations ranging from 7 to 8 cm and an average of 3 to 4 cm. In contrast,
walls further from the corridor show negligible deformations. The developed
automated process for feature extraction and deformation monitoring
demonstrates potential for structural health monitoring. By integrating LiDAR
data with machine learning, the methodology provides an efficient system for
identifying and analyzing structural deformations, highlighting the importance
of continuous monitoring for ensuring structural integrity and public safety in
urban infrastructure. This approach represents a substantial advancement in
automated feature extraction and deformation analysis, contributing to more
effective management of urban infrastructure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I am requesting the withdrawal of my paper due to the need for
  significant revisions to ensure the accuracy and integrity of the presented
  findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-Agnostic Modeling of Source Reliability on Wikipedia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacopo D'Ignazi, Andreas Kaltenbrunner, Yelena Mejova, Michele Tizzani, Kyriaki Kalimeri, Mariano Beiró, Pablo Aragón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last few years, content verification through reliable sources has
become a fundamental need to combat disinformation. Here, we present a
language-agnostic model designed to assess the reliability of sources across
multiple language editions of Wikipedia. Utilizing editorial activity data, the
model evaluates source reliability within different articles of varying
controversiality such as Climate Change, COVID-19, History, Media, and Biology
topics. Crafting features that express domain usage across articles, the model
effectively predicts source reliability, achieving an F1 Macro score of
approximately 0.80 for English and other high-resource languages. For
mid-resource languages, we achieve 0.65 while the performance of low-resource
languages varies; in all cases, the time the domain remains present in the
articles (which we dub as permanence) is one of the most predictive features.
We highlight the challenge of maintaining consistent model performance across
languages of varying resource levels and demonstrate that adapting models from
higher-resource languages can improve performance. This work contributes not
only to Wikipedia's efforts in ensuring content verifiability but in ensuring
reliability across diverse user-generated content in various language
communities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> of Foundation Models in Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wasif Khan, Seowung Leem, Kyle B. See, Joshua K. Wong, Shaoting Zhang, Ruogu Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) are large-scale deep learning models that are
developed using large datasets and self-supervised learning methods. These
models serve as a base for different downstream tasks, including healthcare.
FMs have been adopted with great success across various domains within
healthcare. Existing healthcare-based surveys have not yet included all of
these domains. Therefore, we provide a detailed survey of FMs in healthcare. We
focus on the history, learning strategies, flagship models, applications, and
challenges of FMs. We explore how FMs such as the BERT and GPT families are
reshaping various healthcare domains, including clinical large language models,
medical image analysis, and omics. Furthermore, we provide a detailed taxonomy
of healthcare applications facilitated by FMs, such as clinical NLP, medical
computer vision, graph learning, and other biology-related tasks. Despite the
promising opportunities FMs provide, they also have several associated
challenges, which are explained in detail. We also outline open research issues
and potential lessons learned to provide researchers and practitioners with
insights into the capabilities of FMs in healthcare to advance their deployment
and mitigate associated risks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pareto Set Learning for Multi-Objective Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erlong Liu, Yu-Chang Wu, Xiaobin Huang, Chengrui Gao, Ren-Jian Wang, Ke Xue, Chao Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective decision-making problems have emerged in numerous real-world
scenarios, such as video games, navigation and robotics. Considering the clear
advantages of Reinforcement Learning (RL) in optimizing decision-making
processes, researchers have delved into the development of Multi-Objective RL
(MORL) methods for solving multi-objective decision problems. However, previous
methods either cannot obtain the entire Pareto front, or employ only a single
policy network for all the preferences over multiple objectives, which may not
produce personalized solutions for each preference. To address these
limitations, we propose a novel decomposition-based framework for MORL, Pareto
Set Learning for MORL (PSL-MORL), that harnesses the generation capability of
hypernetwork to produce the parameters of the policy network for each
decomposition weight, generating relatively distinct policies for various
scalarized subproblems with high efficiency. PSL-MORL is a general framework,
which is compatible for any RL algorithm. The theoretical result guarantees the
superiority of the model capacity of PSL-MORL and the optimality of the
obtained policy network. Through extensive experiments on diverse benchmarks,
we demonstrate the effectiveness of PSL-MORL in achieving dense coverage of the
Pareto front, significantly outperforming state-of-the-art MORL methods in the
hypervolume and sparsity indicators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025 Accept</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feedback-driven object detection and iterative model improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sönke Tenckhoff, Mario Koddenbrock, Erik Rodner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated object detection has become increasingly valuable across diverse
applications, yet efficient, high-quality annotation remains a persistent
challenge. In this paper, we present the development and evaluation of a
platform designed to interactively improve object detection models. The
platform allows uploading and annotating images as well as fine-tuning object
detection models. Users can then manually review and refine annotations,
further creating improved snapshots that are used for automatic object
detection on subsequent image uploads - a process we refer to as semi-automatic
annotation resulting in a significant gain in annotation efficiency.
  Whereas iterative refinement of model results to speed up annotation has
become common practice, we are the first to quantitatively evaluate its
benefits with respect to time, effort, and interaction savings. Our
experimental results show clear evidence for a significant time reduction of up
to 53% for semi-automatic compared to manual annotation. Importantly, these
efficiency gains did not compromise annotation quality, while matching or
occasionally even exceeding the accuracy of manual annotations. These findings
demonstrate the potential of our lightweight annotation platform for creating
high-quality object detection datasets and provide best practices to guide
future development of annotation platforms.
  The platform is open-source, with the frontend and backend repositories
available on GitHub (https://github.com/ml-lab-htw/iterative-annotate). To
support the understanding of our labeling process, we have created an
explanatory video demonstrating the methodology using microscopy images of E.
coli bacteria as an example. The video is available on YouTube
(https://www.youtube.com/watch?v=CM9uhE8NN5E).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AI4EA24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jui-Che Chiang, Hou-Ning Hu, Bo-Syuan Hou, Chia-Yu Tseng, Yu-Lun Liu, Min-Hung Chen, Yen-Yu Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although facial landmark detection (FLD) has gained significant progress,
existing FLD methods still suffer from performance drops on partially
non-visible faces, such as faces with occlusions or under extreme lighting
conditions or poses. To address this issue, we introduce ORFormer, a novel
transformer-based method that can detect non-visible regions and recover their
missing features from visible parts. Specifically, ORFormer associates each
image patch token with one additional learnable token called the messenger
token. The messenger token aggregates features from all but its patch. This
way, the consensus between a patch and other patches can be assessed by
referring to the similarity between its regular and messenger embeddings,
enabling non-visible region identification. Our method then recovers occluded
patches with features aggregated by the messenger tokens. Leveraging the
recovered features, ORFormer compiles high-quality heatmaps for the downstream
FLD task. Extensive experiments show that our method generates heatmaps
resilient to partial occlusions. By integrating the resultant heatmaps into
existing FLD methods, our method performs favorably against the state of the
arts on challenging datasets such as WFLW and COFW.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025 Project Link: https://ben0919.github.io/ORFormer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WINE: Wavelet-Guided GAN Inversion and Editing for High-Fidelity
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaewon Kim, Seung-Jun Moon, Gyeong-Moon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advanced GAN inversion models aim to convey high-fidelity information
from original images to generators through methods using generator tuning or
high-dimensional feature learning. Despite these efforts, accurately
reconstructing image-specific details remains as a challenge due to the
inherent limitations both in terms of training and structural aspects, leading
to a bias towards low-frequency information. In this paper, we look into the
widely used pixel loss in GAN inversion, revealing its predominant focus on the
reconstruction of low-frequency features. We then propose WINE, a
Wavelet-guided GAN Inversion aNd Editing model, which transfers the
high-frequency information through wavelet coefficients via newly proposed
wavelet loss and wavelet fusion scheme. Notably, WINE is the first attempt to
interpret GAN inversion in the frequency domain. Our experimental results
showcase the precision of WINE in preserving high-frequency details and
enhancing image quality. Even in editing scenarios, WINE outperforms existing
state-of-the-art GAN inversion models with a fine balance between editability
and reconstruction quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out
  Context Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengyuan Liu, Nikhil Kandpal, Colin Raffel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The influence of contextual input on the behavior of large language models
(LLMs) has prompted the development of context attribution methods that aim to
quantify each context span's effect on an LLM's generations. The leave-one-out
(LOO) error, which measures the change in the likelihood of the LLM's response
when a given span of the context is removed, provides a principled way to
perform context attribution, but can be prohibitively expensive to compute for
large models. In this work, we introduce AttriBoT, a series of novel techniques
for efficiently computing an approximation of the LOO error for context
attribution. Specifically, AttriBoT uses cached activations to avoid redundant
operations, performs hierarchical attribution to reduce computation, and
emulates the behavior of large target models with smaller proxy models. Taken
together, AttriBoT can provide a >300x speedup while remaining more faithful to
a target model's LOO error than prior context attribution methods. This stark
increase in performance makes computing context attributions for a given
response 30x faster than generating the response itself, empowering real-world
applications that require computing attributions at scale. We release a
user-friendly and efficient implementation of AttriBoT to enable efficient LLM
interpretability as well as encourage future development of efficient context
attribution methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Electricity Price Prediction Using Multi-Kernel Gaussian Process
  Regression Combined with Kernel-Based Support Vector Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Das, Stephan Schlüter, Lorenz Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new hybrid model for predicting German electricity
prices. The algorithm is based on combining Gaussian Process Regression (GPR)
and Support Vector Regression (SVR). While GPR is a competent model for
learning the stochastic pattern within the data and interpolation, its
performance for out-of-sample data is not very promising. By choosing a
suitable data-dependent covariance function, we can enhance the performance of
GPR for the tested German hourly power prices. However, since the out-of-sample
prediction depends on the training data, the prediction is vulnerable to noise
and outliers. To overcome this issue, a separate prediction is made using SVR,
which applies margin-based optimization, having an advantage in dealing with
non-linear processes and outliers, since only certain necessary points (support
vectors) in the training data are responsible for regression. Both individual
predictions are later combined using the performance-based weight assignment
method. A test on historic German power prices shows that this approach
outperforms its chosen benchmarks such as the autoregressive exogenous model,
the naive approach, as well as the long short-term memory approach of
prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Set-based Neural Network Encoding Without Weight Tying 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16625v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16625v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Andreis, Soro Bedionita, Philip H. S. Torr, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a neural network weight encoding method for network property
prediction that utilizes set-to-set and set-to-vector functions to efficiently
encode neural network parameters. Our approach is capable of encoding neural
networks in a model zoo of mixed architecture and different parameter sizes as
opposed to previous approaches that require custom encoding models for
different architectures. Furthermore, our \textbf{S}et-based \textbf{N}eural
network \textbf{E}ncoder (SNE) takes into consideration the hierarchical
computational structure of neural networks. To respect symmetries inherent in
network weight space, we utilize Logit Invariance to learn the required minimal
invariance properties. Additionally, we introduce a \textit{pad-chunk-encode}
pipeline to efficiently encode neural network layers that is adjustable to
computational and memory constraints. We also introduce two new tasks for
neural network property prediction: cross-dataset and cross-architecture. In
cross-dataset property prediction, we evaluate how well property predictors
generalize across model zoos trained on different datasets but of the same
architecture. In cross-architecture property prediction, we evaluate how well
property predictors transfer to model zoos of different architecture not seen
during training. We show that SNE outperforms the relevant baselines on
standard benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximation Rates in Fréchet Metrics: Barron Spaces, Paley-Wiener
  Spaces, and Fourier Multipliers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Abdeljawad, Thomas Dittrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operator learning is a recent development in the simulation of Partial
Differential Equations (PDEs) by means of neural networks. The idea behind this
approach is to learn the behavior of an operator, such that the resulting
neural network is an (approximate) mapping in infinite-dimensional spaces that
is capable of (approximately) simulating the solution operator governed by the
PDE. In our work, we study some general approximation capabilities for linear
differential operators by approximating the corresponding symbol in the Fourier
domain. Analogous to the structure of the class of H\"ormander-Symbols, we
consider the approximation with respect to a topology that is induced by a
sequence of semi-norms. In that sense, we measure the approximation error in
terms of a Fr\'echet metric, and our main result identifies sufficient
conditions for achieving a predefined approximation error. Secondly, we then
focus on a natural extension of our main theorem, in which we manage to reduce
the assumptions on the sequence of semi-norms. Based on existing approximation
results for the exponential spectral Barron space, we then present a concrete
example of symbols that can be approximated well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Federated Graph Learning in One-shot Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11304v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11304v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guochen Yan, Xunkai Li, Luyuan Xie, Wentao Zhang, Qingni Shen, Yuejian Fang, Zhonghai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Graph Learning (FGL) has emerged as a promising paradigm for
breaking data silos among distributed private graphs. In practical scenarios
involving heterogeneous distributed graph data, personalized Federated Graph
Learning (pFGL) aims to enhance model utility by training personalized models
tailored to client needs. However, existing pFGL methods often require numerous
communication rounds under heterogeneous graphs, leading to significant
communication overhead and security concerns. While One-shot Federated Learning
(OFL) enables collaboration in a single round, existing OFL methods are
designed for image-centric tasks and ineffective for graph data, leaving a
critical gap in the field. Additionally, personalized models derived from
existing methods suffer from bias, failing to effectively generalize to the
minority. To address these challenges, we propose the first $\textbf{O}$ne-shot
$\textbf{p}$ersonalized $\textbf{F}$ederated $\textbf{G}$raph
$\textbf{L}$earning method ($\textbf{O-pFGL}$) for node classification,
compatible with Secure Aggregation protocols for privacy preservation.
Specifically, for effective graph learning in one communication round, our
method estimates and aggregates class-wise feature distribution statistics to
construct a global pseudo-graph on the server, facilitating the training of a
global graph model. To mitigate bias, we introduce a two-stage personalized
training approach that adaptively balances local personal information and
global insights from the pseudo-graph, improving both personalization and
generalization. Extensive experiments on 12 multi-scale graph datasets
demonstrate that our method significantly outperforms state-of-the-art
baselines across various settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Sub-graph Distillation for Robust Semi-supervised Continual
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Fan, Yu Wang, Pengfei Zhu, Qinghua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) has shown promising results and comparable
performance to learning at once in a fully supervised manner. However, CL
strategies typically require a large number of labeled samples, making their
real-life deployment challenging. In this work, we focus on semi-supervised
continual learning (SSCL), where the model progressively learns from partially
labeled data with unknown categories. We provide a comprehensive analysis of
SSCL and demonstrate that unreliable distributions of unlabeled data lead to
unstable training and refinement of the progressing stages. This problem
severely impacts the performance of SSCL. To address the limitations, we
propose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for
semi-supervised continual learning, which leverages both semantic and
structural information to achieve more stable knowledge distillation on
unlabeled data and exhibit robustness against distribution bias. Firstly, we
formalize a general model of structural distillation and design a dynamic graph
construction for the continual learning progress. Next, we define a structure
distillation vector and design a dynamic sub-graph distillation algorithm,
which enables end-to-end training and adaptability to scale up tasks. The
entire proposed method is adaptable to various CL methods and supervision
settings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100,
and ImageNet-100, with varying supervision ratios, demonstrate the
effectiveness of our proposed approach in mitigating the catastrophic
forgetting problem in semi-supervised continual learning scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balanced Neural ODEs: nonlinear model order reduction and Koopman
  operator approximations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10174v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10174v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Aka, Johannes Brunnemann, Jörg Eiden, Arne Speerforck, Lars Mikelsons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational Autoencoders (VAEs) are a powerful framework for learning latent
representations of reduced dimensionality, while Neural ODEs excel in learning
transient system dynamics. This work combines the strengths of both to generate
fast surrogate models with adjustable complexity reacting on time-varying
inputs signals. By leveraging the VAE's dimensionality reduction using a
nonhierarchical prior, our method adaptively assigns stochastic noise,
naturally complementing known NeuralODE training enhancements and enabling
probabilistic time series modeling. We show that standard Latent ODEs struggle
with dimensionality reduction in systems with time-varying inputs. Our approach
mitigates this by continuously propagating variational parameters through time,
establishing fixed information channels in latent space. This results in a
flexible and robust method that can learn different system complexities, e.g.
deep neural networks or linear matrices. Hereby, it enables efficient
approximation of the Koopman operator without the need for predefining its
dimensionality. As our method balances dimensionality reduction and
reconstruction accuracy, we call it Balanced Neural ODE (B-NODE). We
demonstrate the effectiveness of this methods on several academic and
real-world test cases, e.g. a power plant or MuJoCo data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference paper under review, after revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spurious Feature Eraser: Stabilizing Test-Time Adaptation for
  Vision-Language Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00376v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00376v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Ma, Yan Zhu, Changqing Zhang, Peilin Zhao, Baoyuan Wu, Long-Kai Huang, Qinghua Hu, Bingzhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language foundation models have exhibited remarkable success across a
multitude of downstream tasks due to their scalability on extensive image-text
paired data. However, these models also display significant limitations when
applied to downstream tasks, such as fine-grained image classification, as a
result of ``decision shortcuts'' that hinder their generalization capabilities.
In this work, we find that the CLIP model possesses a rich set of features,
encompassing both \textit{desired invariant causal features} and
\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP
on downstream tasks originates from its inability to effectively utilize
pre-trained features in accordance with specific task requirements. To address
this challenge, we propose a simple yet effective method, Spurious Feature
Eraser (SEraser), to alleviate the decision shortcuts by erasing the spurious
features. Specifically, we introduce a test-time prompt tuning paradigm that
optimizes a learnable prompt, thereby compelling the model to exploit invariant
features while disregarding decision shortcuts during the inference phase. The
proposed method effectively alleviates excessive dependence on potentially
misleading spurious information. We conduct comparative analysis of the
proposed method against various approaches which validates the significant
superiority.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ImagiNet: A Multi-Content Benchmark for Synthetic Image Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20020v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20020v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delyan Boychev, Radostin Cholakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent generative models produce images with a level of authenticity that
makes them nearly indistinguishable from real photos and artwork. Potential
harmful use cases of these models, necessitate the creation of robust synthetic
image detectors. However, current datasets in the field contain generated
images with questionable quality or have examples from one predominant content
type which leads to poor generalizability of the underlying detectors. We find
that the curation of a balanced amount of high-resolution generated images
across various content types is crucial for the generalizability of detectors,
and introduce ImagiNet, a dataset of 200K examples, spanning four categories:
photos, paintings, faces, and miscellaneous. Synthetic images in ImagiNet are
produced with both open-source and proprietary generators, whereas real
counterparts for each content type are collected from public datasets. The
structure of ImagiNet allows for a two-track evaluation system: i)
classification as real or synthetic and ii) identification of the generative
model. To establish a strong baseline, we train a ResNet-50 model using a
self-supervised contrastive objective (SelfCon) for each track which achieves
evaluation AUC of up to 0.99 and balanced accuracy ranging from 86% to 95%,
even under conditions that involve compression and resizing. The provided model
is generalizable enough to achieve zero-shot state-of-the-art performance on
previous synthetic detection benchmarks. We provide ablations to demonstrate
the importance of content types and publish code and data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Datasets and Evaluators of AI Safety, AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Agent: Leveraging LLMs For Audio Generation, Editing and
  Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Wang, Chi-Keung Tang, Yu-Wing Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Audio-Agent, a multimodal framework for audio generation,
editing and composition based on text or video inputs. Conventional approaches
for text-to-audio (TTA) tasks often make single-pass inferences from text
descriptions. While straightforward, this design struggles to produce
high-quality audio when given complex text conditions. In our method, we
utilize a pre-trained TTA diffusion network as the audio generation agent to
work in tandem with GPT-4, which decomposes the text condition into atomic,
specific instructions and calls the agent for audio generation. In doing so,
Audio-Agent can generate high-quality audio that is closely aligned with the
provided text or video exhibiting complex and multiple events, while supporting
variable-length and variable-volume generation. For video-to-audio (VTA) tasks,
most existing methods require training a timestamp detector to synchronize
video events with the generated audio, a process that can be tedious and
time-consuming. Instead, we propose a simpler approach by fine-tuning a
pre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both
semantic and temporal conditions that bridge the video and audio modality.
Consequently, our framework contributes a comprehensive solution for both TTA
and VTA tasks without substantial computational overhead in training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Correlation-Aware Graph Convolutional Networks for Multi-Label Node
  Classification <span class="chip">KDD2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17350v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17350v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchen Bei, Weizhi Chen, Hao Chen, Sheng Zhou, Carl Yang, Jiapei Fan, Longtao Huang, Jiajun Bu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label node classification is an important yet under-explored domain in
graph mining as many real-world nodes belong to multiple categories rather than
just a single one. Although a few efforts have been made by utilizing Graph
Convolution Networks (GCNs) to learn node representations and model
correlations between multiple labels in the embedding space, they still suffer
from the ambiguous feature and ambiguous topology induced by multiple labels,
which reduces the credibility of the messages delivered in graphs and overlooks
the label correlations on graph data. Therefore, it is crucial to reduce the
ambiguity and empower the GCNs for accurate classification. However, this is
quite challenging due to the requirement of retaining the distinctiveness of
each label while fully harnessing the correlation between labels
simultaneously. To address these issues, in this paper, we propose a
Correlation-aware Graph Convolutional Network (CorGCN) for multi-label node
classification. By introducing a novel Correlation-Aware Graph Decomposition
module, CorGCN can learn a graph that contains rich label-correlated
information for each label. It then employs a Correlation-Enhanced Graph
Convolution to model the relationships between labels during message passing to
further bolster the classification process. Extensive experiments on five
datasets demonstrate the effectiveness of our proposed CorGCN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, accepted by KDD2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Lebeau, Florent Chatelain, Romain Couillet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a comprehensive understanding of the estimation of a
planted low-rank signal from a general spiked tensor model near the
computational threshold. Relying on standard tools from the theory of large
random matrices, we characterize the large-dimensional spectral behavior of the
unfoldings of the data tensor and exhibit relevant signal-to-noise ratios
governing the detectability of the principal directions of the signal. These
results allow to accurately predict the reconstruction performance of truncated
multilinear SVD (MLSVD) in the non-trivial regime. This is particularly
important since it serves as an initialization of the higher-order orthogonal
iteration (HOOI) scheme, whose convergence to the best low-multilinear-rank
approximation depends entirely on its initialization. We give a sufficient
condition for the convergence of HOOI and show that the number of iterations
before convergence tends to $1$ in the large-dimensional limit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth <span class="highlight-title">System</span>
  Model Fields with Generative Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Hess, Michael Aich, Baoxiang Pan, Niklas Boers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and high-resolution Earth system model (ESM) simulations are
essential to assess the ecological and socio-economic impacts of anthropogenic
climate change, but are computationally too expensive to be run at sufficiently
high spatial resolution. Recent machine learning approaches have shown
promising results in downscaling ESM simulations, outperforming
state-of-the-art statistical approaches. However, existing methods require
computationally costly retraining for each ESM and extrapolate poorly to
climates unseen during training. We address these shortcomings by learning a
consistency model (CM) that efficiently and accurately downscales arbitrary ESM
simulations without retraining in a zero-shot manner. Our approach yields
probabilistic downscaled fields at a resolution only limited by the
observational reference data. We show that the CM outperforms state-of-the-art
diffusion models at a fraction of computational cost while maintaining high
controllability on the downscaling task. Further, our method generalizes to
climate states unseen during training without explicitly formulated physical
constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Symmetries via Weight-Sharing with Doubly Stochastic Tensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Putri A. van der Linden, Alejandro García-Castellanos, Sharvaree Vadgama, Thijs P. Kuipers, Erik J. Bekkers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group equivariance has emerged as a valuable inductive bias in deep learning,
enhancing generalization, data efficiency, and robustness. Classically, group
equivariant methods require the groups of interest to be known beforehand,
which may not be realistic for real-world data. Additionally, baking in fixed
group equivariance may impose overly restrictive constraints on model
architecture. This highlights the need for methods that can dynamically
discover and apply symmetries as soft constraints. For neural network
architectures, equivariance is commonly achieved through group transformations
of a canonical weight tensor, resulting in weight sharing over a given group
$G$. In this work, we propose to learn such a weight-sharing scheme by defining
a collection of learnable doubly stochastic matrices that act as soft
permutation matrices on canonical weight tensors, which can take regular group
representations as a special case. This yields learnable kernel transformations
that are jointly optimized with downstream tasks. We show that when the dataset
exhibits strong symmetries, the permutation matrices will converge to regular
group representations and our weight-sharing networks effectively become
regular group convolutions. Additionally, the flexibility of the method enables
it to effectively pick up on partial symmetries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 14 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable and Resource-Efficient Second-Order Federated Learning via
  Over-the-Air Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07662v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07662v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulmomen Ghalkha, Chaouki Ben Issaid, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Second-order federated learning (FL) algorithms offer faster convergence than
their first-order counterparts by leveraging curvature information. However,
they are hindered by high computational and storage costs, particularly for
large-scale models. Furthermore, the communication overhead associated with
large models and digital transmission exacerbates these challenges, causing
communication bottlenecks. In this work, we propose a scalable second-order FL
algorithm using a sparse Hessian estimate and leveraging over-the-air
aggregation, making it feasible for larger models. Our simulation results
demonstrate more than $67\%$ of communication resources and energy savings
compared to other first and second-order baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, 4 subfigures, letter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Decoders for Transformer-based Semantic Segmentation: A
  Compression Perspective <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03033v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03033v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qishuai Wen, Chun-Guang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art methods for Transformer-based semantic segmentation
typically adopt Transformer decoders that are used to extract additional
embeddings from image embeddings via cross-attention, refine either or both
types of embeddings via self-attention, and project image embeddings onto the
additional embeddings via dot-product. Despite their remarkable success, these
empirical designs still lack theoretical justifications or interpretations,
thus hindering potentially principled improvements. In this paper, we argue
that there are fundamental connections between semantic segmentation and
compression, especially between the Transformer decoders and Principal
Component Analysis (PCA). From such a perspective, we derive a white-box, fully
attentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the
interpretations as follows: 1) the self-attention operator refines image
embeddings to construct an ideal principal subspace that aligns with the
supervision and retains most information; 2) the cross-attention operator seeks
to find a low-rank approximation of the refined image embeddings, which is
expected to be a set of orthonormal bases of the principal subspace and
corresponds to the predefined classes; 3) the dot-product operation yields
compact representation for image embeddings as segmentation masks. Experiments
conducted on dataset ADE20K find that DEPICT consistently outperforms its
black-box counterpart, Segmenter, and it is light weight and more robust.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS2024. Code:https://github.com/QishuaiWen/DEPICT/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenSafe: A Generalizable Safety Enhancer for Safe Reinforcement Learning
  Algorithms Based on Reduced Order Markov Decision Process Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehua Zhou, Xuan Xie, Jiayang Song, Zhan Shu, Lei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe Reinforcement Learning (SRL) aims to realize a safe learning process for
Deep Reinforcement Learning (DRL) algorithms by incorporating safety
constraints. However, the efficacy of SRL approaches often relies on accurate
function approximations, which are notably challenging to achieve in the early
learning stages due to data insufficiency. To address this issue, we introduce
in this work a novel Generalizable Safety enhancer (GenSafe) that is able to
overcome the challenge of data insufficiency and enhance the performance of SRL
approaches. Leveraging model order reduction techniques, we first propose an
innovative method to construct a Reduced Order Markov Decision Process (ROMDP)
as a low-dimensional approximator of the original safety constraints. Then, by
solving the reformulated ROMDP-based constraints, GenSafe refines the actions
of the agent to increase the possibility of constraint satisfaction.
Essentially, GenSafe acts as an additional safety layer for SRL algorithms. We
evaluate GenSafe on multiple SRL approaches and benchmark problems. The results
demonstrate its capability to improve safety performance, especially in the
early learning phases, while maintaining satisfactory task performance. Our
proposed GenSafe not only offers a novel measure to augment existing SRL
methods but also shows broad compatibility with various SRL algorithms, making
it applicable to a wide range of systems and SRL problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fair CoVariance Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Cavallo, Madeline Navarro, Santiago Segarra, Elvin Isufi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Covariance-based data processing is widespread across signal processing and
machine learning applications due to its ability to model data
interconnectivities and dependencies. However, harmful biases in the data may
become encoded in the sample covariance matrix and cause data-driven methods to
treat different subpopulations unfairly. Existing works such as fair principal
component analysis (PCA) mitigate these effects, but remain unstable in low
sample regimes, which in turn may jeopardize the fairness goal. To address both
biases and instability, we propose Fair coVariance Neural Networks (FVNNs),
which perform graph convolutions on the covariance matrix for both fair and
accurate predictions. Our FVNNs provide a flexible model compatible with
several existing bias mitigation techniques. In particular, FVNNs allow for
mitigating the bias in two ways: first, they operate on fair covariance
estimates that remove biases from their principal components; second, they are
trained in an end-to-end fashion via a fairness regularizer in the loss
function so that the model parameters are tailored to solve the task directly
in a fair manner. We prove that FVNNs are intrinsically fairer than analogous
PCA approaches thanks to their stability in low sample regimes. We validate the
robustness and fairness of our model on synthetic and real-world data,
showcasing the flexibility of FVNNs along with the tradeoff between fair and
accurate performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Attention as a Parametric Endofunctor: A Categorical Framework for
  Transformer Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles O'Neill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-attention mechanisms have revolutionised deep learning architectures,
yet their core mathematical structures remain incompletely understood. In this
work, we develop a category-theoretic framework focusing on the linear
components of self-attention. Specifically, we show that the query, key, and
value maps naturally define a parametric 1-morphism in the 2-category
$\mathbf{Para(Vect)}$. On the underlying 1-category $\mathbf{Vect}$, these maps
induce an endofunctor whose iterated composition precisely models multi-layer
attention. We further prove that stacking multiple self-attention layers
corresponds to constructing the free monad on this endofunctor. For positional
encodings, we demonstrate that strictly additive embeddings correspond to
monoid actions in an affine sense, while standard sinusoidal encodings, though
not additive, retain a universal property among injective (faithful)
position-preserving maps. We also establish that the linear portions of
self-attention exhibit natural equivariance to permutations of input tokens,
and show how the "circuits" identified in mechanistic interpretability can be
interpreted as compositions of parametric 1-morphisms. This categorical
perspective unifies geometric, algebraic, and interpretability-based approaches
to transformer analysis, making explicit the underlying structures of
attention. We restrict to linear maps throughout, deferring the treatment of
nonlinearities such as softmax and layer normalisation, which require more
advanced categorical constructions. Our results build on and extend recent work
on category-theoretic foundations for deep learning, offering deeper insights
into the algebraic structure of attention mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private Collaborative Edge Inference via Over-the-Air Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selim F. Yilmaz, Burak Hasircioglu, Li Qiao, Deniz Gunduz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider collaborative inference at the wireless edge, where each client's
model is trained independently on its local dataset. Clients are queried in
parallel to make an accurate decision collaboratively. In addition to
maximizing the inference accuracy, we also want to ensure the privacy of local
models. To this end, we leverage the superposition property of the multiple
access channel to implement bandwidth-efficient multi-user inference methods.
We propose different methods for ensemble and multi-view classification that
exploit over-the-air computation (OAC). We show that these schemes perform
better than their orthogonal counterparts with statistically significant
differences while using fewer resources and providing privacy guarantees. We
also provide experimental results verifying the benefits of the proposed OAC
approach to multi-user inference, and perform an ablation study to demonstrate
the effectiveness of our design choices. We share the source code of the
framework publicly on Github to facilitate further research and
reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures. This work extends from our preliminary study
  presented at the 2022 IEEE International Symposium on Information Theory [1].
  arXiv admin note: text overlap with arXiv:2202.03129</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of
  Large Language Models in Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangru Lin, Shaoguang Mao, Emanuele La Malfa, Valentin Hofmann, Adrian de Wynter, Xun Wang, Si-Qing Chen, Michael Wooldridge, Janet B. Pierrehumbert, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is not monolithic. While benchmarks, including those designed for
multiple languages, are often used as proxies to evaluate the performance of
Large Language Models (LLMs), they tend to overlook the nuances of
within-language variation, and thus fail to model the experience of speakers of
non-standard dialects. Focusing on African American Vernacular English (AAVE),
we present the first study aimed at objectively assessing the fairness and
robustness of LLMs in handling dialects in canonical reasoning tasks, including
algorithm, math, logic, and integrated reasoning. We introduce \textbf{ReDial}
(\textbf{Re}asoning with \textbf{Dial}ect Queries), a benchmark containing
1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE
speakers, including experts with computer science backgrounds, to rewrite seven
popular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate
widely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model
families. Our findings reveal that \textbf{almost all of these widely used
models show significant brittleness and unfairness to queries in AAVE}. Our
work establishes a systematic and objective framework for analyzing LLM bias in
dialectal queries. Moreover, it highlights how mainstream LLMs provide unfair
service to dialect speakers in reasoning tasks, laying a critical foundation
for relevant future research. Code and data can be accessed at
https://github.com/fangru-lin/redial_dialect_robustness_fairness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesis and Analysis of Data as Probability Measures with
  Entropy-Regularized Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brendan Mallery, James M. Murphy, Shuchin Aeron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider synthesis and analysis of probability measures using the
entropy-regularized Wasserstein-2 cost and its unbiased version, the Sinkhorn
divergence. The synthesis problem consists of computing the barycenter, with
respect to these costs, of $m$ reference measures given a set of coefficients
belonging to the $m$-dimensional simplex. The analysis problem consists of
finding the coefficients for the closest barycenter in the Wasserstein-2
distance to a given measure $\mu$. Under the weakest assumptions on the
measures thus far in the literature, we compute the derivative of the
entropy-regularized Wasserstein-2 cost. We leverage this to establish a
characterization of regularized barycenters as solutions to a fixed-point
equation for the average of the entropic maps from the barycenter to the
reference measures. This characterization yields a finite-dimensional, convex,
quadratic program for solving the analysis problem when $\mu$ is a barycenter.
It is shown that these coordinates, as well as the value of the barycenter
functional, can be estimated from samples with dimension-independent rates of
convergence, a hallmark of entropy-regularized optimal transport, and we verify
these rates experimentally. We also establish that barycentric coordinates are
stable with respect to perturbations in the Wasserstein-2 metric, suggesting a
robustness of these coefficients to corruptions. We employ the barycentric
coefficients as features for classification of corrupted point cloud data, and
show that compared to neural network baselines, our approach is more efficient
in small training data regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages. Code to reproduce experiments:
  https://github.com/brendanmallery9/Entropic-Barycenters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KAN KAN Buff Signed Graph Neural Networks? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhieddine Shebaro, Jelena Tešić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Representation Learning focuses on creating embeddings for nodes and
edges that capture their features and connections. Graph Neural Networks (GNNs)
use neural networks to model complex graph relationships. The Kolmogorov-Arnold
Neural Network (KAN) has recently emerged as an alternative to the Multi-Layer
Perceptron (MLP), offering better accuracy and interpretability with fewer
parameters. KANs have been applied to GNN tasks. This paper introduces the
integration of KANs into Signed Graph Convolutional Networks (SGCNs). We
evaluate KAN-enhanced SGCNs (KASGCN) on signed community detection and link
sign prediction tasks to improve embedding quality in signed networks. While
the results show some variability, KASGCN performs competitively with or
similarly to the standard SGCN in the functions tested. Its effectiveness
depends on the specific context, such as the signed graph and parameter
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of Artificial Intelligence Methods for Lead Time Prediction
  in Non-Cycled Areas of Automotive Production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cornelius Hake, Jonas Weigele, Frederik Reichert, Christian Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The present study examines the effectiveness of applying Artificial
Intelligence methods in an automotive production environment to predict unknown
lead times in a non-cycle-controlled production area. Data structures are
analyzed to identify contextual features and then preprocessed using one-hot
encoding. Methods selection focuses on supervised machine learning techniques.
In supervised learning methods, regression and classification methods are
evaluated. Continuous regression based on target size distribution is not
feasible. Classification methods analysis shows that Ensemble Learning and
Support Vector Machines are the most suitable. Preliminary study results
indicate that gradient boosting algorithms LightGBM, XGBoost, and CatBoost
yield the best results. After further testing and extensive hyperparameter
optimization, the final method choice is the LightGBM algorithm. Depending on
feature availability and prediction interval granularity, relative prediction
accuracies of up to 90% can be achieved. Further tests highlight the importance
of periodic retraining of AI models to accurately represent complex production
processes using the database. The research demonstrates that AI methods can be
effectively applied to highly variable production data, adding business value
by providing an additional metric for various control tasks while outperforming
current non AI-based systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Set-Based Training for Neural Network Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14961v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14961v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Koller, Tobias Ladner, Matthias Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are vulnerable to adversarial attacks, i.e., small input
perturbations can significantly affect the outputs of a neural network.
Therefore, to ensure safety of safety-critical environments, the robustness of
a neural network must be formally verified against input perturbations, e.g.,
from noisy sensors. To improve the robustness of neural networks and thus
simplify the formal verification, we present a novel set-based training
procedure in which we compute the set of possible outputs given the set of
possible inputs and compute for the first time a gradient set, i.e., each
possible output has a different gradient. Therefore, we can directly reduce the
size of the output enclosure by choosing gradients toward its center. Small
output enclosures increase the robustness of a neural network and, at the same
time, simplify its formal verification. The latter benefit is due to the fact
that a larger size of propagated sets increases the conservatism of most
verification methods. Our extensive evaluation demonstrates that set-based
training produces robust neural networks with competitive performance, which
can be verified using fast (polynomial-time) verification algorithms due to the
reduced output set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COOL: Efficient and Reliable Chain-Oriented Objective Logic with Neural
  Networks Feedback Control for Program Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13874v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13874v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jipeng Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Program synthesis methods, whether formal or neural-based, lack fine-grained
control and flexible modularity, which limits their adaptation to complex
software development. These limitations stem from rigid Domain-Specific
Language (DSL) frameworks and neural network incorrect predictions. To this
end, we propose the Chain of Logic (CoL), which organizes the synthesis process
into an activity flow and provides heuristic control to guide the process.
Furthermore, by integrating neural networks with libraries and introducing a
Neural Network Feedback Control (NNFC) mechanism, our approach modularizes
synthesis and mitigates the impact of neural network mispredictions.
Experiments on relational and symbolic synthesis tasks show that CoL
significantly enhances the efficiency and reliability of DSL program synthesis
across multiple metrics. Specifically, CoL improves accuracy by 70% while
reducing tree operations by 91% and time by 95%. Additionally, NNFC further
boosts accuracy by 6%, with a 64% reduction in tree operations under
challenging conditions such as insufficient training data, increased
difficulty, and multidomain synthesis. These improvements confirm COOL as a
highly efficient and reliable program synthesis framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoPE: Mixture of Prompt Experts for Parameter-Efficient and Scalable
  Multimodal Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10568v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10568v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Jiang, Lingbo Liu, Changwen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the demonstrated parameter efficiency of prompt-based multimodal
fusion methods, their limited adaptivity and expressiveness often result in
suboptimal performance compared to other tuning approaches. In this paper, we
introduce the Mixture of Prompt Experts (MoPE), the first technique designed to
overcome these limitations by decomposing standard prompts to capture
instance-level features adaptively. Building on this decomposition, MoPE
enhances prompt fusion's expressiveness by leveraging multimodal pairing priors
to route the most effective prompt for each instance dynamically. Compared to
vanilla prompting, our MoPE-based fusion method exhibits greater
expressiveness, scaling more effectively with the training data and the overall
number of trainable parameters. We also investigate regularization terms for
expert routing, which lead to emergent expert specialization with enhanced
adaptiveness and interpretablity. Extensive experiments across six multimodal
datasets spanning four modalities demonstrate state-of-the-art performance for
prompt fusion, matching or even surpassing the performance of fine-tuning while
requiring only 0.8% of the trainable parameters. Project homepage:
https://github.com/songrise/MoPE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review, Extended version of arxiv:2312.03734</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Layer-Adaptive State Pruning for Deep State Space Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minseon Gwak, Seongrok Moon, Joohwan Ko, PooGyeon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the lack of state dimension optimization methods, deep state space
models (SSMs) have sacrificed model capacity, training search space, or
stability to alleviate computational costs caused by high state dimensions. In
this work, we provide a structured pruning method for SSMs, Layer-Adaptive
STate pruning (LAST), which reduces the state dimension of each layer in
minimizing model-level output energy loss by extending modal truncation for a
single system. LAST scores are evaluated using the $\mathcal{H}_{\infty}$ norms
of subsystems and layer-wise energy normalization. The scores serve as global
pruning criteria, enabling cross-layer comparison of states and layer-adaptive
pruning. Across various sequence benchmarks, LAST optimizes previous SSMs,
revealing the redundancy and compressibility of their state spaces. Notably, we
demonstrate that, on average, pruning 33% of states still maintains performance
with 0.52% accuracy loss in multi-input multi-output SSMs without retraining.
Code is available at https://github.com/msgwak/LAST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven Bayesian State Estimation with Compressed Measurement of
  Model-free Process using Semi-supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anubhab Ghosh, Yonina C. Eldar, Saikat Chatterjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research topic is: data-driven Bayesian state estimation with compressed
measurement (BSCM) of model-free process, say for a (causal) tracking
application. The dimension of the temporal measurement vector is lower than the
dimension of the temporal state vector to be estimated. Hence the state
estimation problem is an underdetermined inverse problem. The underlying
dynamical model of the states is assumed to be unknown and hence, we use the
terminology 'model-free process'. In absence of the dynamical model, we can not
employ traditional model-driven methods like Kalman Filter (KF) and Particle
Filter (PF), and instead require data-driven methods. We first experimentally
show that two existing unsupervised learning-based data-driven methods fail to
address the BSCM problem for model-free process; they are - data-driven
nonlinear state estimation (DANSE) method and deep Markov model (DMM) method.
The unsupervised learning uses unlabelled data comprised of only noisy, linear
measurements. While DANSE provides a good predictive / forecasting performance
to model the temporal measurement data as time-series, its unsupervised
learning lacks a regularization for state estimation. We then investigate the
use of a semi-supervised learning approach, and develop a semi-supervised
learning-based DANSE method, referred to as SemiDANSE. In SemiDANSE, we use a
limited amount of labelled data along-with a large amount of unlabelled data,
and that helps to bring the desired regularization for addressing the BSCM
problem. The labelled data means pairwise measurement-and-state data. Using
three chaotic dynamical systems (or processes) with nonlinear dynamical models
as benchmark, we show that the data-driven SemiDANSE provides competitive
performance for BSCM against a hybrid method called KalmanNet and two
model-driven methods -- an extended KF (EKF) and an unscented KF (UKF).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, under review at IEEE TSP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoye Chai, Xiaoqian Qi, Shiyuan Zhang, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile traffic forecasting allows operators to anticipate network dynamics
and performance in advance, offering substantial potential for enhancing
service quality and improving user experience. However, existing models are
often task-oriented and are trained with tailored data, which limits their
effectiveness in diverse mobile network tasks of Base Station (BS) deployment,
resource allocation, energy optimization, etc. and hinders generalization
across different urban environments. Foundation models have made remarkable
strides across various domains of NLP and CV due to their multi-tasking
adaption and zero/few-shot learning capabilities. In this paper, we propose an
innovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to
handle diverse forecasting tasks of short/long-term predictions and
distribution generation across multiple cities to support network planning and
optimization. FoMo combines diffusion models and transformers, where various
spatio-temporal masks are proposed to enable FoMo to learn intrinsic features
of different tasks, and a contrastive learning strategy is developed to capture
the correlations between mobile traffic and urban contexts, thereby improving
its transfer learning capability. Extensive experiments on 9 real-world
datasets demonstrate that FoMo outperforms current models concerning diverse
forecasting tasks and zero/few-shot learning, showcasing a strong universality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Less Certain Adversarial Examples Improves Robust
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04539v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04539v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minxing Zhang, Michael Backes, Xiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revisits the robust overfitting phenomenon of adversarial
training. Observing that models with better robust generalization performance
are less certain in predicting adversarially generated training inputs, we
argue that overconfidence in predicting adversarial examples is a potential
cause. Therefore, we hypothesize that generating less certain adversarial
examples improves robust generalization, and propose a formal definition of
adversarial certainty that captures the variance of the model's predicted
logits on adversarial examples. Our theoretical analysis of synthetic
distributions characterizes the connection between adversarial certainty and
robust generalization. Accordingly, built upon the notion of adversarial
certainty, we develop a general method to search for models that can generate
training-time adversarial inputs with reduced certainty, while maintaining the
model's capability in distinguishing adversarial examples. Extensive
experiments on image benchmarks demonstrate that our method effectively learns
models with consistently improved robustness and mitigates robust overfitting,
confirming the importance of generating less certain adversarial examples for
robust generalization. Our implementations are available as open-source code
at: https://github.com/TrustMLRG/AdvCertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A User's Guide to $\texttt{KSig}$: GPU-Accelerated Computation of the
  Signature Kernel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Csaba Tóth, Danilo Jr Dela Cruz, Harald Oberhauser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The signature kernel is a positive definite kernel for sequential and
temporal data that has become increasingly popular in machine learning
applications due to powerful theoretical guarantees, strong empirical
performance, and recently introduced various scalable variations. In this
chapter, we give a short introduction to $\texttt{KSig}$, a
$\texttt{Scikit-Learn}$ compatible Python package that implements various
GPU-accelerated algorithms for computing signature kernels, and performing
downstream learning tasks. We also introduce a new algorithm based on tensor
sketches which gives strong performance compared to existing algorithms. The
package is available at https://github.com/tgcsaba/ksig.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Gradient Subspaces: Addressing and Overcoming LoRA's
  Limitations in Federated Fine-Tuning of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23111v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23111v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navyansh Mahla, Kshitij Sharad Jadhav, Ganesh Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities across
various domains, particularly in task generalization for both text and vision
data. While fine-tuning these models can significantly enhance their
performance on specific downstream tasks, it often requires high-quality data
that cannot be shared due to privacy concerns. Federated Learning (FL) offers a
promising solution for collaborative training without direct data sharing.
However, many parameter-efficient fine-tuning strategies for LLMs in FL,
particularly those based on Low-Rank Adaptation (LoRA), face limitations. In
this paper, we critically analyze the convergence and performance guarantees of
popular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to
constrained subspace learning of low-rank matrices. This limitation hinders
effective fine-tuning of LLMs in federated settings. Through rigorous
analytical and empirical evaluations, we demonstrate that direct weight
averaging outperforms LoRA-based strategies, leading to superior performance
for fine-tuned models. Our comprehensive comparison unmasks inefficiencies in
LoRA approaches and underscores the advantages of direct weight aggregation. We
extend our analysis to low-rank gradient-based optimizers, such as GaLore, used
during local training steps. Our findings show that GaLore along with
direct-weight aggregation is a more effective approach, outperforming federated
LoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.
While privacy remains paramount in FL discourse, our focus is on assessing
performance outcomes of federated fine-tuned models and evaluating various FL
frameworks from both theoretical and empirical perspectives. Our findings
advocate reassessing the reliance on LoRA within FL contexts, paving the way
for more efficient training methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Random Policy Enables In-Context Reinforcement Learning within Trust
  Horizons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqin Chen, Santiago Paternain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained foundation models have exhibited extraordinary in-context learning
performance, allowing zero-shot generalization to new tasks not encountered
during pretraining. In the case of reinforcement learning (RL), in-context RL
(ICRL) emerges when pretraining FMs on decision-making problems in an
autoregressive-supervised manner. Nevertheless, current state-of-the-art ICRL
algorithms, like Algorithm Distillation, Decision Pretrained Transformer and
Decision Importance Transformer, impose stringent requirements on the
pretraining dataset concerning the source policies, context information, and
action labels. Notably, these algorithms either demand optimal policies or
require varying degrees of well-trained behavior policies for all pretraining
environments. This significantly hinders the application of ICRL to real-world
scenarios, where acquiring optimal or well-trained policies for a substantial
volume of real-world training environments can be intractable. To overcome this
challenge, we introduce a novel approach, termed State-Action Distillation
(SAD), that allows to generate an effective pretraining dataset guided solely
by random policies. In particular, SAD selects query states and corresponding
action labels by distilling outstanding state-action pairs from the entire
state and action spaces by using random policies within a trust horizon, and
then inherits the classical autoregressive-supervised mechanism during
pretraining. To the best of our knowledge, this is the first work that enables
effective ICRL under random policies and random contexts. We also establish
quantitative analysis of the trustworthiness as well as the performance
guarantees of SAD. Moreover, our empirical results across multiple popular ICRL
benchmark environments demonstrate that, on average, SAD outperforms the best
baseline by 236.3% in the offline evaluation and by 135.2% in the online
evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Doubly-Bounded Queue for Constrained Online Learning: Keeping Pace with
  Dynamics of Both Loss and Constraint 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juncheng Wang, Bingjie Yan, Yituo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider online convex optimization with time-varying constraints and
conduct performance analysis using two stringent metrics: dynamic regret with
respect to the online solution benchmark, and hard constraint violation that
does not allow any compensated violation over time. We propose an efficient
algorithm called Constrained Online Learning with Doubly-bounded Queue (COLDQ),
which introduces a novel virtual queue that is both lower and upper bounded,
allowing tight control of the constraint violation without the need for the
Slater condition. We prove via a new Lyapunov drift analysis that COLDQ
achieves $O(T^\frac{1+V_x}{2})$ dynamic regret and $O(T^{V_g})$ hard constraint
violation, where $V_x$ and $V_g$ capture the dynamics of the loss and
constraint functions. For the first time, the two bounds smoothly approach to
the best-known $O(T^\frac{1}{2})$ regret and $O(1)$ violation, as the dynamics
of the losses and constraints diminish. For strongly convex loss functions,
COLDQ matches the best-known $O(\log{T})$ static regret while maintaining the
$O(T^{V_g})$ hard constraint violation. We further introduce an expert-tracking
variation of COLDQ, which achieves the same performance bounds without any
prior knowledge of the system dynamics. Simulation results demonstrate that
COLDQ outperforms the state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-matrix Factorization Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose novel attention architectures, Multi-matrix Factorization
Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard
Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain
as strong performance under stringent Key-Value cache (KV cache) constraints.
MFA enhances model capacity by efficiently scaling up both the number and
dimension of attention heads through low-rank matrix factorization in the
Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory
requirements by repurposing the key cache as value through value projection
re-parameterization. MFA's design enables strong model capacity when working
under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache
limits with minor performance trade-off. Notably, in our extensive and
large-scale experiments, the proposed architecture outperforms MLA and performs
comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdaSociety: An Adaptive Environment with Social Structures for
  Multi-Agent Decision-Making <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03865v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03865v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Huang, Xingbo Wang, Hao Liu, Fanqi Kong, Aoyang Qin, Min Tang, Song-Chun Zhu, Mingjie Bi, Siyuan Qi, Xue Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional interactive environments limit agents' intelligence growth with
fixed tasks. Recently, single-agent environments address this by generating new
tasks based on agent actions, enhancing task diversity. We consider the
decision-making problem in multi-agent settings, where tasks are further
influenced by social connections, affecting rewards and information access.
However, existing multi-agent environments lack a combination of adaptive
physical surroundings and social connections, hindering the learning of
intelligent behaviors. To address this, we introduce AdaSociety, a customizable
multi-agent environment featuring expanding state and action spaces, alongside
explicit and alterable social structures. As agents progress, the environment
adaptively generates new tasks with social structures for agents to undertake.
In AdaSociety, we develop three mini-games showcasing distinct social
structures and tasks. Initial results demonstrate that specific social
structures can promote both individual and collective benefits, though current
reinforcement learning and LLM-based algorithms show limited effectiveness in
leveraging social structures to enhance performance. Overall, AdaSociety serves
as a valuable research platform for exploring intelligence in diverse physical
and social settings. The code is available at
https://github.com/bigai-ai/AdaSociety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS D&B 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lean Attention: Hardware-Aware Scalable Attention Mechanism for the
  Decode-Phase of Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rya Sanovar, Srikant Bharadwaj, Renee St. Amant, Victor Rühle, Saravan Rajmohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have emerged as one of the most widely used
architectures for natural language processing, natural language generation, and
image generation. The size of the state-of-the-art models has increased
steadily reaching billions of parameters. These huge models are memory hungry
and incur significant inference latency even on cutting edge AI-accelerators,
such as GPUs. Specifically, the time and memory complexity of the attention
operation is quadratic in terms of the total context length, i.e., prompt and
output tokens. Thus, several optimizations such as key-value tensor caching and
FlashAttention computation have been proposed to deliver the low latency
demands of applications relying on such large models. However, these techniques
do not cater to the computationally distinct nature of different phases during
inference.
  To that end, we propose LeanAttention, a scalable technique of computing
self-attention for the token-generation phase (decode-phase) of decoder-only
transformer models. LeanAttention enables scaling the attention mechanism
implementation for the challenging case of long context lengths by re-designing
the execution flow for the decode-phase. We identify that the associative
property of online softmax can be treated as a reduction operation thus
allowing us to parallelize the attention computation over these large context
lengths. We extend the "stream-K" style reduction of tiled calculation to
self-attention to enable parallel computation resulting in an average of 2.6x
attention execution speedup over FlashAttention-2 and up to 8.33x speedup for
512k context lengths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Poisoning Attacks on Federated Learning-based Wireless Traffic
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14389v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14389v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Zhang, Minghong Fang, Jiayuan Huang, Yuchen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) offers a distributed framework to train a global
control model across multiple base stations without compromising the privacy of
their local network data. This makes it ideal for applications like wireless
traffic prediction (WTP), which plays a crucial role in optimizing network
resources, enabling proactive traffic flow management, and enhancing the
reliability of downstream communication-aided applications, such as IoT
devices, autonomous vehicles, and industrial automation systems. Despite its
promise, the security aspects of FL-based distributed wireless systems,
particularly in regression-based WTP problems, remain inadequately
investigated. In this paper, we introduce a novel fake traffic injection (FTI)
attack, designed to undermine the FL-based WTP system by injecting fabricated
traffic distributions with minimal knowledge. We further propose a defense
mechanism, termed global-local inconsistency detection (GLID), which
strategically removes abnormal model parameters that deviate beyond a specific
percentile range estimated through statistical methods in each dimension.
Extensive experimental evaluations, performed on real-world wireless traffic
datasets, demonstrate that both our attack and defense strategies significantly
outperform existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IFIP/IEEE Networking 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Radar Signal Recognition through Self-Supervised Learning and Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03461v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03461v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Huang, Simon Denman, Akila Pemasiri, Clinton Fookes, Terrence Martin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic radar signal recognition (RSR) plays a pivotal role in electronic
warfare (EW), as accurately classifying radar signals is critical for informing
decision-making processes. Recent advances in deep learning have shown
significant potential in improving RSR performance in domains with ample
annotated data. However, these methods fall short in EW scenarios where
annotated RF data are scarce or impractical to obtain. To address these
challenges, we introduce a self-supervised learning (SSL) method which utilises
masked signal modelling and RF domain adaption to enhance RSR performance in
environments with limited RF samples and labels. Specifically, we investigate
pre-training masked autoencoders (MAE) on baseband in-phase and quadrature
(I/Q) signals from various RF domains and subsequently transfer the learned
representation to the radar domain, where annotated data are limited. Empirical
results show that our lightweight self-supervised ResNet model with domain
adaptation achieves up to a 17.5% improvement in 1-shot classification accuracy
when pre-trained on in-domain signals (i.e., radar signals) and up to a 16.31%
improvement when pre-trained on out-of-domain signals (i.e., comm signals),
compared to its baseline without SSL. We also provide reference results for
several MAE designs and pre-training strategies, establishing a new benchmark
for few-shot radar signal classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactually Fair Reinforcement Learning via Sequential Data
  Preprocessing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06366v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06366v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jitao Wang, Chengchun Shi, John D. Piette, Joshua R. Loftus, Donglin Zeng, Zhenke Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When applied in healthcare, reinforcement learning (RL) seeks to dynamically
match the right interventions to subjects to maximize population benefit.
However, the learned policy may disproportionately allocate efficacious actions
to one subpopulation, creating or exacerbating disparities in other
socioeconomically-disadvantaged subgroups. These biases tend to occur in
multi-stage decision making and can be self-perpetuating, which if unaccounted
for could cause serious unintended consequences that limit access to care or
treatment benefit. Counterfactual fairness (CF) offers a promising statistical
tool grounded in causal inference to formulate and study fairness. In this
paper, we propose a general framework for fair sequential decision making. We
theoretically characterize the optimal CF policy and prove its stationarity,
which greatly simplifies the search for optimal CF policies by leveraging
existing RL algorithms. The theory also motivates a sequential data
preprocessing algorithm to achieve CF decision making under an additive noise
assumption. We prove and then validate our policy learning approach in
controlling unfairness and attaining optimal value through simulations.
Analysis of a digital health dataset designed to reduce opioid misuse shows
that our proposal greatly enhances fair access to counseling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computational and Statistical Asymptotic Analysis of the JKO Scheme for
  Iterative Algorithms to update distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang Wu, Yazhen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The seminal paper of Jordan, Kinderlehrer, and Otto introduced what is now
widely known as the JKO scheme, an iterative algorithmic framework for
computing distributions. This scheme can be interpreted as a Wasserstein
gradient flow and has been successfully applied in machine learning contexts,
such as deriving policy solutions in reinforcement learning. In this paper, we
extend the JKO scheme to accommodate models with unknown parameters.
Specifically, we develop statistical methods to estimate these parameters and
adapt the JKO scheme to incorporate the estimated values. To analyze the
adopted statistical JKO scheme, we establish an asymptotic theory via
stochastic partial differential equations that describes its limiting dynamic
behavior. Our framework allows both the sample size used in parameter
estimation and the number of algorithmic iterations to go to infinity. This
study offers a unified framework for joint computational and statistical
asymptotic analysis of the statistical JKO scheme. On the computational side,
we examine the scheme's dynamic behavior as the number of iterations increases,
while on the statistical side, we investigate the large-sample behavior of the
resulting distributions computed through the scheme. We conduct numerical
simulations to evaluate the finite-sample performance of the proposed methods
and validate the developed asymptotic theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELDER: Enhancing Lifelong Model Editing with Mixture-of-LoRA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11869v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11869v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaang Li, Quan Wang, Zhongnan Wang, Yongdong Zhang, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) require model editing to efficiently update
specific knowledge within them and avoid factual errors. Most model editing
methods are solely designed for single-time use and result in a significant
forgetting effect in lifelong editing scenarios, where sequential edits are
conducted over time. Previous approaches manage sequential edits by freezing
original parameters and discretely allocating new parameters for each knowledge
update. However, these methods lack robustness to minor input variations due to
the discrete mapping between data and parameters. To overcome this challenge,
we propose ELDER, a novel approach to create a continuous association between
data and adapters. ELDER integrates multiple LoRAs through a router network and
is trained to establish a smooth data-adapter association, thereby enhancing
the edit robustness and generalization of semantically equivalent inputs. To
ensure inputs containing the same knowledge will be processed by the same
LoRAs, we design a novel loss to guide the model link LoRA allocations with
edit knowledge. Furthermore, we propose a deferral mechanism to retain the
original LLM capabilities post-edit. Extensive experiments on GPT-2 XL and
LLaMA2-7B demonstrate that ELDER effectively edits models in the lifelong
setting, outperforming eight baselines while exhibiting strong scalability and
preserving LLMs' general abilities on downstream tasks. Our code is available
at https://github.com/JiaangL/ELDER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zerui Xu, Fang Wu, Yuanyuan Zhang, Yue Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) exhibits promise in the clinical domain. However, it is
constrained by data scarcity and ethical considerations, as the generation of
clinical trials presents significant challenges due to stringent privacy
regulations, high costs, and the extended duration required for conducting
studies with human participants. Despite the advancements of large language
models (LLMs) in general generation tasks, their potential in facilitating the
generation of synthetic clinical trials is under-explored. To address this gap,
we introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs
to generate artificial yet realistic and diverse clinical trials with binary
success/failure labels. Experiments conducted on real clinical trials from the
\url{ClinicalTrials.gov} database demonstrate that our synthetic data can
effectively augment real datasets. Furthermore, by fine-tuning a pre-trained
model as a binary classifier on synthetic clinical trial datasets, we
demonstrate that this augmentation enhances model training for downstream tasks
such as trial outcome prediction. Our findings suggest that LLMs for synthetic
clinical trial generation hold promise for accelerating clinical research and
upholding ethical standards for patient privacy. The code is publicly available
at
https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Foundation Models for Wearable Movement Data in Mental Health
  Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15240v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15240v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franklin Y. Ruan, Aiwei Zhang, Jenny Y. Oh, SouYoung Jin, Nicholas C. Jacobson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained foundation models and transformer architectures have driven the
success of large language models (LLMs) and other modern AI breakthroughs.
However, similar advancements in health data modeling remain limited due to the
need for innovative adaptations. Wearable movement data offers a valuable
avenue for exploration, as it's a core feature in nearly all commercial
smartwatches, well established in clinical and mental health research, and the
sequential nature of the data shares similarities to language. We introduce the
Pretrained Actigraphy Transformer (PAT), the first open source foundation model
designed for time-series wearable movement data. Leveraging transformer-based
architectures and novel techniques, such as patch embeddings, and pretraining
on data from 29,307 participants in a national U.S. sample, PAT achieves
state-of-the-art performance in several mental health prediction tasks. PAT is
also lightweight and easily interpretable, making it a robust tool for mental
health research.
  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-Efficient Split Learning for Fine-Tuning Large Language Models in
  Edge Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuguang Li, Shaohua Wu, Liang Li, Songge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, we propose an energy-efficient split learning (SL) framework
for fine-tuning large language models (LLMs) using geo-distributed personal
data at the network edge, where LLMs are split and alternately across massive
mobile devices and an edge server. Considering the device heterogeneity and
channel dynamics in edge networks, a \underline{C}ut l\underline{A}yer and
computing \underline{R}esource \underline{D}ecision (CARD) algorithm is
developed to minimize training delay and energy consumption. Simulation results
demonstrate that the proposed approach reduces the average training delay and
server's energy consumption by 70.8% and 53.1%, compared to the benchmarks,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Go AIs be adversarially robust? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12843v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12843v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Tseng, Euan McLean, Kellin Pelrine, Tony T. Wang, Adam Gleave
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior work found that superhuman Go AIs can be defeated by simple adversarial
strategies, especially "cyclic" attacks. In this paper, we study whether adding
natural countermeasures can achieve robustness in Go, a favorable domain for
robustness since it benefits from incredible average-case capability and a
narrow, innately adversarial setting. We test three defenses: adversarial
training on hand-constructed positions, iterated adversarial training, and
changing the network architecture. We find that though some of these defenses
protect against previously discovered attacks, none withstand freshly trained
adversaries. Furthermore, most of the reliably effective attacks these
adversaries discover are different realizations of the same overall class of
cyclic attacks. Our results suggest that building robust AI systems is
challenging even with extremely superhuman systems in some of the most
tractable settings, and highlight two key gaps: efficient generalization of
defenses, and diversity in training. For interactive examples of attacks and a
link to our codebase, see https://goattack.far.ai.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>63 pages, AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physically Guided Deep Unsupervised Inversion for 1D Magnetotelluric
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15274v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15274v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Goyes-Peñafiel, Umair bin Waheed, Henry Arguello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global demand for unconventional energy sources such as geothermal energy
and white hydrogen requires new exploration techniques for precise subsurface
structure characterization and potential reservoir identification. The
Magnetotelluric (MT) method is crucial for these tasks, providing critical
information on the distribution of subsurface electrical resistivity at depths
ranging from hundreds to thousands of meters. However, traditional iterative
algorithm-based inversion methods require the adjustment of multiple
parameters, demanding time-consuming and exhaustive tuning processes to achieve
proper cost function minimization. Recent advances have incorporated deep
learning algorithms for MT inversion, primarily based on supervised learning,
and large labeled datasets are needed for training. This work utilizes
TensorFlow operations to create a differentiable forward MT operator,
leveraging its automatic differentiation capability. Moreover, instead of
solving for the subsurface model directly, as classical algorithms perform,
this paper presents a new deep unsupervised inversion algorithm guided by
physics to estimate 1D MT models. Instead of using datasets with the observed
data and their respective model as labels during training, our method employs a
differentiable modeling operator that physically guides the cost function
minimization, making the proposed method solely dependent on observed data.
Therefore, the optimization algorithm updates the network weights to minimize
the data misfit. We test the proposed method with field and synthetic data at
different acquisition frequencies, demonstrating that the resistivity models
obtained are more accurate than those calculated using other techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures, github repository, submitted to IEEE-GRSL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\text{Transformer}^2$: Self-adaptive LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Sun, Edoardo Cetin, Yujin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-adaptive large language models (LLMs) aim to solve the challenges posed
by traditional fine-tuning methods, which are often computationally intensive
and static in their ability to handle diverse tasks. We introduce
$\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for
unseen tasks in real-time by selectively adjusting only the singular components
of their weight matrices. During inference, $\text{Transformer}^2$ employs a
two-pass mechanism: first, a dispatch system identifies the task properties,
and then task-specific "expert" vectors, trained using reinforcement learning,
are dynamically mixed to obtain targeted behavior for the incoming prompt. Our
method outperforms ubiquitous approaches such as LoRA, with fewer parameters
and greater efficiency. $\text{Transformer}^2$ demonstrates versatility across
different LLM architectures and modalities, including vision-language tasks.
$\text{Transformer}^2$ represents a significant leap forward, offering a
scalable, efficient solution for enhancing the adaptability and task-specific
performance of LLMs, paving the way for truly dynamic, self-organizing AI
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 panges, 11 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ E2ESlack: An End-to-End Graph-Based Framework for Pre-Routing Slack
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Bodhe, Zhanguang Zhang, Atia Hamidizadeh, Shixiong Kai, Yingxue Zhang, Mingxuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-routing slack prediction remains a critical area of research in
Electronic Design Automation (EDA). Despite numerous machine learning-based
approaches targeting this task, there is still a lack of a truly end-to-end
framework that engineers can use to obtain TNS/WNS metrics from raw circuit
data at the placement stage. Existing works have demonstrated effectiveness in
Arrival Time (AT) prediction but lack a mechanism for Required Arrival Time
(RAT) prediction, which is essential for slack prediction and obtaining TNS/WNS
metrics. In this work, we propose E2ESlack, an end-to-end graph-based framework
for pre-routing slack prediction. The framework includes a TimingParser that
supports DEF, SDF and LIB files for feature extraction and graph construction,
an arrival time prediction model and a fast RAT estimation module. To the best
of our knowledge, this is the first work capable of predicting path-level
slacks at the pre-routing stage. We perform extensive experiments and
demonstrate that our proposed RAT estimation method outperforms the SOTA
ML-based prediction method and also pre-routing STA tool. Additionally, the
proposed E2ESlack framework achieves TNS/WNS values comparable to post-routing
STA results while saving up to 23x runtime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient descent with generalized Newton's method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Bu, Shiyun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the generalized Newton's method (GeN) -- a Hessian-informed
approach that applies to any optimizer such as SGD and Adam, and covers the
Newton-Raphson method as a sub-case. Our method automatically and dynamically
selects the learning rate that accelerates the convergence, without the
intensive tuning of the learning rate scheduler. In practice, our method is
easily implementable, since it only requires additional forward passes with
almost zero computational overhead (in terms of training time and memory cost),
if the overhead is amortized over many iterations. We present extensive
experiments on language and vision tasks (e.g. GPT and ResNet) to showcase that
GeN optimizers match the state-of-the-art performance, which was achieved with
carefully tuned learning rate schedulers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can AI Help with Your Personal Finances? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19784v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19784v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oudom Hean, Utsha Saha, Binita Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have emerged as a
transformative development in artificial intelligence (AI), drawing significant
attention from industry and academia. Trained on vast datasets, these
sophisticated AI systems exhibit impressive natural language processing and
content generation capabilities. This paper explores the potential of LLMs to
address key challenges in personal finance, focusing on the United States. We
evaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,
Anthropic's Claude, and Meta's Llama, to assess their effectiveness in
providing accurate financial advice on topics such as mortgages, taxes, loans,
and investments. Our findings show that while these models achieve an average
accuracy rate of approximately 70%, they also display notable limitations in
certain areas. Specifically, LLMs struggle to provide accurate responses for
complex financial queries, with performance varying significantly across
different topics. Despite these limitations, the analysis reveals notable
improvements in newer versions of these models, highlighting their growing
utility for individuals and financial advisors. As these AI systems continue to
evolve, their potential for advancing AI-driven applications in personal
finance becomes increasingly promising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smartphone-based Eye Tracking <span class="highlight-title">System</span> using Edge Intelligence and Model
  Optimisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishan Gunawardena, Gough Yumu Lui, Jeewani Anupama Ginige, Bahman Javadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A significant limitation of current smartphone-based eye-tracking algorithms
is their low accuracy when applied to video-type visual stimuli, as they are
typically trained on static images. Also, the increasing demand for real-time
interactive applications like games, VR, and AR on smartphones requires
overcoming the limitations posed by resource constraints such as limited
computational power, battery life, and network bandwidth. Therefore, we
developed two new smartphone eye-tracking techniques for video-type visuals by
combining Convolutional Neural Networks (CNN) with two different Recurrent
Neural Networks (RNN), namely Long Short Term Memory (LSTM) and Gated Recurrent
Unit (GRU). Our CNN+LSTM and CNN+GRU models achieved an average Root Mean
Square Error of 0.955 cm and 1.091 cm, respectively. To address the
computational constraints of smartphones, we developed an edge intelligence
architecture to enhance the performance of smartphone-based eye tracking. We
applied various optimisation methods like quantisation and pruning to deep
learning models for better energy, CPU, and memory usage on edge devices,
focusing on real-time processing. Using model quantisation, the model inference
time in the CNN+LSTM and CNN+GRU models was reduced by 21.72% and 19.50%,
respectively, on edge devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I have included the three papers as reference, which are closely
  related. We have expanded the future work section to provide a more thorough
  discussion of the concepts of "varying lighting conditions" and "dynamic user
  environments." We have added a note below Table 4 to clarify the
  abbreviations' meaning. Elaborated the role of the Domain Expert within the
  presentation layer in Section 4.1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACPO: AI-Enabled Compiler Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09982v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09982v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir H. Ashouri, Muhammad Asif Manzoor, Duc Minh Vu, Raymond Zhang, Colin Toft, Ziwen Wang, Angel Zhang, Bryan Chan, Tomasz S. Czajkowski, Yaoqing Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key to performance optimization of a program is to decide correctly when
a certain transformation should be applied by a compiler. This is an ideal
opportunity to apply machine-learning models to speed up the tuning process;
while this realization has been around since the late 90s, only recent
advancements in ML enabled a practical application of ML to compilers as an
end-to-end framework.
  This paper presents ACPO: An AI-Enabled Compiler Framework, a novel framework
that provides LLVM with simple and comprehensive tools to benefit from
employing ML models for different optimization passes. We first showcase the
high-level view, class hierarchy, and functionalities of ACPO and subsequently,
demonstrate \taco{a couple of use cases of ACPO by ML-enabling the Loop Unroll
and Function Inlining passes used in LLVM's O3. and finally, describe how ACPO
can be leveraged to optimize other passes. Experimental results reveal that the
ACPO model for Loop Unroll can gain on average 4%, 3%, 5.4%, and 0.2% compared
to LLVM's vanilla O3 optimization when deployed on Polybench, Coral-2,
CoreMark, and Graph-500, respectively. Furthermore, by including both Function
Inlining and Loop Unroll models, ACPO can provide a combined speedup of 4.5% on
Polybench and 2.4% on Cbench when compared with LLVM's O3, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACPO (12 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular
  Data Classification via Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12404v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12404v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhee Kim, Taesung Kim, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable in-context learning
capabilities across diverse applications. In this work, we explore the
effectiveness of LLMs for generating realistic synthetic tabular data,
identifying key prompt design elements to optimize performance. We introduce
EPIC, a novel approach that leverages balanced, grouped data samples and
consistent formatting with unique variable mapping to guide LLMs in generating
accurate synthetic data across all classes, even for imbalanced datasets.
Evaluations on real-world datasets show that EPIC achieves state-of-the-art
machine learning classification performance, significantly improving generation
efficiency. These findings highlight the effectiveness of EPIC for synthetic
tabular data generation, particularly in addressing class imbalance. Our source
code for our work is available at:
https://seharanul17.github.io/project-synthetic-tabular-llm/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">system</span>atic <span class="highlight-title">review</span> of the use of Deep Learning in Satellite Imagery for
  Agriculture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01272v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01272v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Victor, Zhen He, Aiden Nibali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agricultural research is essential for increasing food production to meet the
requirements of an increasing population in the coming decades. Recently,
satellite technology has been improving rapidly and deep learning has seen much
success in generic computer vision tasks and many application areas which
presents an important opportunity to improve analysis of agricultural land.
Here we present a systematic review of 150 studies to find the current uses of
deep learning on satellite imagery for agricultural research. Although we
identify 5 categories of agricultural monitoring tasks, the majority of the
research interest is in crop segmentation and yield prediction. We found that,
when used, modern deep learning methods consistently outperformed traditional
machine learning across most tasks; the only exception was that Long Short-Term
Memory (LSTM) Recurrent Neural Networks did not consistently outperform Random
Forests (RF) for yield prediction. The reviewed studies have largely adopted
methodologies from generic computer vision, except for one major omission:
benchmark datasets are not utilised to evaluate models across studies, making
it difficult to compare results. Additionally, some studies have specifically
utilised the extra spectral resolution available in satellite imagery, but
other divergent properties of satellite images - such as the hugely different
scales of spatial patterns - are not being taken advantage of in the reviewed
studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures and 10 tables in main paper. Final version, as
  submitted and accepted at JSTARS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Double Equivariance for Inductive Link Prediction for Both New Nodes and
  New Relation Types 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01313v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01313v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Zhou, Yucheng Zhang, Jianfei Gao, Yangze Zhou, Bruno Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of fully inductive link prediction in knowledge graphs has gained
significant attention, with various graph neural networks being proposed to
address it. This task presents greater challenges than traditional inductive
link prediction tasks with only new nodes, as models must be capable of
zero-shot generalization to both unseen nodes and unseen relation types in the
inference graph. Despite the development of novel models, a unifying
theoretical understanding of their success remains elusive, and the limitations
of these methods are not well-studied. In this work, we introduce the concept
of double permutation-equivariant representations and demonstrate its necessity
for effective performance in this task. We show that many existing models,
despite their diverse architectural designs, conform to this framework.
However, we also identify inherent limitations in double
permutation-equivariant representations, which restrict these models's ability
to learn effectively on datasets with varying characteristics. Our findings
suggest that while double equivariance is necessary for meta-learning across
knowledge graphs from different domains, it is not sufficient. There remains a
fundamental gap between double permutation-equivariant models and the concept
of foundation models designed to learn patterns across all domains.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-13T00:00:00Z">2025-01-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Outlier Connections Detection in Databases Network Traffic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Rodniansky, Tania Butovsky, Mikhail Shpak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The article describes a practical method for detecting outlier database
connections in real-time. Outlier connections are detected with a specified
level of confidence. The method is based on generalized security rules and a
simple but effective real-time machine learning mechanism. The described method
is non-intrusive to the database and does not depend on the type of database.
The method is used to proactively control access even before database
connection is established, minimize false positives, and maintain the required
response speed to detected database connection outliers. The capabilities of
the system are demonstrated with several examples of outliers in real-world
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the effects of logical database design on database size, query
  complexity, query performance, and energy consumption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toni Taipalus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Database normalization theory is the basis for logical design of relational
databases. Normalization reduces data redundancy and consequently eliminates
potential data anomalies, while increasing the computational cost of read
operations. Despite decades worth of applications of normalization theory, it
still remains largely unclear to what extent normalization affects database
size and efficiency. In this study, we study the effects of database
normalization using the Internet Movie Database (IMDb) public dataset and
PostgreSQL. The results indicate, rather intuitively, that (i) database size on
disk is reduced through normalization from 1NF to 2NF by 10%, but not from 2NF
to 4NF, (ii) the number of tables and table rows in total increase
monotonically from 1NF to 2NF to 4NF, and that (iii) query complexity increases
with further normalization. Surprisingly, however, the results also indicate
that (iv) normalization from 1NF to 2NF increases throughput by a factor of 4,
and consequently, (v) energy consumption per transaction reduces by 74% with
normalization from 1NF to 2NF. The results imply that the gains of
normalization from 2NF to 4NF in terms of throughput and energy consumption are
minimal, yet increase the storage space requirements by approximately 7%. While
these results represent merely one specific case, they provide needed empirical
evaluation on the practical effects and magnitude of database normalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An ontology-based description of nano computed tomography measurements
  in electronic laboratory notebooks: from metadata schema to first user
  experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Kirchner, D. C. Florian Wieland, Sarah Irvine, Sven Schimek, Jan Reimers, Rossella Aversa, Alexey Boubnov, Christian Lucas, Silja Flenner, Imke Greving, André Lopes Marinho, Tak Ming Wong, Regine Willumeit-Römer, Catriona Eschke, Berit Zeller-Plumhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the importance of well-documented metadata has been
discussed increasingly in many research fields. Making all metadata generated
during scientific research available in a findable, accessible, interoperable,
and reusable (FAIR) manner remains a significant challenge for researchers
across fields. Scientific communities are agreeing to achieve this by making
all data available in a semantically annotated knowledge graph using semantic
web technologies. Most current approaches do not gather metadata in a
consistent and community-agreed standardized way, and there are insufficient
tools to support the process of turning them into a knowledge graph. We present
an example solution in which the creation of a schema and ontology are placed
at the beginning of the scientific process which is then - using the electronic
laboratory notebook framework Herbie - turned into a bespoke data collection
platform to facilitate validation and semantic annotation of the metadata
immediately during an experiment. Using the example of synchrotron
radiation-based nano computed tomography measurements, we present a holistic
approach which can capture the complex metadata of such research instruments in
a flexible and straightforward manner. Different instrument setups of this
beamline can be considered, allowing a user-friendly experience. We show how
Herbie turns all semantic documents into an accessible user interface, where
all data entered automatically fulfills all requirements of being FAIR, and
present how data can be directly extracted via competency questions without
requiring familiarity with the fine-grained structure of the knowledge graph.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 13 figures, 5 tables. Fabian Kirchner and Florian Wieland
  have contributed equally to the manuscript. Corresponding authors:
  fabian.kirchner@hereon.de, catriona.eschke@hereon.de</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Multiple Temporal Network Kernel Density Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Shao, Peng Cheng, Xiang Lian, Lei Chen, Wangze Ni, Xuemin Lin, Chen Zhang, Liping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kernel density estimation (KDE) has become a popular method for visual
analysis in various fields, such as financial risk forecasting, crime
clustering, and traffic monitoring. KDE can identify high-density areas from
discrete datasets. However, most existing works only consider planar distance
and spatial data. In this paper, we introduce a new model, called TN-KDE, that
applies KDE-based techniques to road networks with temporal data. Specifically,
we introduce a novel solution, Range Forest Solution (RFS), which can
efficiently compute KDE values on spatiotemporal road networks. To support the
insertion operation, we present a dynamic version, called Dynamic Range Forest
Solution (DRFS). We also propose an optimization called Lixel Sharing (LS) to
share similar KDE values between two adjacent lixels. Furthermore, our
solutions support many non-polynomial kernel functions and still report exact
values. Experimental results show that our solutions achieve up to 6 times
faster than the state-of-the-art method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADKGD: Anomaly Detection in Knowledge Graphs with Dual-Channel Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayang Wu, Wensheng Gan, Jiahao Zhang, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current development of large language models (LLMs), it is important
to ensure the accuracy and reliability of the underlying data sources. LLMs are
critical for various applications, but they often suffer from hallucinations
and inaccuracies due to knowledge gaps in the training data. Knowledge graphs
(KGs), as a powerful structural tool, could serve as a vital external
information source to mitigate the aforementioned issues. By providing a
structured and comprehensive understanding of real-world data, KGs enhance the
performance and reliability of LLMs. However, it is common that errors exist in
KGs while extracting triplets from unstructured data to construct KGs. This
could lead to degraded performance in downstream tasks such as
question-answering and recommender systems. Therefore, anomaly detection in KGs
is essential to identify and correct these errors. This paper presents an
anomaly detection algorithm in knowledge graphs with dual-channel learning
(ADKGD). ADKGD leverages a dual-channel learning approach to enhance
representation learning from both the entity-view and triplet-view
perspectives. Furthermore, using a cross-layer approach, our framework
integrates internal information aggregation and context information
aggregation. We introduce a kullback-leibler (KL)-loss component to improve the
accuracy of the scoring function between the dual channels. To evaluate ADKGD's
performance, we conduct empirical studies on three real-world KGs: WN18RR,
FB15K, and NELL-995. Experimental results demonstrate that ADKGD outperforms
the state-of-the-art anomaly detection algorithms. The source code and datasets
are publicly available at https://github.com/csjywu1/ADKGD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. 11 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Historical Butterfly Counting in Large Temporal Bipartite
  Networks via Graph Structure-aware Index 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuyang Mang, Jingbang Chen, Hangrui Zhou, Yu Gao, Yingli Zhou, Qingyu Shi, Richard Peng, Yixiang Fang, Chenhao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bipartite graphs are ubiquitous in many domains, e.g., e-commerce platforms,
social networks, and academia, by modeling interactions between distinct entity
sets. Within these graphs, the butterfly motif, a complete 2*2 biclique,
represents the simplest yet significant subgraph structure, crucial for
analyzing complex network patterns. Counting the butterflies offers significant
benefits across various applications, including community analysis and
recommender systems. Additionally, the temporal dimension of bipartite graphs,
where edges activate within specific time frames, introduces the concept of
historical butterfly counting, i.e., counting butterflies within a given time
interval. This temporal analysis sheds light on the dynamics and evolution of
network interactions, offering new insights into their mechanisms. Despite its
importance, no existing algorithm can efficiently solve the historical
butterfly counting task. To address this, we design two novel indices whose
memory footprints are dependent on #butterflies and #wedges, respectively.
Combining these indices, we propose a graph structure-aware indexing approach
that significantly reduces memory usage while preserving exceptional query
speed. We theoretically prove that our approach is particularly advantageous on
power-law graphs, a common characteristic of real-world bipartite graphs, by
surpassing traditional complexity barriers for general graphs. Extensive
experiments reveal that our query algorithms outperform existing methods by up
to five magnitudes, effectively balancing speed with manageable memory
requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subspace Collision: An Efficient and Accurate Framework for
  High-dimensional Approximate Nearest Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14754v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14754v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuqi Wei, Xiaodong Lee, Zhenyu Liao, Themis Palpanas, Botao Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate Nearest Neighbor (ANN) search in high-dimensional Euclidean
spaces is a fundamental problem with a wide range of applications. However,
there is currently no ANN method that performs well in both indexing and query
answering performance, while providing rigorous theoretical guarantees for the
quality of the answers. In this paper, we first design SC-score, a metric that
we show follows the Pareto principle and can act as a proxy for the Euclidean
distance between data points. Inspired by this, we propose a novel ANN search
framework called Subspace Collision (SC), which can provide theoretical
guarantees on the quality of its results. We further propose SuCo, which
achieves efficient and accurate ANN search by designing a clustering-based
lightweight index and query strategies for our proposed subspace collision
framework. Extensive experiments on real-world datasets demonstrate that both
the indexing and query answering performance of SuCo outperform
state-of-the-art ANN methods that can provide theoretical guarantees,
performing 1-2 orders of magnitude faster query answering with only up to
one-tenth of the index memory footprint. Moreover, SuCo achieves top
performance (best for hard datasets) even when compared to methods that do not
provide theoretical guarantees. This paper was published in SIGMOD 2025.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Electrochemistry Platform with Real-Time Normality Testing of
  Voltammetry Measurements Using ML 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anees Al-Najjar, Nageswara S. V. Rao, Craig A. Bridges, Sheng Dai, Alex Walters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electrochemistry workflows utilize various instruments and computing systems
to execute workflows consisting of electrocatalyst synthesis, testing and
evaluation tasks. The heterogeneity of the software and hardware of these
ecosystems makes it challenging to orchestrate a complete workflow from
production to characterization by automating its tasks. We propose an
autonomous electrochemistry computing platform for a multi-site ecosystem that
provides the services for remote experiment steering, real-time measurement
transfer, and AI/ML-driven analytics. We describe the integration of a mobile
robot and synthesis workstation into the ecosystem by developing custom
hub-networks and software modules to support remote operations over the
ecosystem's wireless and wired networks. We describe a workflow task for
generating I-V voltammetry measurements using a potentiostat, and a machine
learning framework to ensure their normality by detecting abnormal conditions
such as disconnected electrodes. We study a number of machine learning methods
for the underlying detection problem, including smooth, non-smooth, structural
and statistical methods, and their fusers. We present experimental results to
illustrate the effectiveness of this platform, and also validate the proposed
ML method by deriving its rigorous generalization equations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 14 figures, accepted in the IEEE 20th International
  Conference on e-Science (e-Science), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Smells-sus: Sustainability Smells in IaC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seif Ashraf, Mohammad Hamdaqa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practitioners use Infrastructure as Code (IaC) scripts to efficiently
configure IT infrastructures through machine-readable definition files.
However, during the development of these scripts, some code patterns or
deployment choices may lead to sustainability issues like inefficient resource
utilization or redundant provisioning for example. We call this type of
patterns sustainability smells. These inefficiencies pose significant
environmental and financial challenges, given the growing scale of cloud
computing. This research focuses on Terraform, a widely adopted IaC tool. Our
study involves defining seven sustainability smells and validating them through
a survey with 19 IaC practitioners. We utilized a dataset of 28,327 Terraform
scripts from 395 open-source repositories. We performed a detailed qualitative
analysis of a randomly sampled 1,860 Terraform scripts from the original
dataset to identify code patterns that correspond to the sustainability smells
and used the other 26,467 Terraform scripts to study the prevalence of the
defined sustainability smells. Our results indicate varying prevalence rates of
these smells across the dataset. The most prevalent smell is Monolithic
Infrastructure, which appears in 9.67\% of the scripts. Additionally, our
findings highlight the complexity of conducting root cause analysis for
sustainability issues, as these smells often arise from a confluence of script
structures, configuration choices, and deployment contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication-Efficient, 2D Parallel Stochastic Gradient Descent for
  Distributed-Memory Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Devarakonda, Ramakrishnan Kannan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed-memory implementations of numerical optimization algorithm, such
as stochastic gradient descent (SGD), require interprocessor communication at
every iteration of the algorithm. On modern distributed-memory clusters where
communication is more expensive than computation, the scalability and
performance of these algorithms are limited by communication cost. This work
generalizes prior work on 1D $s$-step SGD and 1D Federated SGD with Averaging
(FedAvg) to yield a 2D parallel SGD method (HybridSGD) which attains a
continuous performance trade off between the two baseline algorithms. We
present theoretical analysis which show the convergence, computation,
communication, and memory trade offs between $s$-step SGD, FedAvg, 2D parallel
SGD, and other parallel SGD variants. We implement all algorithms in C++ and
MPI and evaluate their performance on a Cray EX supercomputing system. Our
empirical results show that HybridSGD achieves better convergence than FedAvg
at similar processor scales while attaining speedups of $5.3\times$ over
$s$-step SGD and speedups up to $121\times$ over FedAvg when used to solve
binary classification tasks using the convex, logistic regression model on
datasets obtained from the LIBSVM repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Big Atomics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Anderson, Guy E. Blelloch, Siddhartha Jayanti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we give theoretically and practically efficient
implementations of Big Atomics, i.e., $k$-word linearizable registers that
support the load, store, and compare-and-swap (CAS) operations. While modern
hardware supports $k = 1$ and sometimes $k = 2$ (e.g., double-width
compare-and-swap in x86), our implementations support arbitrary $k$. Big
Atomics are useful in many applications, including atomic manipulation of
tuples, version lists, and implementing load-linked/store-conditional (LL/SC).
We design fast, lock-free implementations of big atomics based on a novel
fast-path-slow-path approach we develop. We then use them to develop an
efficient concurrent hash table, as evidence of their utility.
  We experimentally validate the approach by comparing a variety of
implementations of big atomics under a variety of workloads (thread counts,
load/store ratios, contention, oversubscription, and number of atomics). The
experiments compare two of our lock-free variants with C++ std::atomic, a
lock-based version, a version using sequence locks, and an indirect version.
The results show that our approach is close to the fastest under all conditions
and far outperforms others under oversubscription. We also compare our big
atomics based concurrent hash table to a variety of other state-of-the-art hash
tables that support arbitrary length keys and values, including implementations
from Intel's TBB, Facebook's Folly, libcuckoo, and a recent release from Boost.
The results show that our approach of using big atomics in the design of hash
tables is a promising direction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Security-by-design: Securing a compromised <span class="highlight-title">system</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Awais Rashid, Sana Belguith, Matthew Bradbury, Sadie Creese, Ivan Flechais, Neeraj Suri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital infrastructures are seeing convergence and connectivity at
unprecedented scale. This is true for both current critical national
infrastructures and emerging future systems that are highly cyber-physical in
nature with complex intersections between humans and technologies, e.g., smart
cities, intelligent transportation, high-value manufacturing and Industry 4.0.
Diverse legacy and non-legacy software systems underpinned by heterogeneous
hardware compose on-the-fly to deliver services to millions of users with
varying requirements and unpredictable actions. This complexity is compounded
by intricate and complicated supply-chains with many digital assets and
services outsourced to third parties. The reality is that, at any particular
point in time, there will be untrusted, partially-trusted or compromised
elements across the infrastructure. Given this reality, and the societal scale
of digital infrastructures, delivering secure and resilient operations is a
major challenge. We argue that this requires us to move beyond the paradigm of
security-by-design and embrace the challenge of securing-a-compromised-system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Article for the Rossfest Symposium in memory of Ross Anderson,
  Cambridge, UK, 25 March 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KubeDSM: A Kubernetes-based Dynamic Scheduling and Migration Framework
  for Cloud-Assisted Edge Clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Pashaeehir, Sina Shariati, Shayan Shafaghi, Manni Moghimi, Mahmoud Momtazpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge computing has become critical for enabling latency-sensitive
applications, especially when paired with cloud resources to form
cloud-assisted edge clusters. However, efficient resource management remains
challenging due to edge nodes' limited capacity and unreliable connectivity.
This paper introduces KubeDSM, a Kubernetes-based dynamic scheduling and
migration framework tailored for cloud-assisted edge environments. KubeDSM
addresses the challenges of resource fragmentation, dynamic scheduling, and
live migration while ensuring Quality of Service (QoS) for latency-sensitive
applications. Unlike Kubernetes' default scheduler, KubeDSM adopts batch
scheduling to minimize resource fragmentation and incorporates a live migration
mechanism to optimize edge resource utilization. Specifically, KubeDSM
facilitates three key operations: intra-edge migration to reduce fragmentation,
edge-to-cloud migration during resource shortages, and cloud-to-edge migration
when resources become available, thereby increasing the number of pods
allocated to the edge. Our results demonstrate that KubeDSM consistently
achieves a higher average edge ratio and a lower standard deviation in edge
ratios, highlighting its ability to provide more effective and stable
scheduling across different deployments. We also explore the impact of
migration strategies and Quality of Service (QoS) configurations on the edge
ratios achieved by KubeDSM. The findings reveal that enabling migrations
significantly enhances the edge ratio by reducing fragmentation. Additionally,
KubeDSM's adaptability in respecting QoS requirements while maximizing overall
edge ratios is confirmed through different QoS scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Data Locality to Accelerate Sparse Matrix-Matrix
  Multiplication on CPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jordi Wolfson-Pou, Jan Laukemann, Fabrizio Petrini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation
in many applications. Current multithreaded implementations are based on
Gustavson's algorithm and often perform poorly on large matrices due to limited
cache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic
NUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To
generate locality, MAGNUS reorders the intermediate product into discrete
cache-friendly chunks using a two-level hierarchical approach. The accumulator
is applied to each chunk, where the chunk size is chosen such that the
accumulator is cache-efficient. MAGNUS is input- and system-aware: based on the
matrix characteristics and target system specifications, the optimal number of
chunks is computed by minimizing the storage cost of the necessary data
structures. MAGNUS allows for a hybrid accumulation strategy in which each
chunk uses a different accumulator based on an input threshold. We consider two
accumulators: an AVX-512 vectorized bitonic sorting algorithm and classical
dense accumulation. An OpenMP implementation of MAGNUS is compared with several
baselines for a variety of different matrices on three Intel x86 architectures.
For matrices from the SuiteSparse collection, MAGNUS is faster than all the
baselines in most cases and is orders of magnitude faster than Intel MKL for
several matrices. For massive random matrices that model social network graphs,
MAGNUS scales to the largest matrix sizes, while the baselines fail to do so.
Furthermore, MAGNUS is close to the optimal bound for these matrices,
regardless of the matrix size, structure, and density.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Lower Bound for Set Agreement in Dynamic Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Fraigniaud, Minh Hang Nguyen, Ami Paz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a positive integer $k$, $k$-set agreement is the distributed task in
which each process $i\in [n]$ in a group of $n$ processing nodes starts with an
input value $x_i$ in the set $\{0,\dots,k\}$, and must output a value $y_i$
such that (1) for every $i \in [n]$, $y_i$ is the input value of some process,
and (2)$|\{y_i : i\in [n]\}|\leq k$. That is, at most $k$ different values in
total must be outputted by the processes. The case $k=1$ correspond to (binary)
consensus, arguably the most studied problem in distributed computing. While
lower bounds for consensus have been obtained for most of the standard
distributed computing models, the design of lower bounds for $k$-set agreement
with $k>1$ is notoriously known to be much more difficult, and remains open for
many models. The main techniques for designing lower bounds for k-set agreement
with $k>1$ use tools from algebraic topology.
  The algebraic topology tools are difficult to manipulate, and require a lot
of care for avoiding mistakes. This difficulty increases when the
communications are mediated by a network of arbitrary structure. Recently, the
KNOWALL model has been specifically designed as a first attempt to understand
the LOCAL model through the lens of algebraic topology, and Casta\~neda et
al.(2021) have designed lower bounds for $k$-set agreement in the KNOWALL
model, with applications to dynamic networks.
  In this work, we re-prove the same lower bound for $k$-set agreement in the
KNOWALL model. This new proof stands out in its simplicity, which makes it
accessible to a broader audience, and increases confidence in the result.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The infrastructure powering IBM's Gen AI model development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Talia Gershon, Seetharami Seelam, Brian Belgodere, Milton Bonilla, Lan Hoang, Danny Barnett, I-Hsin Chung, Apoorve Mohan, Ming-Hung Chen, Lixiang Luo, Robert Walkup, Constantinos Evangelinos, Shweta Salaria, Marc Dombrowa, Yoonho Park, Apo Kayi, Liran Schour, Alim Alim, Ali Sydney, Pavlos Maniotis, Laurent Schares, Bernard Metzler, Bengi Karacali-Akyamac, Sophia Wen, Tatsuhiro Chiba, Sunyanan Choochotkaew, Takeshi Yoshimura, Claudia Misale, Tonia Elengikal, Kevin O Connor, Zhuoran Liu, Richard Molina, Lars Schneidenbach, James Caden, Christopher Laibinis, Carlos Fonseca, Vasily Tarasov, Swaminathan Sundararaman, Frank Schmuck, Scott Guthridge, Jeremy Cohn, Marc Eshel, Paul Muench, Runyu Liu, William Pointer, Drew Wyskida, Bob Krull, Ray Rose, Brent Wolfe, William Cornejo, John Walter, Colm Malone, Clifford Perucci, Frank Franco, Nigel Hinds, Bob Calio, Pavel Druyan, Robert Kilduff, John Kienle, Connor McStay, Andrew Figueroa, Matthew Connolly, Edie Fost, Gina Roma, Jake Fonseca, Ido Levy, Michele Payne, Ryan Schenkel, Amir Malki, Lion Schneider, Aniruddha Narkhede, Shekeba Moshref, Alexandra Kisin, Olga Dodin, Bill Rippon, Henry Wrieth, John Ganci, Johnny Colino, Donna Habeger-Rose, Rakesh Pandey, Aditya Gidh, Aditya Gaur, Dennis Patterson, Samsuddin Salmani, Rambilas Varma, Rumana Rumana, Shubham Sharma, Aditya Gaur, Mayank Mishra, Rameswar Panda, Aditya Prasad, Matt Stallone, Gaoyuan Zhang, Yikang Shen, David Cox, Ruchir Puri, Dakshi Agrawal, Drew Thorstensen, Joel Belog, Brent Tang, Saurabh Kumar Gupta, Amitabha Biswas, Anup Maheshwari, Eran Gampel, Jason Van Patten, Matthew Runion, Sai Kaki, Yigal Bogin, Brian Reitz, Steve Pritko, Shahan Najam, Surya Nambala, Radhika Chirra, Rick Welp, Frank DiMitri, Felipe Telles, Amilcar Arvelo, King Chu, Ed Seminaro, Andrew Schram, Felix Eickhoff, William Hanson, Eric Mckeever, Michael Light, Dinakaran Joseph, Piyush Chaudhary, Piyush Shivam, Puneet Chaudhary, Wesley Jones, Robert Guthrie, Chris Bostic, Rezaul Islam, Steve Duersch, Wayne Sawdon, John Lewars, Matthew Klos, Michael Spriggs, Bill McMillan, George Gao, Ashish Kamra, Gaurav Singh, Marc Curry, Tushar Katarki, Joe Talerico, Zenghui Shi, Sai Sindhur Malleni, Erwan Gallen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI Infrastructure plays a key role in the speed and cost-competitiveness of
developing and deploying advanced AI models. The current demand for powerful AI
infrastructure for model training is driven by the emergence of generative AI
and foundational models, where on occasion thousands of GPUs must cooperate on
a single training job for the model to be trained in a reasonable time.
Delivering efficient and high-performing AI training requires an end-to-end
solution that combines hardware, software and holistic telemetry to cater for
multiple types of AI workloads. In this report, we describe IBM's hybrid cloud
infrastructure that powers our generative AI model development. This
infrastructure includes (1) Vela: an AI-optimized supercomputing capability
directly integrated into the IBM Cloud, delivering scalable, dynamic,
multi-tenant and geographically distributed infrastructure for large-scale
model training and other AI workflow steps and (2) Blue Vela: a large-scale,
purpose-built, on-premises hosting environment that is optimized to support our
largest and most ambitious AI model training tasks. Vela provides IBM with the
dual benefit of high performance for internal use along with the flexibility to
adapt to an evolving commercial landscape. Blue Vela provides us with the
benefits of rapid development of our largest and most ambitious models, as well
as future-proofing against the evolving model landscape in the industry. Taken
together, they provide IBM with the ability to rapidly innovate in the
development of both AI models and commercial offerings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corresponding Authors: Talia Gershon, Seetharami Seelam,Brian
  Belgodere, Milton Bonilla</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Freeze-Tag Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19706v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19706v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharareh Alipour, Kajal Baghestani, Mahdis Mirzaei, Soroush Sahraei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the Freeze-Tag Problem (FTP), introduced by Arkin et al. (SODA'02),
where the objective is to activate a group of n robots, starting from a single
initially active robot. Robots are positioned in $\mathbb{R}^d$, and once
activated, they move at a constant speed to wake up others. The goal is to
minimize the time required to activate the last robot, known as the makespan.
We establish new upper bounds for the makespan under the $l_1$ and $l_2$ norms
in $\mathbb{R}^2$ and $\mathbb{R}^3$. Specifically, we improve the previous
upper bound for $(\mathbb{R}^2, l_2)$ from $7.07r$ (Bonichon et al., DISC'24)
to $5.064r$. For $(\mathbb{R}^3, l_1)$, we derive a makespan bound of $13r$,
which translates to $22.52r$ for $(\mathbb{R}^3, l_2)$. Here, $r$ denotes the
maximum distance of any robot from the initially active robot under the given
norm. To our knowledge, these are the first makespan bounds for FTP in
$\mathbb{R}^3$. Additionally, we show that the maximum makespan for $n$ robots
is not necessarily achieved when robots are equally distributed along the
boundary in $(\mathbb{R}^2, l_2)$. We further investigate FTP in
$(\mathbb{R}^3, l_2)$ for specific configurations where robots lie on a
boundary, providing insights into practical scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Large Foundation Models Design: A Perspective From Model and
  <span class="highlight-title">System</span> Co-Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01990v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01990v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Liu, Yanxuan Yu, Zhixin Lai, Yite Wang, Jing Wu, Zhongwei Wan, Sina Alinejad, Benjamin Lengerich, Ying Nian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on modern efficient training and inference technologies on
foundation models and illustrates them from two perspectives: model and system
design. Model and System Design optimize LLM training and inference from
different aspects to save computational resources, making LLMs more efficient,
affordable, and more accessible. The paper list repository is available at
\url{https://github.com/NoakLiu/Efficient-Foundation-Models-Survey}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Convergence of Continual Federated Learning Using Incrementally
  Aggregated Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satish Kumar Keshri, Nazreen Shah, Ranjitha Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The holy grail of machine learning is to enable Continual Federated Learning
(CFL) to enhance the efficiency, privacy, and scalability of AI systems while
learning from streaming data. The primary challenge of a CFL system is to
overcome global catastrophic forgetting, wherein the accuracy of the global
model trained on new tasks declines on the old tasks. In this work, we propose
Continual Federated Learning with Aggregated Gradients (C-FLAG), a novel
replay-memory based federated strategy consisting of edge-based gradient
updates on memory and aggregated gradients on the current data. We provide
convergence analysis of the C-FLAG approach which addresses forgetting and bias
while converging at a rate of $O(1/\sqrt{T})$ over $T$ communication rounds. We
formulate an optimization sub-problem that minimizes catastrophic forgetting,
translating CFL into an iterative algorithm with adaptive learning rates that
ensure seamless learning across tasks. We empirically show that C-FLAG
outperforms several state-of-the-art baselines on both task and
class-incremental settings with respect to metrics such as accuracy and
forgetting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Streetscape Application Services Stack (SASS): Towards a Distributed
  Sensing Architecture for Urban Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19714v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19714v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navid Salami Pargoo, Mahshid Ghasemi, Shuren Xia, Mehmet Kerem Turkcan, Taqiya Ehsan, Chengbo Zang, Yuan Sun, Javad Ghaderi, Gil Zussman, Zoran Kostic, Jorge Ortiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As urban populations grow, cities are becoming more complex, driving the
deployment of interconnected sensing systems to realize the vision of smart
cities. These systems aim to improve safety, mobility, and quality of life
through applications that integrate diverse sensors with real-time
decision-making. Streetscape applications-focusing on challenges like
pedestrian safety and adaptive traffic management-depend on managing
distributed, heterogeneous sensor data, aligning information across time and
space, and enabling real-time processing. These tasks are inherently complex
and often difficult to scale. The Streetscape Application Services Stack (SASS)
addresses these challenges with three core services: multimodal data
synchronization, spatiotemporal data fusion, and distributed edge computing. By
structuring these capabilities as clear, composable abstractions with clear
semantics, SASS allows developers to scale streetscape applications efficiently
while minimizing the complexity of multimodal integration.
  We evaluated SASS in two real-world testbed environments: a controlled
parking lot and an urban intersection in a major U.S. city. These testbeds
allowed us to test SASS under diverse conditions, demonstrating its practical
applicability. The Multimodal Data Synchronization service reduced temporal
misalignment errors by 88%, achieving synchronization accuracy within 50
milliseconds. Spatiotemporal Data Fusion service improved detection accuracy
for pedestrians and vehicles by over 10%, leveraging multicamera integration.
The Distributed Edge Computing service increased system throughput by more than
an order of magnitude. Together, these results show how SASS provides the
abstractions and performance needed to support real-time, scalable urban
applications, bridging the gap between sensing infrastructure and actionable
streetscape intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regular Model Checking Upside-Down: An Invariant-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.03060v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.03060v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Esparza, Michael Raskin, Christoph Welzel-Mohr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regular model checking is a technique for the verification of infinite-state
systems whose configurations can be represented as finite words over a suitable
alphabet. The form we are studying applies to systems whose set of initial
configurations is regular, and whose transition relation is captured by a
length-preserving transducer. To verify safety properties, regular model
checking iteratively computes automata recognizing increasingly larger regular
sets of reachable configurations, and checks if they contain unsafe
configurations. Since this procedure often does not terminate, acceleration,
abstraction, and widening techniques have been developed to compute a regular
superset of the reachable configurations.
  In this paper, we develop a complementary procedure. Instead of approaching
the set of reachable configurations from below, we start with the set of all
configurations and approach it from above. We use that the set of reachable
configurations is equal to the intersection of all inductive invariants of the
system. Since this intersection is non-regular in general, we introduce
b-invariants, defined as those representable by CNF-formulas with at most b
clauses. We prove that, for every $b\geq0$, the intersection of all inductive
b-invariants is regular, and we construct an automaton recognizing it. We show
that whether this automaton accepts some unsafe configuration is in EXPSPACE
for every $b\geq0$, and PSPACE-complete for b=1. Finally, we study how large
must b be to prove safety properties of a number of benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-12T00:00:00Z">2025-01-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a visually interpretable analysis of Two-Phase Locking
  membership 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Martinenghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two-phase locking (2PL) is a consolidated policy commonly adopted by Database
Management Systems to enforce serializability of a schedule. While the policy
is well understood, both in its standard and in the strict version,
automatically deriving a suitable tabular/graphical analysis of schedules with
respect to 2PL is far from trivial, and requires several technicalities that do
not straightforwardly translate to visual cues. In this paper, we delve into
the details of the development of a tool for 2PL analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Data Sketches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Zhang, Mohsen Heidari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in quantum technologies, particularly in quantum sensing
and simulation, have facilitated the generation and analysis of inherently
quantum data. This progress underscores the necessity for developing efficient
and scalable quantum data management strategies. This goal faces immense
challenges due to the exponential dimensionality of quantum data and its unique
quantum properties such as no-cloning and measurement stochasticity.
Specifically, classical storage and manipulation of an arbitrary n-qubit
quantum state requires exponential space and time. Hence, there is a critical
need to revisit foundational data management concepts and algorithms for
quantum data. In this paper, we propose succinct quantum data sketches to
support basic database operations such as search and selection. We view our
work as an initial step towards the development of quantum data management
model, opening up many possibilities for future research in this direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Optimizing Locality of Graph Transposition on Modern Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Koohi Esfahani, Hans Vandierendonck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the shared-memory Graph Transposition (GT) problem, a
fundamental graph algorithm that is widely used in graph analytics and
scientific computing.
  Previous GT algorithms have significant memory requirements that are
proportional to the number of vertices and threads which obstructs their use on
large graphs. Moreover, atomic memory operations have become comparably fast on
recent CPU architectures, which creates new opportunities for improving the
performance of concurrent atomic accesses in GT.
  We design PoTra, a GT algorithm which leverages graph structure and processor
and memory architecture to optimize locality and performance. PoTra limits the
size of additional data structures close to CPU cache sizes and utilizes the
skewed degree distribution of graph datasets to optimize locality and
performance. We present the performance model of PoTra to explain the
connection between cache and memory response times and graph locality.
  Our evaluation of PoTra on three CPU architectures and 20 real-world and
synthetic graph datasets with up to 128 billion edges demonstrates that PoTra
achieves up to 8.7 times speedup compared to previous works and if there is a
performance loss it remains limited to 15.7%, on average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoCoI: Distributed Coded Inference <span class="highlight-title">System</span> for Straggler Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Liu, Chao Huang, Ming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) are widely applied in real-time
applications on resource-constrained devices. To accelerate CNN inference,
prior works proposed to distribute the inference workload across multiple
devices. However, they did not address stragglers and device failures in
distributed inference, which is challenging due to the devices' time-varying
and possibly unknown computation/communication capacities. To address this, we
propose a distributed coded inference system, called CoCoI. It splits the
convolutional layers of CNN, considering the data dependency of
high-dimensional inputs and outputs, and then adapts coding schemes to generate
task redundancy. With CoCoI, the inference results can be determined once a
subset of devices complete their subtasks, improving robustness against
stragglers and failures. To theoretically analyze the tradeoff between
redundancy and subtask workload, we formulate an optimal splitting problem to
minimize the expected inference latency. Despite its non-convexity, we
determine an approximate strategy with minor errors, and prove that CoCoI
outperforms uncoded benchmarks. For performance evaluation, we build a testbed
with Raspberry Pi 4Bs. The experimental results show that the approximate
strategy closely matches the optimal solution. When compared with uncoded
benchmarks, CoCoI reduces inference latency by up to 34.2% in the presence of
stragglers and device failures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, and the last 3 are appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COMPASS: A Compiler Framework for Resource-Constrained Crossbar-Array
  Based In-Memory Deep Learning Accelerators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihoon Park, Jeongin Choe, Dohyun Kim, Jae-Joon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, crossbar array based in-memory accelerators have been gaining
interest due to their high throughput and energy efficiency. While software and
compiler support for the in-memory accelerators has also been introduced, they
are currently limited to the case where all weights are assumed to be on-chip.
This limitation becomes apparent with the significantly increasing network
sizes compared to the in-memory footprint.
  Weight replacement schemes are essential to address this issue. We propose
COMPASS, a compiler framework for resource-constrained crossbar-based
processing-in-memory (PIM) deep neural network (DNN) accelerators. COMPASS is
specially targeted for networks that exceed the capacity of PIM crossbar
arrays, necessitating access to external memories. We propose an algorithm to
determine the optimal partitioning that divides the layers so that each
partition can be accelerated on chip. Our scheme takes into account the data
dependence between layers, core utilization, and the number of write
instructions to minimize latency, memory accesses, and improve energy
efficiency. Simulation results demonstrate that COMPASS can accommodate much
more networks using a minimal memory footprint, while improving throughput by
1.78X and providing 1.28X savings in energy-delay product (EDP) over baseline
partitioning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted IEEE DATE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV
  Cache Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Qianli, Hong Zicong, Chen Fahao, Li Peng, Guo Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serving large language models (LLMs) for massive users is challenged by the
significant memory footprint of the transient state, known as the key-value
(KV) cache, which scales with sequence length and number of requests. Instead
of renting or buying more expensive GPUs, the load imbalance of the KV cache
across GPUs, coupled with recent advances in inter-GPU communication, provides
an opportunity to serve more requests via request migration. However, high
migration overhead and unpredictable request patterns make it challenging.
Therefore, this paper proposes MELL, a memory-efficient LLM serving system via
multi-GPU KV cache management. It saves the number of GPUs needed in the system
by considering the dynamic KV cache load and the costly request migration.
Specifically, we first develop an adaptive request migration mechanism to
balance the computational and communication overheads and adapt to diverse
resource conditions. Then, we design an online algorithm tailored to a
multi-LLM request and multi-GPU scheduling problem with migration enabled. It
aims to minimise the required GPUs while limiting the number of migrations.
Finally, we implement a prototype of MELL and demonstrate that it reduces the
number of GPUs by 31% and increases the GPU utilization by 43% at most compared
to existing LLM serving systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling
  Autonomous Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinfang Chen, Manish Shetty, Gagan Somashekar, Minghua Ma, Yogesh Simmhan, Jonathan Mace, Chetan Bansal, Rujia Wang, Saravan Rajmohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI for IT Operations (AIOps) aims to automate complex operational tasks, such
as fault localization and root cause analysis, to reduce human workload and
minimize customer impact. While traditional DevOps tools and AIOps algorithms
often focus on addressing isolated operational tasks, recent advances in Large
Language Models (LLMs) and AI agents are revolutionizing AIOps by enabling
end-to-end and multitask automation. This paper envisions a future where AI
agents autonomously manage operational tasks throughout the entire incident
lifecycle, leading to self-healing cloud systems, a paradigm we term AgentOps.
Realizing this vision requires a comprehensive framework to guide the design,
development, and evaluation of these agents. To this end, we present AIOPSLAB,
a framework that not only deploys microservice cloud environments, injects
faults, generates workloads, and exports telemetry data but also orchestrates
these components and provides interfaces for interacting with and evaluating
agents. We discuss the key requirements for such a holistic framework and
demonstrate how AIOPSLAB can facilitate the evaluation of next-generation AIOps
agents. Through evaluations of state-of-the-art LLM agents within the benchmark
created by AIOPSLAB, we provide insights into their capabilities and
limitations in handling complex operational tasks in cloud environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reciprocating Locks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dave Dice, Alex Kogan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present "Reciprocating Locks", a novel mutual exclusion locking algorithm,
targeting cache-coherent shared memory (CC), that enjoys a number of desirable
properties. The doorway arrival phase and the release operation both run in
constant-time. Waiting threads use local spinning and only a single waiting
element is required per thread, regardless of the number of locks a thread
might hold at a given time. While our lock does not provide strict FIFO
admission, it bounds bypass and has strong anti-starvation properties. The lock
is compact, space efficient, and has been intentionally designed to be readily
usable in real-world general purpose computing environments such as the linux
kernel, pthreads, or C++. We show the lock exhibits high throughput under
contention and low latency in the uncontended case. The performance of
Reciprocating Locks is competitive with and often better than the best
state-of-the-art scalable spin locks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scam Detection for Ethereum Smart Contracts: Leveraging Graph
  Representation Learning for Secure Blockchain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12370v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12370v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Jin, Ze Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the increasing abuse of fraudulent activities that result in
significant financial and reputational harm, Ethereum smart contracts face a
significant problem in detecting fraud. Existing monitoring methods typically
rely on lease code analysis or physically extracted features, which suffer from
scalability and adaptability limitations. In this study, we use graph
representation learning to observe purchase trends and find fraudulent deals.
We can achieve powerful categorisation performance by using innovative machine
learning versions and transforming Ethereum invoice data into graph structures.
Our method addresses label imbalance through SMOTE-ENN techniques and evaluates
models like Multi-Layer Perceptron ( MLP ) and Graph Convolutional Networks (
GCN). Experimental results show that the MLP type surpasses the GCN in this
environment, with domain-specific assessments closely aligned with real-world
assessments. This study provides a scalable and efficient way to improve
Ethereum's ecosystem's confidence and security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to BDICN 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-11T00:00:00Z">2025-01-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TWIX: Automatically Reconstructing Structured Data from Templatized
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Lin, Mawil Hasan, Rohan Kosalge, Alvin Cheung, Aditya G. Parameswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many documents, that we call templatized documents, are programmatically
generated by populating fields in a visual template. Effective data extraction
from these documents is crucial to supporting downstream analytical tasks.
Current data extraction tools often struggle with complex document layouts,
incur high latency and/or cost on large datasets, and often require significant
human effort, when extracting tables or values given user-specified fields from
documents. The key insight of our tool, TWIX, is to predict the underlying
template used to create such documents, modeling the visual and structural
commonalities across documents. Data extraction based on this predicted
template provides a more principled, accurate, and efficient solution at a low
cost. Comprehensive evaluations on 34 diverse real-world datasets show that
uncovering the template is crucial for data extraction from templatized
documents. TWIX achieves over 90% precision and recall on average,
outperforming tools from industry: Textract and Azure Document Intelligence,
and vision-based LLMs like GPT-4-Vision, by over 25% in precision and recall.
TWIX scales easily to large datasets and is 734X faster and 5836X cheaper than
vision-based LLMs for extracting data from a large document collection with 817
pages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aster: Enhancing LSM-structures for Scalable Graph Database <span class="chip">SIGMOD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingheng Mo, Junfeng Liu, Fan Wang, Siqiang Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a proliferation of applications requiring the management of
large-scale, evolving graphs under workloads with intensive graph updates and
lookups. Driven by this challenge, we introduce Poly-LSM, a high-performance
key-value storage engine for graphs with the following novel techniques: (1)
Poly-LSM is embedded with a new design of graph-oriented LSM-tree structure
that features a hybrid storage model for concisely and effectively storing
graph data. (2) Poly-LSM utilizes an adaptive mechanism to handle edge
insertions and deletions on graphs with optimized I/O efficiency. (3) Poly-LSM
exploits the skewness of graph data to encode the key-value entries. Building
upon this foundation, we further implement Aster, a robust and versatile graph
database that supports Gremlin query language facilitating various graph
applications. In our experiments, we compared Aster against several mainstream
real-world graph databases. The results demonstrate that Aster outperforms all
baseline graph databases, especially on large-scale graphs. Notably, on the
billion-scale Twitter graph dataset, Aster achieves up to 17x throughput
improvement compared to the best-performing baseline graph system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGMOD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Matrix Multiplication meets the Submodular Width 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06189v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06189v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Abo-Khamis, Xiao Hu, Dan Suciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One fundamental question in database theory is the following: Given a Boolean
conjunctive query Q, what is the best complexity for computing the answer to Q
in terms of the input database size N? When restricted to the class of
combinatorial algorithms, it is known that the best known complexity for any
query Q is captured by the submodular width of Q. However, beyond combinatorial
algorithms, certain queries are known to admit faster algorithms that often
involve a clever combination of fast matrix multiplication and data
partitioning. Nevertheless, there is no systematic way to derive and analyze
the complexity of such algorithms for arbitrary queries Q.
  In this work, we introduce a general framework that captures the best
complexity for answering any Boolean conjunctive query Q using matrix
multiplication. Our framework unifies both combinatorial and non-combinatorial
techniques under the umbrella of information theory. It generalizes the notion
of submodular width to a new stronger notion called the omega-submodular width
that naturally incorporates the power of fast matrix multiplication. We
describe a matching algorithm that computes the answer to any query Q in time
corresponding to the omega-submodular width of Q. We show that our framework
recovers the best known complexities for Boolean queries that have been studied
in the literature, to the best of our knowledge, and also discovers new
algorithms for some classes of queries that improve upon the best known
complexities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Marketplace: A Benchmark for Data Management in Microservices <span class="chip">SIGMOD'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12605v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12605v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Laigner, Zhexiang Zhang, Yijian Liu, Leonardo Freitas Gomes, Yongluan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microservice architectures have become a popular approach for designing
scalable distributed applications. Despite their extensive use in industrial
settings for over a decade, there is limited understanding of the data
management challenges that arise in these applications. Consequently, it has
been difficult to advance data system technologies that effectively support
microservice applications. To fill this gap, we present Online Marketplace, a
microservice benchmark that highlights core data management challenges that
existing benchmarks fail to address. These challenges include transaction
processing, query processing, event processing, constraint enforcement, and
data replication. We have defined criteria for various data management issues
to enable proper comparison across data systems and platforms.
  Through case studies with state-of-the-art data platforms, we discuss the
issues encountered while implementing and meeting Online Marketplace's
criteria. By capturing the overhead of meeting the key data management
requirements that are overlooked by existing benchmarks, we gain actionable
insights into the experimental platforms. This highlights the significance of
Online Marketplace in advancing future data systems to meet the needs of
microservice practitioners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version accepted at SIGMOD'25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeSplit: A Novel Defense Against Client-Side Backdoor Attacks in Split
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Rieger, Alessandro Pegoraro, Kavita Kumari, Tigist Abera, Jonathan Knauer, Ahmad-Reza Sadeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Split Learning (SL) is a distributed deep learning approach enabling multiple
clients and a server to collaboratively train and infer on a shared deep neural
network (DNN) without requiring clients to share their private local data. The
DNN is partitioned in SL, with most layers residing on the server and a few
initial layers and inputs on the client side. This configuration allows
resource-constrained clients to participate in training and inference. However,
the distributed architecture exposes SL to backdoor attacks, where malicious
clients can manipulate local datasets to alter the DNN's behavior. Existing
defenses from other distributed frameworks like Federated Learning are not
applicable, and there is a lack of effective backdoor defenses specifically
designed for SL.
  We present SafeSplit, the first defense against client-side backdoor attacks
in Split Learning (SL). SafeSplit enables the server to detect and filter out
malicious client behavior by employing circular backward analysis after a
client's training is completed, iteratively reverting to a trained checkpoint
where the model under examination is found to be benign. It uses a two-fold
analysis to identify client-induced changes and detect poisoned models. First,
a static analysis in the frequency domain measures the differences in the
layer's parameters at the server. Second, a dynamic analysis introduces a novel
rotational distance metric that assesses the orientation shifts of the server's
layer parameters during training. Our comprehensive evaluation across various
data distributions, client counts, and attack scenarios demonstrates the high
efficacy of this dual analysis in mitigating backdoor attacks while preserving
model utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at NDSS 2025; 18 pages, 6 Tables, and 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ladder-residual: parallelism-aware architecture for accelerating large
  model inference with communication overlapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muru Zhang, Mayank Mishra, Zhongzhu Zhou, William Brandon, Jue Wang, Yoon Kim, Jonathan Ragan-Kelley, Shuaiwen Leon Song, Ben Athiwaratkun, Tri Dao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model inference is both memory-intensive and time-consuming,
often requiring distributed algorithms to efficiently scale. Various model
parallelism strategies are used in multi-gpu training and inference to
partition computation across multiple devices, reducing memory load and
computation time. However, using model parallelism necessitates communication
of information between GPUs, which has been a major bottleneck and limits the
gains obtained by scaling up the number of devices. We introduce Ladder
Residual, a simple architectural modification applicable to all residual-based
models that enables straightforward overlapping that effectively hides the
latency of communication. Our insight is that in addition to systems
optimization, one can also redesign the model architecture to decouple
communication from computation. While Ladder Residual can allow
communication-computation decoupling in conventional parallelism patterns, we
focus on Tensor Parallelism in this paper, which is particularly bottlenecked
by its heavy communication. For a Transformer model with 70B parameters,
applying Ladder Residual to all its layers can achieve 30% end-to-end wall
clock speed up at inference time with TP sharding over 8 devices. We refer the
resulting Transformer model as the Ladder Transformer. We train a 1B and 3B
Ladder Transformer from scratch and observe comparable performance to a
standard dense transformer baseline. We also show that it is possible to
convert parts of the Llama-3.1 8B model to our Ladder Residual architecture
with minimal accuracy degradation by only retraining for 3B tokens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stingray: Fast Concurrent Transactions Without Consensus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srivatsan Sridhar, Alberto Sonnino, Lefteris Kokoris-Kogias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have improved the throughput and latency of blockchains by
processing transactions accessing different parts of the state concurrently.
However, these systems are unable to concurrently process (a) transactions
accessing the same state, even if they are (almost) commutative, e.g., payments
much smaller than an account's balance, and (b) multi-party transactions, e.g.,
asset swaps. Moreover, they are slow to recover from contention, requiring
once-in-a-day synchronization. We present Stingray, a novel blockchain
architecture that addresses these limitations. The key conceptual contributions
are a replicated bounded counter that processes (almost) commutative
transactions concurrently, and a FastUnlock protocol that uses a fallback
consensus protocol for fast contention recovery. We prove Stingray's security
in an asynchronous network with Byzantine faults and demonstrate on a global
testbed that Stingray achieves 10,000 times the throughput of prior systems for
commutative workloads.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semitopology: a topological approach to decentralised collaborative
  action 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09287v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09287v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murdoch Gabbay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce semitopology, a generalisation of point-set topology that
removes the restriction that intersections of open sets need necessarily be
open. The intuition is that points represent participants in a decentralised
system, and open sets represent collections of participants that collectively
have the authority to collaborate to update their local state; we call this an
actionable coalition.
  Examples of actionable coalition include: majority stakes in proof-of-stake
blockchains; communicating peers in peer-to-peer networks; and even pedestrians
working together to not bump into one another in the street. Where actionable
coalitions exist, they have in common that: collaborations are local (updating
the states of the participants in the coalition, but not immediately those of
the whole system); collaborations are voluntary (up to and including breaking
rules); participants may be heterogeneous in their computing power or in their
goals (not all pedestrians want to go to the same place); participants can
choose with whom to collaborate; and they are not assumed subject to permission
or synchronisation by a central authority.
  We develop a topology-flavoured mathematics that goes some way to explaining
how and why these complex decentralised systems can exhibit order, and gives us
new ways to understand existing practical implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See also arXiv:2310.00956, which takes a point-free algebraic
  approach ("semiframes"). This update updates metadata and content</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphSnapShot: Caching Local Structure for Fast Graph Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17918v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17918v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Liu, Roger Waleffe, Meng Jiang, Shivaram Venkataraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our recent research, we have developed a framework called GraphSnapShot,
which has been proven an useful tool for graph learning acceleration.
GraphSnapShot is a framework for fast cache, storage, retrieval and computation
for graph learning. It can quickly store and update the local topology of graph
structure and allows us to track patterns in the structure of graph networks,
just like take snapshots of the graphs. In experiments, GraphSnapShot shows
efficiency, it can achieve up to 30% training acceleration and 73% memory
reduction for lossless graph ML training compared to current baselines such as
dgl.This technique is particular useful for large dynamic graph learning tasks
such as social media analysis and recommendation systems to process complex
relationships between entities.
  The code for GraphSnapShot is publicly available at
https://github.com/NoakLiu/GraphSnapShot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiently Training 7B LLM with 1 Million Sequence Length on 8 GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pinxue Zhao, Hailin Zhang, Fangcheng Fu, Xiaonan Nie, Qibin Liu, Fang Yang, Yuanbo Peng, Dian Jiao, Shuaipeng Li, Jinbao Xue, Yangyu Tao, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, Large Language Models (LLMs) have been trained using extended
context lengths to foster more creative applications. However, long context
training poses great challenges considering the constraint of GPU memory. It
not only leads to substantial activation memory consumption during training,
but also incurs considerable memory fragmentation. To facilitate long context
training, existing frameworks have adopted strategies such as recomputation and
various forms of parallelisms. Nevertheless, these techniques rely on redundant
computation or extensive communication, resulting in low Model FLOPS
Utilization (MFU). In this paper, we propose MEMO, a novel LLM training
framework designed for fine-grained activation memory management. Given the
quadratic scaling of computation and linear scaling of memory with sequence
lengths when using FlashAttention, we offload memory-consuming activations to
CPU memory after each layer's forward pass and fetch them during the backward
pass. To maximize the swapping of activations without hindering computation,
and to avoid exhausting limited CPU memory, we implement a token-wise
activation recomputation and swapping mechanism. Furthermore, we tackle the
memory fragmentation issue by employing a bi-level Mixed Integer Programming
(MIP) approach, optimizing memory reuse across transformer layers. Empirical
results demonstrate that MEMO achieves an average of 1.97x and 1.80x MFU
compared to Megatron-LM and DeepSpeed, respectively. This improvement is
attributed to MEMO's ability to minimize memory fragmentation, reduce
recomputation and intensive communication, and circumvent the delays associated
with the memory reorganization process due to fragmentation. By leveraging
fine-grained activation memory management, MEMO facilitates efficient training
of 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU
of 52.30%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lightweight Federated Learning with Differential Privacy and Straggler
  Resilience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Hong, Xiaojun Lin, Lingjie Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enables collaborative model training through model
parameter exchanges instead of raw data. To avoid potential inference attacks
from exchanged parameters, differential privacy (DP) offers rigorous guarantee
against various attacks. However, conventional methods of ensuring DP by adding
local noise alone often result in low training accuracy. Combining secure
multi-party computation (SMPC) with DP, while improving the accuracy, incurs
high communication and computation overheads as well as straggler
vulnerability, in either client-to-server or client-to-client links. In this
paper, we propose LightDP-FL, a novel lightweight scheme that ensures provable
DP against untrusted peers and server, while maintaining straggler resilience,
low overheads and high training accuracy. Our scheme incorporates both
individual and pairwise noise into each client's parameter, which can be
implemented with minimal overheads. Given the uncertain straggler and colluder
sets, we utilize the upper bound on the numbers of stragglers and colluders to
prove sufficient noise variance conditions to ensure DP in the worst case.
Moreover, we optimize the expected convergence bound to ensure accuracy
performance by flexibly controlling the noise variances. Using the CIFAR-10
dataset, our experimental results demonstrate that LightDP-FL achieves faster
convergence and stronger straggler resilience compared to baseline methods of
the same DP level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at IEEE International Conference on Computer Communications
  (INFOCOM) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PraFFL: A Preference-Aware Scheme in Fair Federated Learning <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08973v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08973v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongguang Ye, Wei-Bin Kou, Ming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness in federated learning has emerged as a critical concern, aiming to
develop an unbiased model among groups (e.g., male or female) of diverse
sensitive features. However, there is a trade-off between model performance and
fairness, i.e., improving model fairness will decrease model performance.
Existing approaches have characterized such a trade-off by introducing
hyperparameters to quantify client's preferences for model fairness and model
performance. Nevertheless, these approaches are limited to scenarios where each
client has only a single pre-defined preference, and fail to work in practical
systems where each client generally has multiple preferences. To this end, we
propose a Preference-aware scheme in Fair Federated Learning (called PraFFL) to
generate preference-specific models in real time. PraFFL can adaptively adjust
the model based on each client's preferences to meet their needs. We
theoretically prove that PraFFL can offer the optimal model tailored to an
arbitrary preference of each client, and show its linear convergence.
Experimental results show that our proposed PraFFL outperforms six fair
federated learning algorithms in terms of the model's capability of adapting to
clients' different preferences. Our implementation is available at
https://github.com/rG223/PraFFL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-10T00:00:00Z">2025-01-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measures in SQL <span class="chip">SIGMOD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Hyde, John Fremlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SQL has attained widespread adoption, but Business Intelligence tools still
use their own higher level languages based upon a multidimensional paradigm.
Composable calculations are what is missing from SQL, and we propose a new kind
of column, called a measure, that attaches a calculation to a table. Like
regular tables, tables with measures are composable and closed when used in
queries. SQL-with-measures has the power, conciseness and reusability of
multidimensional languages but retains SQL semantics. Measure invocations can
be expanded in place to simple, clear SQL. To define the evaluation semantics
for measures, we introduce context-sensitive expressions (a way to evaluate
multidimensional expressions that is consistent with existing SQL semantics), a
concept called evaluation context, and several operations for setting and
modifying the evaluation context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in SIGMOD-Companion 24, June 9-15, 2024, Santiago,
  AA, Chile; 10 pages; updated with corrections as of 2024/05/31, and for
  formatting as of 2025/01/10</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AgileDART: An Agile and Scalable Edge Stream Processing Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-Wei Ching, Xin Chen, Chaeeun Kim, Tongze Wang, Dong Chen, Dilma Da Silva, Liting Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge applications generate a large influx of sensor data on massive scales,
and these massive data streams must be processed shortly to derive actionable
intelligence. However, traditional data processing systems are not well-suited
for these edge applications as they often do not scale well with a large number
of concurrent stream queries, do not support low-latency processing under
limited edge computing resources, and do not adapt to the level of
heterogeneity and dynamicity commonly present in edge computing environments.
As such, we present AgileDart, an agile and scalable edge stream processing
engine that enables fast stream processing of many concurrently running
low-latency edge applications' queries at scale in dynamic, heterogeneous edge
environments. The novelty of our work lies in a dynamic dataflow abstraction
that leverages distributed hash table-based peer-to-peer overlay networks to
autonomously place, chain, and scale stream operators to reduce query
latencies, adapt to workload variations, and recover from failures and a
bandit-based path planning model that re-plans the data shuffling paths to
adapt to unreliable and heterogeneous edge networks. We show that AgileDart
outperforms Storm and EdgeWise on query latency and significantly improves
scalability and adaptability when processing many real-world edge stream
applications' queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE Transactions on Mobile Computing (TMC); 18 pages
  for the main paper and 5 pages for the appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An experimental comparison of tree-data structures for connectivity
  queries on fully-dynamic undirected graphs (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Chen, Michael H. Böhlen, Sven Helmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During the past decades significant efforts have been made to propose data
structures for answering connectivity queries on fully dynamic graphs, i.e.,
graphs with frequent insertions and deletions of edges. However, a
comprehensive understanding of how these data structures perform in practice is
missing, since not all of them have been implemented, let alone evaluated
experimentally. We provide reference implementations for the proposed data
structures and experimentally evaluate them on a wide range of graphs. Our
findings show that the current solutions are not ready to be deployed in
systems as is, as every data structure has critical weaknesses when used in
practice. Key limitations that must be overcome are the space and time overhead
incurred by balanced data structures, the degeneration of the runtime of
space-efficient data structures in worst case scenarios, and the maintenance
costs for balanced data structures. We detail our findings in the experimental
evaluation and provide recommendations for implementing robust solutions for
answering connectivity queries on dynamic graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Partition Constraints for Conjunctive Queries: Bounds and Worst-Case
  Optimal Joins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Deeds, Timo Camillo Merkl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last decade, various works have used statistics on relations to
improve both the theory and practice of conjunctive query execution. Starting
with the AGM bound which took advantage of relation sizes, later works
incorporated statistics like functional dependencies and degree constraints.
Each new statistic prompted work along two lines; bounding the size of
conjunctive query outputs and worst-case optimal join algorithms. In this work,
we continue in this vein by introducing a new statistic called a
\emph{partition constraint}. This statistic captures latent structure within
relations by partitioning them into sub-relations which each have much tighter
degree constraints. We show that this approach can both refine existing
cardinality bounds and improve existing worst-case optimal join algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Batched DGEMMs for scientific codes running on long vector architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Banchelli, Marta Garcia-Gasulla, Filippo Mantovani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we evaluate the performance of SeisSol, a simulator of seismic
wave phenomena and earthquake dynamics, on a RISC-V-based system utilizing a
vector processing unit. We focus on GEMM libraries and address their limited
ability to leverage long vector architectures by developing a batched DGEMM
library in plain C. This library achieves speedups ranging from approximately
3.5x to 32.6x compared to the reference implementation. We then integrate the
batched approach into the SeisSol application, ensuring portability across
different CPU architectures. Lastly, we demonstrate that our implementation is
portable to an Intel CPU, resulting in improved execution times in most cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the First PPAM Workshop on RISC-V (PPAM24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Different Application Types across Heterogeneous Cloud
  Compute Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nivedhitha Duggi, Masoud Rafiei, Mohsen Amini Salehi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrastructure as a Service (IaaS) clouds have become the predominant
underlying infrastructure for the operation of modern and smart technology.
IaaS clouds have proven to be useful for multiple reasons such as reduced
costs, increased speed and efficiency, and better reliability and scalability.
Compute services offered by such clouds are heterogeneous -- they offer a set
of architecturally diverse machines that fit efficiently executing different
workloads. However, there has been little study to shed light on the
performance of popular application types on these heterogeneous compute servers
across different clouds. Such a study can help organizations to optimally (in
terms of cost, latency, throughput, consumed energy, carbon footprint, etc.)
employ cloud compute services. At HPCC lab, we have focused on such benchmarks
in different research projects and, in this report, we curate those benchmarks
in a single document to help other researchers in the community using them.
Specifically, we introduce our benchmarks datasets for three application types
in three different domains, namely: Deep Neural Networks (DNN) Inference for
industrial applications, Machine Learning (ML) Inference for assistive
technology applications, and video transcoding for multimedia use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. arXiv admin note: text overlap with
  arXiv:2011.11711 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scale-up Unlearnable Examples Learning with High-Performance Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanfan Zhu, Issac Lyngaas, Murali Gopalakrishnan Meena, Mary Ellen I. Koran, Bradley Malin, Daniel Moyer, Shunxing Bao, Anuj Kapadia, Xiao Wang, Bennett Landman, Yuankai Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in AI models are structured to retain user interactions,
which could inadvertently include sensitive healthcare data. In the healthcare
field, particularly when radiologists use AI-driven diagnostic tools hosted on
online platforms, there is a risk that medical imaging data may be repurposed
for future AI training without explicit consent, spotlighting critical privacy
and intellectual property concerns around healthcare data usage. Addressing
these privacy challenges, a novel approach known as Unlearnable Examples (UEs)
has been introduced, aiming to make data unlearnable to deep learning models. A
prominent method within this area, called Unlearnable Clustering (UC), has
shown improved UE performance with larger batch sizes but was previously
limited by computational resources. To push the boundaries of UE performance
with theoretically unlimited resources, we scaled up UC learning across various
datasets using Distributed Data Parallel (DDP) training on the Summit
supercomputer. Our goal was to examine UE efficacy at high-performance
computing (HPC) levels to prevent unauthorized learning and enhance data
security, particularly exploring the impact of batch size on UE's
unlearnability. Utilizing the robust computational capabilities of the Summit,
extensive experiments were conducted on diverse datasets such as Pets,
MedMNist, Flowers, and Flowers102. Our findings reveal that both overly large
and overly small batch sizes can lead to performance instability and affect
accuracy. However, the relationship between batch size and unlearnability
varied across datasets, highlighting the necessity for tailored batch size
strategies to achieve optimal data protection. Our results underscore the
critical role of selecting appropriate batch sizes based on the specific
characteristics of each dataset to prevent learning and ensure data security in
deep learning applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Optimal Fault Tolerance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lewis-Pye, Tim Roughgarden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The optimal fault-tolerance achievable by any protocol has been characterized
in a wide range of settings. For example, for state machine replication (SMR)
protocols operating in the partially synchronous setting, it is possible to
simultaneously guarantee consistency against $\alpha$-bounded adversaries
(i.e., adversaries that control less than an $\alpha$ fraction of the
participants) and liveness against $\beta$-bounded adversaries if and only if
$\alpha + 2\beta \leq 1$.
  This paper characterizes to what extent "better-than-optimal" fault-tolerance
guarantees are possible for SMR protocols when the standard consistency
requirement is relaxed to allow a bounded number $r$ of consistency violations.
We prove that bounding rollback is impossible without additional timing
assumptions and investigate protocols that tolerate and recover from
consistency violations whenever message delays around the time of an attack are
bounded by a parameter $\Delta^*$ (which may be arbitrarily larger than the
parameter $\Delta$ that bounds post-GST message delays in the partially
synchronous model). Here, a protocol's fault-tolerance can be a non-constant
function of $r$, and we prove, for each $r$, matching upper and lower bounds on
the optimal ``recoverable fault-tolerance'' achievable by any SMR protocol. For
example, for protocols that guarantee liveness against 1/3-bounded adversaries
in the partially synchronous setting, a 5/9-bounded adversary can always cause
one consistency violation but not two, and a 2/3-bounded adversary can always
cause two consistency violations but not three. Our positive results are
achieved through a generic ``recovery procedure'' that can be grafted on to any
accountable SMR protocol and restores consistency following a violation while
rolling back only transactions that were finalized in the previous $2\Delta^*$
timesteps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ML-Based Optimum Number of CUDA Streams for the GPU Implementation of
  the Tridiagonal Partition Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milena Veneva, Toshiyuki Imamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a heuristic for finding the optimum number of CUDA
streams by using tools common to the modern AI-oriented approaches and applied
to the parallel partition algorithm. A time complexity model for the GPU
realization of the partition method is built. Further, a refined time
complexity model for the partition algorithm being executed on multiple CUDA
streams is formulated. Computational experiments for different SLAE sizes are
conducted, and the optimum number of CUDA streams for each of them is found
empirically. Based on the collected data a model for the sum of the times for
the non-dominant GPU operations (that take part in the stream overlap) is
formulated using regression analysis. A fitting non-linear model for the
overhead time connected with the creation of CUDA streams is created.
Statistical analysis is done for all the built models. An algorithm for finding
the optimum number of CUDA streams is formulated. Using this algorithm,
together with the two models mentioned above, predictions for the optimum
number of CUDA streams are made. Comparing the predicted values with the actual
data, the algorithm is deemed to be acceptably good.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, 5 tables, MMCP conference 2024, Yerevan, Armenia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Encoded Spatial Attribute in Multi-Tier Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asfia Kawnine, Francis Palma, Seyed Alireza Rahimi Azghadi, Hung Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents an Encoded Spatial Multi-Tier Federated Learning
approach for a comprehensive evaluation of aggregated models for geospatial
data. In the client tier, encoding spatial information is introduced to better
predict the target outcome. The research aims to assess the performance of
these models across diverse datasets and spatial attributes, highlighting
variations in predictive accuracy. Using evaluation metrics such as accuracy,
our research reveals insights into the complexities of spatial granularity and
the challenges of capturing underlying patterns in the data. We extended the
scope of federated learning (FL) by having multi-tier along with the
functionality of encoding spatial attributes. Our N-tier FL approach used
encoded spatial data to aggregate in different tiers. We obtained multiple
models that predicted the different granularities of spatial data. Our findings
underscore the need for further research to improve predictive accuracy and
model generalization, with potential avenues including incorporating additional
features, refining model architectures, and exploring alternative modeling
approaches. Our experiments have several tiers representing different levels of
spatial aspects. We obtained accuracy of 75.62% and 89.52% for the global model
without having to train the model using the data constituted with the
designated tier. The research also highlights the importance of the proposed
approach in real-time applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE ICCE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STHFL: Spatio-Temporal Heterogeneous Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunxin Guo, Hongsong Wang, Shuxia Lin, Xu Yang, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a new framework that protects data privacy and allows
multiple devices to cooperate in training machine learning models. Previous
studies have proposed multiple approaches to eliminate the challenges posed by
non-iid data and inter-domain heterogeneity issues. However, they ignore the
\textbf{spatio-temporal} heterogeneity formed by different data distributions
of increasing task data in the intra-domain. Moreover, the global data is
generally a long-tailed distribution rather than assuming the global data is
balanced in practical applications. To tackle the \textbf{spatio-temporal}
dilemma, we propose a novel setting named \textbf{Spatio-Temporal
Heterogeneity} Federated Learning (STHFL). Specially, the Global-Local Dynamic
Prototype (GLDP) framework is designed for STHFL. In GLDP, the model in each
client contains personalized layers which can dynamically adapt to different
data distributions. For long-tailed data distribution, global prototypes are
served as complementary knowledge for the training on classes with few samples
in clients without leaking privacy. As tasks increase in clients, the knowledge
of local prototypes generated in previous tasks guides for training in the
current task to solve catastrophic forgetting. Meanwhile, the global-local
prototypes are updated through the moving average method after training local
prototypes in clients. Finally, we evaluate the effectiveness of GLDP, which
achieves remarkable results compared to state-of-the-art methods in STHFL
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Practical Cross-Layer Approach for ML-Driven Storage Placement in
  Warehouse-Scale Computers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Yang, Yan Li, Martin Maas, Mustafa Uysal, Ubaid Ullah Hafeez, Arif Merchant, Richard McDougall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Storage systems account for a major portion of the total cost of ownership
(TCO) of warehouse-scale computers, and thus have a major impact on the overall
system's efficiency. Machine learning (ML)-based methods for solving key
problems in storage system efficiency, such as data placement, have shown
significant promise. However, there are few known practical deployments of such
methods. Studying this problem in the context of real-world hyperscale data
center deployments at Google, we identify a number of challenges that we
believe cause this lack of practical adoption. Specifically, prior work assumes
a monolithic model that resides entirely within the storage layer, an
unrealistic assumption in real-world data center deployments. We propose a
cross-layer approach that moves ML out of the storage system and performs it in
the application running on top of it, co-designed with a scheduling algorithm
at the storage layer that consumes predictions from these application-level
models. This approach combines small, interpretable models with a co-designed
heuristic that adapts to different online environments. We build a
proof-of-concept of this approach in a production distributed computation
framework at Google. Evaluations in a test deployment and large-scale
simulation studies using production traces show improvements of as much as
3.47x in TCO savings compared to state of the art baselines. We believe this
work represents a significant step towards more practical ML-driven storage
placement in warehouse-scale computers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaboration of Large Language Models and Small Recommendation Models
  for Device-Cloud Recommendation <span class="chip">KDD'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheqi Lv, Tianyu Zhan, Wenjie Wang, Xinyu Lin, Shengyu Zhang, Wenqiao Zhang, Jiwei Li, Kun Kuang, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising
research direction that has demonstrated exceptional performance in this field.
However, its inability to capture real-time user preferences greatly limits the
practical application of LLM4Rec because (i) LLMs are costly to train and infer
frequently, and (ii) LLMs struggle to access real-time data (its large number
of parameters poses an obstacle to deployment on devices). Fortunately, small
recommendation models (SRMs) can effectively supplement these shortcomings of
LLM4Rec diagrams by consuming minimal resources for frequent training and
inference, and by conveniently accessing real-time data on devices.
  In light of this, we designed the Device-Cloud LLM-SRM Collaborative
Recommendation Framework (LSC4Rec) under a device-cloud collaboration setting.
LSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the
benefits of cloud and edge computing, achieving a complementary synergy. We
enhance the practicability of LSC4Rec by designing three strategies:
collaborative training, collaborative inference, and intelligent request.
During training, LLM generates candidate lists to enhance the ranking ability
of SRM in collaborative scenarios and enables SRM to update adaptively to
capture real-time user interests. During inference, LLM and SRM are deployed on
the cloud and on the device, respectively. LLM generates candidate lists and
initial ranking results based on user behavior, and SRM get reranking results
based on the candidate list, with final results integrating both LLM's and
SRM's scores. The device determines whether a new candidate list is needed by
comparing the consistency of the LLM's and SRM's sorted lists. Our
comprehensive and extensive experimental analysis validates the effectiveness
of each strategy in LSC4Rec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on KDD'25: Proceedings of the ACM SIGKDD Conference on
  Knowledge Discovery and Data Mining 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Over-the-Air Model Updating for Wireless Online Federated
  Learning with Delayed Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juncheng Wang, Yituo Liu, Ben Liang, Min Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study online federated learning over a wireless network, where the central
server updates an online global model sequence to minimize the time-varying
loss of multiple local devices over time. The server updates the global model
through over-the-air model-difference aggregation from the local devices over a
noisy multiple-access fading channel. We consider the practical scenario where
information on both the local loss functions and the channel states is delayed,
and each local device is under a time-varying power constraint. We propose
Constrained Over-the-air Model Updating with Delayed infOrmation (COMUDO),
where a new lower-and-upper-bounded virtual queue is introduced to counter the
delayed information and control the hard constraint violation. We show that its
local model updates can be efficiently computed in closed-form expressions.
Furthermore, through a new Lyapunov drift analysis, we show that COMUDO
provides bounds on the dynamic regret, static regret, and hard constraint
violation. Simulation results on image classification tasks under practical
wireless network settings show substantial accuracy gain of COMUDO over
state-of-the-art approaches, especially in the low-power region.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in INFOCOM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AgileDART: An Agile and Scalable Edge Stream Processing Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-Wei Ching, Xin Chen, Chaeeun Kim, Tongze Wang, Dong Chen, Dilma Da Silva, Liting Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge applications generate a large influx of sensor data on massive scales,
and these massive data streams must be processed shortly to derive actionable
intelligence. However, traditional data processing systems are not well-suited
for these edge applications as they often do not scale well with a large number
of concurrent stream queries, do not support low-latency processing under
limited edge computing resources, and do not adapt to the level of
heterogeneity and dynamicity commonly present in edge computing environments.
As such, we present AgileDart, an agile and scalable edge stream processing
engine that enables fast stream processing of many concurrently running
low-latency edge applications' queries at scale in dynamic, heterogeneous edge
environments. The novelty of our work lies in a dynamic dataflow abstraction
that leverages distributed hash table-based peer-to-peer overlay networks to
autonomously place, chain, and scale stream operators to reduce query
latencies, adapt to workload variations, and recover from failures and a
bandit-based path planning model that re-plans the data shuffling paths to
adapt to unreliable and heterogeneous edge networks. We show that AgileDart
outperforms Storm and EdgeWise on query latency and significantly improves
scalability and adaptability when processing many real-world edge stream
applications' queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE Transactions on Mobile Computing (TMC); 18 pages
  for the main paper and 5 pages for the appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decentralized Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David McAllister, Matthew Tancik, Jiaming Song, Angjoo Kanazawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale AI model training divides work across thousands of GPUs, then
synchronizes gradients across them at each step. This incurs a significant
network burden that only centralized, monolithic clusters can support, driving
up infrastructure costs and straining power systems. We propose Decentralized
Diffusion Models, a scalable framework for distributing diffusion model
training across independent clusters or datacenters by eliminating the
dependence on a centralized, high-bandwidth networking fabric. Our method
trains a set of expert diffusion models over partitions of the dataset, each in
full isolation from one another. At inference time, the experts ensemble
through a lightweight router. We show that the ensemble collectively optimizes
the same objective as a single model trained over the whole dataset. This means
we can divide the training burden among a number of "compute islands," lowering
infrastructure costs and improving resilience to localized GPU failures.
Decentralized diffusion models empower researchers to take advantage of
smaller, more cost-effective and more readily available compute like on-demand
GPU nodes rather than central integrated systems. We conduct extensive
experiments on ImageNet and LAION Aesthetics, showing that decentralized
diffusion models FLOP-for-FLOP outperform standard diffusion models. We finally
scale our approach to 24 billion parameters, demonstrating that high-quality
diffusion models can now be trained with just eight individual GPU nodes in
less than a week.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://decentralizeddiffusion.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Minimizing speculation overhead in a parallel recognizer for regular
  texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14975v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14975v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelo Borsotti, Luca Breveglieri, Stefano Crespi Reghizzi, Angelo Morzenti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative data-parallel algorithms for language recognition have been
widely experimented for various types of finite-state automata (FA),
deterministic (DFA) and nondeterministic (NFA), often derived from regular
expressions (RE). Such an algorithm cuts the input string into chunks,
independently recognizes each chunk in parallel by means of identical FAs, and
at last joins the chunk results and checks overall consistency. In chunk
recognition, it is necessary to speculatively start the FAs in any state, thus
causing an overhead that reduces the speedup compared to a serial algorithm.
Existing data-parallel DFA-based recognizers suffer from the excessive number
of starting states, and the NFA-based ones suffer from the number of
nondeterministic transitions. Our data-parallel algorithm is based on the new
FA type called reduced interface DFA (RI-DFA), which minimizes the speculation
overhead without incurring in the penalty of nondeterministic transitions or of
impractically enlarged DFA machines. The algorithm is proved to be correct and
theoretically efficient, because it combines the state-reduction of an NFA with
the speed of deterministic transitions, thus improving on both DFA-based and
NFA-based existing implementations. The practical applicability of the RI-DFA
approach is confirmed by a quantitative comparison of the number of starting
states for a large public benchmark of complex FAs. On multi-core computing
architectures, the RI-DFA recognizer is much faster than the NFA-based one on
all benchmarks, while it matches the DFA-based one on some benchmarks and
performs much better on some others. The extra time cost needed to construct an
RI-DFA compared to a DFA is moderate and is compatible with a practical use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Nearly Linear-Time Distributed Algorithm for Maximum Cardinality
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04140v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04140v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taisuke Izumi, Naoki Kitamura, Yutaro Yamaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a randomized $\tilde{O}(\mu(G))$-round algorithm
for the maximum cardinality matching problem in the CONGEST model, where
$\mu(G)$ means the maximum size of a matching of the input graph $G$. The
proposed algorithm substantially improves the current best worst-case running
time. The key technical ingredient is a new randomized algorithm of finding an
augmenting path of length $\ell$ with high probability within $\tilde{O}(\ell)$
rounds, which positively settles an open problem left in the prior work by
Ahmadi and Kuhn [DISC'20].
  The idea of our augmenting path algorithm is based on a recent result by
Kitamura and Izumi [IEICE Trans.'22], which efficiently identifies a sparse
substructure of the input graph containing an augmenting path, following a new
concept called \emph{alternating base trees}. Their algorithm, however, resorts
in part to a centralized approach of collecting the entire information of the
substructure into a single vertex for constructing a long augmenting path. The
technical highlight of this paper is to provide a fully-decentralized
counterpart of such a centralized method. To develop the algorithm, we prove
several new structural properties of alternating base trees, which are of
independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-09T00:00:00Z">2025-01-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaussDB-Global: A Geographically Distributed Database <span class="highlight-title">System</span> <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puya Memarzia, Huaxin Zhang, Kelvin Ho, Ronen Grosman, Jiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geographically distributed database systems use remote replication to protect
against regional failures. These systems are sensitive to severe latency
penalties caused by centralized transaction management, remote access to
sharded data, and log shipping over long distances. To tackle these issues, we
present GaussDB-Global, a sharded geographically distributed database system
with asynchronous replication, for OLTP applications. To tackle the transaction
management bottleneck, we take a decentralized approach using synchronized
clocks. Our system can seamlessly transition between centralized and
decentralized transaction management, providing efficient fault tolerance and
streamlining deployment. To alleviate the remote read and log shipping issues,
we support reads on asynchronous replicas with strong consistency, tunable
freshness guarantees, and dynamic load balancing. Our experimental results on a
geographically distributed cluster show that our approach provides up to 14x
higher read throughput, and 50% more TPC-C throughput compared to our baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, published in ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference Queries over Taxonomic Domains <span class="chip">VLDB 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paolo Ciaccia, Davide Martinenghi, Riccardo Torlone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When composing multiple preferences characterizing the most suitable results
for a user, several issues may arise. Indeed, preferences can be partially
contradictory, suffer from a mismatch with the level of detail of the actual
data, and even lack natural properties such as transitivity. In this paper we
formally investigate the problem of retrieving the best results complying with
multiple preferences expressed in a logic-based language. Data are stored in
relational tables with taxonomic domains, which allow the specification of
preferences also over values that are more generic than those in the database.
In this framework, we introduce two operators that rewrite preferences for
enforcing the important properties of transitivity, which guarantees soundness
of the result, and specificity, which solves all conflicts among preferences.
Although, as we show, these two properties cannot be fully achieved together,
we use our operators to identify the only two alternatives that ensure
transitivity and minimize the residual conflicts. Building on this finding, we
devise a technique, based on an original heuristics, for selecting the best
results according to the two possible alternatives. We finally show, with a
number of experiments over both synthetic and real-world datasets, the
effectiveness and practical feasibility of the overall approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 11 figures, this is an extended version of a paper
  published in PVLDB 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHASE: A Native Relational Database for Hybrid Queries on Structured and
  Unstructured Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Ma, Kai Zhang, Zhenying He, Yinan Jing, X. Sean Wang, Zhenqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Querying both structured and unstructured data has become a new paradigm in
data analytics and recommendation. With unstructured data, such as text and
videos, are converted to high-dimensional vectors and queried with approximate
nearest neighbor search (ANNS). State-of-the-art database systems implement
vector search as a plugin in the relational query engine, which tries to
utilize the ANN index to enhance performance. After investigating a broad range
of hybrid queries, we find that such designs may miss potential optimization
opportunities and achieve suboptimal performance for certain queries. In this
paper, we propose CHASE, a query engine that is natively designed to support
efficient hybrid queries on structured and unstructured data. CHASE performs
specific designs and optimizations on multiple stages in query processing.
First, semantic analysis is performed to categorize queries and optimize query
plans dynamically. Second, new physical operators are implemented to avoid
redundant computations, which is the case with existing operators. Third,
compilation-based techniques are adopted for efficient machine code generation.
Extensive evaluations using real-world datasets demonstrate that CHASE achieves
substantial performance improvements, with speedups ranging from 13% to an
extraordinary 7500 times compared to existing systems. These results highlight
CHASE's potential as a robust solution for executing hybrid queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ThriftLLM: On Cost-Effective Selection of Large Language Models for
  Classification Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keke Huang, Yimin Shi, Dujian Ding, Yifei Li, Yang Fei, Laks Lakshmanan, Xiaokui Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities in
comprehending and generating natural language content, attracting widespread
popularity in both industry and academia in recent years. An increasing number
of services have sprung up which offer LLMs for various tasks via APIs.
Different LLMs demonstrate expertise in different domains of queries (e.g.,
text classification queries). Meanwhile, LLMs of different scales, complexity,
and performance are priced diversely. Driven by this observation, a growing
number of researchers are investigating the LLM ensemble strategy with a focus
on cost-effectiveness, aiming to decrease overall usage costs while enhancing
performance. However, to the best of our knowledge, none of the existing works
addresses the problem, i.e., how to find an LLM ensemble subject to a cost
budget, which maximizes the ensemble performance.
  In this paper, we formalize the performance of an ensemble of models (LLMs)
using the notion of prediction accuracy which we formally define. We develop an
approach for aggregating responses from multiple LLMs to enhance ensemble
performance. Building on this, we formulate the ensemble selection problem as
that of selecting a set of LLMs subject to a cost budget such that the overall
prediction accuracy is maximized. We theoretically establish the non-decreasing
and non-submodular properties of the prediction accuracy function and provide
evidence that the Optimal Ensemble Selection problem is likely to be NP-hard.
Subsequently, we apply dynamic programming and propose an algorithm called
ThriftLLM. We prove that ThriftLLM achieves a near-optimal approximation
guarantee. In addition, it achieves state-of-the-art query performance on
multiple real-world datasets against 3 competitors in our extensive
experimental evaluation, strongly supporting the effectiveness and superiority
of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Overlapping Community Search via Subspace Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14692v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14692v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Sima, Jianke Yu, Xiaoyang Wang, Wenjie Zhang, Ying Zhang, Xuemin Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overlapping Community Search (OCS) identifies nodes that interact with
multiple communities based on a specified query. Existing community search
approaches fall into two categories: algorithm-based models and Machine
Learning-based (ML) models. Despite the long-standing focus on this topic
within the database domain, current solutions face two major limitations: 1)
Both approaches fail to address personalized user requirements in OCS,
consistently returning the same set of nodes for a given query regardless of
user differences. 2) Existing ML-based CS models suffer from severe training
efficiency issues. In this paper, we formally redefine the problem of OCS. By
analyzing the gaps in both types of approaches, we then propose a general
solution for OCS named Sparse Subspace Filter (SSF), which can extend any
ML-based CS model to enable personalized search in overlapping structures. To
overcome the efficiency issue in the current models, we introduce Simplified
Multi-hop Attention Networks (SMN), a lightweight yet effective community
search model with larger receptive fields. To the best of our knowledge, this
is the first ML-based study of overlapping community search. Extensive
experiments validate the superior performance of SMN within the SSF pipeline,
achieving a 13.73% improvement in F1-Score and up to 3 orders of magnitude
acceleration in model efficiency compared to state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEA-SQL: Semantic-Enhanced Text-to-SQL with Adaptive Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaofan Li, Yingxia Shao, Yawen Li, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have significantly
contributed to the progress of the Text-to-SQL task. A common requirement in
many of these works is the post-correction of SQL queries. However, the
majority of this process entails analyzing error cases to develop prompts with
rules that eliminate model bias. And there is an absence of execution
verification for SQL queries. In addition, the prevalent techniques primarily
depend on GPT-4 and few-shot prompts, resulting in expensive costs. To
investigate the effective methods for SQL refinement in a cost-efficient
manner, we introduce Semantic-Enhanced Text-to-SQL with Adaptive Refinement
(SEA-SQL), which includes Adaptive Bias Elimination and Dynamic Execution
Adjustment, aims to improve performance while minimizing resource expenditure
with zero-shot prompts. Specifically, SEA-SQL employs a semantic-enhanced
schema to augment database information and optimize SQL queries. During the SQL
query generation, a fine-tuned adaptive bias eliminator is applied to mitigate
inherent biases caused by the LLM. The dynamic execution adjustment is utilized
to guarantee the executability of the bias eliminated SQL query. We conduct
experiments on the Spider and BIRD datasets to demonstrate the effectiveness of
this framework. The results demonstrate that SEA-SQL achieves state-of-the-art
performance in the GPT3.5 scenario with 9%-58% of the generation cost.
Furthermore, SEA-SQL is comparable to GPT-4 with only 0.9%-5.3% of the
generation cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article has been accepted by Frontiers of Computer Science (FCS),
  with the DOI: {10.1007/s11704-025-41136-3}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Oblivious Algorithms for Multi-way Joins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Hu, Zhiang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In cloud databases, cloud computation over sensitive data uploaded by clients
inevitably causes concern about data security and privacy. Even when encryption
primitives and trusted computing environments are integrated into query
processing to safeguard the actual contents of the data, access patterns of
algorithms can still leak private information about the data. Oblivious Random
Access Memory (ORAM) and circuits are two generic approaches to address this
issue, ensuring that access patterns of algorithms remain oblivious to the
data. However, deploying these methods on insecure algorithms, particularly for
multi-way join processing, is computationally expensive and inherently
challenging.
  In this paper, we propose a novel sorting-based algorithm for multi-way join
processing that operates without relying on ORAM simulations or other security
assumptions. Our algorithm is a non-trivial, provably oblivious composition of
basic primitives, with time complexity matching the insecure worst-case optimal
join algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,
with cache complexity matching the insecure lower bound, also up to a
logarithmic factor. This clean and straightforward approach has the potential
to be extended to other security settings and implemented in practical database
systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Popcorn: Accelerating Kernel K-means on GPUs through Sparse Linear
  Algebra 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Bellavita, Thomas Pasquali, Laura Del Rio Martin, Flavio Vella, Giulia Guidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  K-means is a popular clustering algorithm with significant applications in
numerous scientific and engineering areas. One drawback of K-means is its
inability to identify non-linearly separable clusters, which may lead to
inaccurate solutions in certain cases. Kernel K-means is a variant of classical
K-means that can find non-linearly separable clusters. However, it scales
quadratically with respect to the size of the dataset, taking several minutes
to cluster even medium-sized datasets on traditional CPU-based machines. In
this paper, we present a formulation of Kernel K-means using sparse-dense
matrix multiplication (SpMM) and sparse matrix-vector multiplication (SpMV),
and we show that our formulation enables the rapid implementation of a fast
GPU-based version of Kernel K-means with little programming effort. Our
implementation, named Popcorn, is the first open-source GPU-based
implementation of Kernel K-means. Popcorn achieves a speedup of up to 123.8x
over a CPU implementation of Kernel K-means and a speedup of up to 2.6x over a
GPU implementation of Kernel K-means that does not use sparse matrix
computations. Our results support the effectiveness of sparse matrices as tools
for efficient parallel programming.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction-Assisted Online Distributed Deep Learning Workload Scheduling
  in GPU Clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Luo, Jia Liu, Myungjin Lee, Ness B. Shroff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent explosive growth of deep learning (DL) models has necessitated a
compelling need for efficient job scheduling for distributed deep learning
training with mixed parallelisms (DDLwMP) in GPU clusters. This paper proposes
an adaptive shortest-remaining-processing-time-first (A-SRPT) scheduling
algorithm, a novel prediction-assisted online scheduling approach designed to
mitigate the challenges associated with DL cluster scheduling. By modeling each
job as a graph corresponding to heterogeneous Deep Neural Network (DNN) models
and their associated distributed training configurations, A-SRPT strategically
assigns jobs to the available GPUs, thereby minimizing inter-server
communication overhead. Observing that most DDLwMP jobs recur, A-SRPT
incorporates a random forest regression model to predict training iterations.
Crucially, A-SRPT maps the complex scheduling problem into a single-machine
instance, which is addressed optimally by a preemptive
"shortest-remaining-processing-time-first" strategy. This optimized solution
serves as a guide for actual job scheduling within the GPU clusters, leading to
a theoretically provable competitive scheduling efficiency. We conduct
extensive real-world testbed and simulation experiments to verify our proposed
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INFOCOM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Fair Ordering and Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shir Cohen, Neel Basu, Soumya Basu, Lorenzo Alvisi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In blockchain systems, fair transaction ordering is crucial for a trusted and
regulation-compliant economic ecosystem. Unlike traditional State Machine
Replication (SMR) systems, which focus solely on liveness and safety,
blockchain systems also require a fairness property. This paper examines these
properties and aims to eliminate algorithmic bias in transaction ordering
services.
  We build on the notion of equal opportunity. We characterize transactions in
terms of relevant and irrelevant features, requiring that the order be
determined solely by the relevant ones. Specifically, transactions with
identical relevant features should have an equal chance of being ordered before
one another. We extend this framework to define a property where the greater
the distance in relevant features between transactions, the higher the
probability of prioritizing one over the other.
  We reveal a surprising link between equal opportunity in SMR and Differential
Privacy (DP), showing that any DP mechanism can be used to ensure fairness in
SMR. This connection not only enhances our understanding of the interplay
between privacy and fairness in distributed computing but also opens up new
opportunities for designing fair distributed protocols using well-established
DP techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Track reconstruction as a service for collider physics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan-Tang Chou, Miles Cochran-Branson, Javier Duarte, Yongbin Feng, Philip Harris, Shih-Chieh Hsu, Xiangyang Ju, Miaoyuan Liu, William Patrick McCormack, Kevin Pedro, Jan-Frederik Schulte, Nhan Tran, Yao Yao, Haoran Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing charged-particle track reconstruction algorithms is crucial for
efficient event reconstruction in Large Hadron Collider (LHC) experiments due
to their significant computational demands. Existing track reconstruction
algorithms have been adapted to run on massively parallel coprocessors, such as
graphics processing units (GPUs), to reduce processing time. Nevertheless,
challenges remain in fully harnessing the computational capacity of
coprocessors in a scalable and non-disruptive manner. This paper proposes an
inference-as-a-service approach for particle tracking in high energy physics
experiments. To evaluate the efficacy of this approach, two distinct tracking
algorithms are tested: Patatrack, a rule-based algorithm, and Exa$.$TrkX, a
machine learning-based algorithm. The as-a-service implementations show
enhanced GPU utilization and can process requests from multiple CPU cores
concurrently without increasing per-request latency. The impact of data
transfer is minimal and insignificant compared to running on local
coprocessors. This approach greatly improves the computational efficiency of
charged particle tracking, providing a solution to the computing challenges
anticipated in the High-Luminosity LHC era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 8 figures, submitted to JINST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TimeRL: Efficient Deep Reinforcement Learning with Polyhedral Dependence
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro F. Silvestre, Peter Pietzuch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern deep learning (DL) workloads increasingly use complex deep
reinforcement learning (DRL) algorithms that generate training data within the
learning loop. This results in programs with several nested loops and dynamic
data dependencies between tensors. While DL systems with eager execution
support such dynamism, they lack the optimizations and smart scheduling of
graph-based execution. Graph-based execution, however, cannot express dynamic
tensor shapes, instead requiring the use of multiple static subgraphs. Either
execution model for DRL thus leads to redundant computation, reduced
parallelism, and less efficient memory management.
  We describe TimeRL, a system for executing dynamic DRL programs that combines
the dynamism of eager execution with the whole-program optimizations and
scheduling of graph-based execution. TimeRL achieves this by introducing the
declarative programming model of recurrent tensors, which allows users to
define dynamic dependencies as intuitive recurrence equations. TimeRL
translates recurrent tensors into a polyhedral dependence graph (PDG) with
dynamic dependencies as symbolic expressions. Through simple PDG
transformations, TimeRL applies whole-program optimizations, such as automatic
vectorization, incrementalization, and operator fusion. The PDG also allows for
the computation of an efficient program-wide execution schedule, which decides
on buffer deallocations, buffer donations, and GPU/CPU memory swapping. We show
that TimeRL executes current DRL algorithms up to 47$\times$ faster than
existing DRL systems, while using 16$\times$ less GPU peak memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures, 5 bibliography pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Byzantine Fault Tolerant Protocols with Near-Constant Work per Node
  without Signatures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous distributed tasks have to be handled in a setting where a fraction
of nodes behaves Byzantine, that is, deviates arbitrarily from the intended
protocol. Resilient, deterministic protocols rely on the detection of
majorities to avoid inconsistencies if there is a Byzantine minority, which
requires individual nodes to handle a communication load that is proportional
to the size of the network -- an intolerable disadvantage in large networks.
  Randomized protocols circumvent this by probing only small parts of the
network, thus allowing for consistent decisions quickly and with a high level
of confidence with communication that is near-constant in the network size.
However, such protocols usually come with the drawback of limiting the fault
tolerance of the protocol. For instance, by severely restricting the number or
type of failures that the protocol can tolerate.
  We present randomized protocols to reliably aggregate and broadcast
information, form consensus and compute common coins that tolerate a constant
fraction of Byzantine failures, do not require cryptographic methods and have a
near-constant time and message complexity per node. Our main technique is to
compute a system of witness committees as a pre-computation step almost
optimally. This pre-computation step allows to solve the aforementioned
distributed tasks repeatedly and efficiently, but may have far reaching further
applications, e.g., for sharding of distributed data structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Validation of GPU Computation in Decentralized, Trustless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Boniardi, Stanley Bishop, Alison Haire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifying computational processes in decentralized networks poses a
fundamental challenge, particularly for Graphics Processing Unit (GPU)
computations. Our investigation reveals significant limitations in existing
approaches: exact recomputation fails due to computational non-determinism
across GPU nodes, Trusted Execution Environments (TEEs) require specialized
hardware, and Fully Homomorphic Encryption (FHE) faces prohibitive
computational costs. To address these challenges, we explore three verification
methodologies adapted from adjacent technical domains: model fingerprinting
techniques, semantic similarity analysis, and GPU profiling. Through systematic
exploration of these approaches, we develop novel probabilistic verification
frameworks, including a binary reference model with trusted node verification
and a ternary consensus framework that eliminates trust requirements. These
methodologies establish a foundation for ensuring computational integrity
across untrusted networks while addressing the inherent challenges of
non-deterministic execution in GPU-accelerated workloads.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Distributed Deployment of Mixture-of-Experts Model Inference
  in Serverless Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengfan Liu, Wei Wang, Chuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of serverless computing, running machine learning (ML)
inference services over a serverless platform has been advocated, given its
labor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models
have been a dominant type of model architectures to enable large models
nowadays, with parallel expert networks. Serving large MoE models on serverless
computing is potentially beneficial, but has been underexplored due to
substantial challenges in handling the skewed expert popularity and
scatter-gather communication bottleneck in MoE model execution, for
cost-efficient serverless MoE deployment and performance guarantee. We study
optimized MoE model deployment and distributed inference serving on a
serverless platform, that effectively predict expert selection, pipeline
communication with model execution, and minimize the overall billed cost of
serving MoE models. Especially, we propose a Bayesian optimization framework
with multi-dimensional epsilon-greedy search to learn expert selections and
optimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian
decision-making method for predicting expert popularity; 2) flexibly pipelined
scatter-gather communication; and 3) an optimal model deployment algorithm for
distributed MoE serving. Extensive experiments on AWS Lambda show that our
designs reduce the billed cost of all MoE layers by at least 75.67% compared to
CPU clusters while maintaining satisfactory inference throughput. As compared
to LambdaML in serverless computing, our designs achieves 43.41% lower cost
with a throughput decrease of at most 18.76%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Graph Algorithms with Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joan Boyar, Faith Ellen, Kim S. Larsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate the study of deterministic distributed graph algorithms with
predictions in synchronous message passing systems. The process at each node in
the graph is given a prediction, which is some extra information about the
problem instance that may be incorrect. The processes may use the predictions
to help them solve the problem. The overall goal is to develop algorithms that
both work faster when predictions are good and do not work much worse than
algorithms without predictions when predictions are bad. Concepts from the more
general area of algorithms with predictions, such as error measures,
consistency, robustness, and smoothness, are adapted to distributed graph
algorithms with predictions.
  We consider algorithms with predictions for four distributed graph problems,
Maximal Independent Set, Maximal Matching, $(\Delta+1)$-Vertex Coloring, and
$(2\Delta-1)$-Edge Coloring, where $\Delta$ denotes the degree of the graph.
For each, we define an appropriate error measure. We present generic templates
that can be used to design deterministic distributed graph algorithms with
predictions from existing algorithms without predictions. Using these
templates, we develop algorithms with predictions for Maximal Independent Set.
Alternative error measures for the Maximal Independent Set problem are also
considered. We obtain algorithms with predictions for general graphs and for
rooted trees and analyze them using two of these error measures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 2 figures, 6 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Scalable <span class="highlight-title">System</span> for Visual Analysis of Ocean Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toshit Jain, Upkar Singh, Varun Singh, Vijay Kumar Boda, Ingrid Hotz, Sathish S. Vadhiyar, P. N. Vinayachandran, Vijay Natarajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Oceanographers rely on visual analysis to interpret model simulations,
identify events and phenomena, and track dynamic ocean processes. The ever
increasing resolution and complexity of ocean data due to its dynamic nature
and multivariate relationships demands a scalable and adaptable visualization
tool for interactive exploration. We introduce pyParaOcean, a scalable and
interactive visualization system designed specifically for ocean data analysis.
pyParaOcean offers specialized modules for common oceanographic analysis tasks,
including eddy identification and salinity movement tracking. These modules
seamlessly integrate with ParaView as filters, ensuring a user-friendly and
easy-to-use system while leveraging the parallelization capabilities of
ParaView and a plethora of inbuilt general-purpose visualization
functionalities. The creation of an auxiliary dataset stored as a Cinema
database helps address I/O and network bandwidth bottlenecks while supporting
the generation of quick overview visualizations. We present a case study on the
Bay of Bengal (BoB) to demonstrate the utility of the system and scaling
studies to evaluate the efficiency of the system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topology-aware Microservice Architecture in Edge Networks: Deployment
  Optimization and Implementation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Chen, Chang Wu, Fangyu Zhang, Chengdi Lu, Yongsheng Huang, Hancheng Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a ubiquitous deployment paradigm, integrating microservice architecture
(MSA) into edge networks promises to enhance the flexibility and scalability of
services. However, it also presents significant challenges stemming from
dispersed node locations and intricate network topologies. In this paper, we
have proposed a topology-aware MSA characterized by a three-tier network
traffic model encompassing the service, microservices, and edge node layers.
This model meticulously characterizes the complex dependencies between edge
network topologies and microservices, mapping microservice deployment onto link
traffic to accurately estimate communication delay. Building upon this model,
we have formulated a weighted sum communication delay optimization problem
considering different types of services. Then, a novel topology-aware and
individual-adaptive microservices deployment (TAIA-MD) scheme is proposed to
solve the problem efficiently, which accurately senses the network topology and
incorporates an individual-adaptive mechanism in a genetic algorithm to
accelerate the convergence and avoid local optima. Extensive simulations show
that, compared to the existing deployment schemes, TAIA-MD improves the
communication delay performance by approximately 30% to 60% and effectively
enhances the overall network performance. Furthermore, we implement the TAIA-MD
scheme on a practical microservice physical platform. The experimental results
demonstrate that TAIA-MD achieves superior robustness in withstanding link
failures and network fluctuations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 17 figures, submitted to IEEE Transactions for potential
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CloudSim 7G: An Integrated Toolkit for Modeling and Simulation of Future
  Generation Cloud Computing Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13386v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13386v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Remo Andreoli, Jie Zhao, Tommaso Cucinotta, Rajkumar Buyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud Computing has established itself as an efficient and cost-effective
paradigm for the execution of web-based applications, and scientific workloads,
that need elasticity and on-demand scalability capabilities. However, the
evaluation of novel resource provisioning and management techniques is a major
challenge due to the complexity of large-scale data centers. Therefore, Cloud
simulators are an essential tool for academic and industrial researchers, to
investigate the effectiveness of novel algorithms and mechanisms in large-scale
scenarios. This paper proposes CloudSim 7G, the seventh generation of CloudSim,
which features a re-engineered and generalized internal architecture to
facilitate the integration of multiple CloudSim extensions within the same
simulated environment. As part of the new design, we introduced a set of
standardized interfaces to abstract common functionalities and carried out
extensive refactoring and refinement of the codebase. The result is a
substantial reduction in lines of code with no loss in functionality,
significant improvements in run-time performance and memory efficiency (up to
25% less heap memory allocated), as well as increased flexibility, ease-of-use,
and extensibility of the framework. These improvements benefit not only
CloudSim developers but also researchers and practitioners using the framework
for modeling and simulating next-generation Cloud Computing environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Second revision of paper, submitted to Wiley Online Software:
  Practice and Experience</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decentralized Federated Anomaly Detection in Smart Grids: A P2P Gossip
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Akbar Husnoo, Adnan Anwar, Md Enamul Haque, A. N. Mahmood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing security and privacy concerns in the Smart Grid sector have
led to a significant demand for robust intrusion detection systems within
critical smart grid infrastructure. To address the challenges posed by privacy
preservation and decentralized power system zones with distinct data ownership,
Federated Learning (FL) has emerged as a promising privacy-preserving solution
which facilitates collaborative training of attack detection models without
necessitating the sharing of raw data. However, FL presents several
implementation limitations in the power system domain due to its heavy reliance
on a centralized aggregator and the risks of privacy leakage during model
update transmission. To overcome these technical bottlenecks, this paper
introduces a novel decentralized federated anomaly detection scheme based on
two main gossip protocols namely Random Walk and Epidemic. Our findings
indicate that the Random Walk protocol exhibits superior performance compared
to the Epidemic protocol, highlighting its efficacy in decentralized federated
learning environments. Experimental validation of the proposed framework
utilizing publicly available industrial control systems datasets demonstrates
superior attack detection accuracy while safeguarding data confidentiality and
mitigating the impact of communication latency and stragglers. Furthermore, our
approach yields a notable 35% improvement in training time compared to
conventional FL, underscoring the efficacy and robustness of our decentralized
learning method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence Analysis of Split Federated Learning on Heterogeneous Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15166v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15166v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengchao Han, Chao Huang, Geng Tian, Ming Tang, Xin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Split federated learning (SFL) is a recent distributed approach for
collaborative model training among multiple clients. In SFL, a global model is
typically split into two parts, where clients train one part in a parallel
federated manner, and a main server trains the other. Despite the recent
research on SFL algorithm development, the convergence analysis of SFL is
missing in the literature, and this paper aims to fill this gap. The analysis
of SFL can be more challenging than that of federated learning (FL), due to the
potential dual-paced updates at the clients and the main server. We provide
convergence analysis of SFL for strongly convex and general convex objectives
on heterogeneous data. The convergence rates are $O(1/T)$ and
$O(1/\sqrt[3]{T})$, respectively, where $T$ denotes the total number of rounds
for SFL training. We further extend the analysis to non-convex objectives and
the scenario where some clients may be unavailable during training.
Experimental experiments validate our theoretical results and show that SFL
outperforms FL and split learning (SL) when data is highly heterogeneous across
a large number of clients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Conference on Neural Information Processing Systems
  (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deterministic and Reliable Software-Defined Vehicles: key building
  blocks, challenges, and vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Veloso Teixeira, Duarte Raposo, Rui Lopes, Susana Sargento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As vehicle systems become increasingly complex, with more features, services,
sensors, actuators, and processing units, it is important to view vehicles not
just as modes of transportation moving toward full autonomy, but also as
adaptive systems that respond to the needs of their occupants. Vehicular
services can be developed to support these adaptations. However, the increasing
complexity of vehicular service development, even with current
standardizations, best practices and guidelines, are insufficient to tackle the
high complexity of development, with expectations of up to 1 (U.S.) billion
lines of code for a fully (level 5) autonomous vehicle. Within this survey, the
paradigm of Deterministic Software Defined Vehicles is explored, aiming to
enhance the quality and ease of developing automotive services by focusing on
service-oriented architectures, virtualization techniques, and the necessary
deterministic intra- and inter-vehicular communications. Considering the main
open challenges for such verticals, a vision architecture towards improved
services development and orchestration is presented, focusing on: a) a
deterministic network configurator; b) a data layer configurator; c) a
hypervisor configurator; d) the vehicle abstraction layer; and e) a software
orchestrator.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-08T00:00:00Z">2025-01-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Histologic <span class="highlight-title">Dataset</span> of Normal and Atypical Mitotic Figures on Human
  Breast Cancer (AMi-Br) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christof A. Bertram, Viktoria Weiss, Taryn A. Donovan, Sweta Banerjee, Thomas Conrad, Jonas Ammeling, Robert Klopfleisch, Christopher Kaltenecker, Marc Aubreville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessment of the density of mitotic figures (MFs) in histologic tumor
sections is an important prognostic marker for many tumor types, including
breast cancer. Recently, it has been reported in multiple works that the
quantity of MFs with an atypical morphology (atypical MFs, AMFs) might be an
independent prognostic criterion for breast cancer. AMFs are an indicator of
mutations in the genes regulating the cell cycle and can lead to aberrant
chromosome constitution (aneuploidy) of the tumor cells. To facilitate further
research on this topic using pattern recognition, we present the first ever
publicly available dataset of atypical and normal MFs (AMi-Br). For this, we
utilized two of the most popular MF datasets (MIDOG 2021 and TUPAC) and
subclassified all MFs using a three expert majority vote. Our final dataset
consists of 3,720 MFs, split into 832 AMFs (22.4%) and 2,888 normal MFs (77.6%)
across all 223 tumor cases in the combined set. We provide baseline
classification experiments to investigate the consistency of the dataset, using
a Monte Carlo cross-validation and different strategies to combat class
imbalance. We found an averaged balanced accuracy of up to 0.806 when using a
patch-level data set split, and up to 0.713 when using a patient-level split.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keyword Search in the Deep Web 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Calì, Davide Martinenghi, Riccardo Torlone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Deep Web is constituted by data that are accessible through Web pages,
but not readily indexable by search engines as they are returned in dynamic
pages. In this paper we propose a conceptual framework for answering keyword
queries on Deep Web sources represented as relational tables with so-called
access limitations. We formalize the notion of optimal answer, characterize
queries for which an answer can be found, and present a method for query
processing based on the construction of a query plan that minimizes the
accesses to the data sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures. A short version appeared in ER 2016</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced
  AI Applications with Retrieval Augmented Generation and Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02994v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02994v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irene Siragusa, Salvatore Contino, Massimo La Ciura, Rosario Alicata, Roberto Pirrone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing interest in developing Artificial Intelligence applications in
the medical domain, suffers from the lack of high-quality data set, mainly due
to privacy-related issues. In addition, the recent increase in large multimodal
models (LMM) leads to the need for multimodal medical data sets, where clinical
reports and findings are attached to the corresponding CT or MRI scans. This
paper illustrates the entire workflow for building the MedPix 2.0 data set.
Starting with the well-known multimodal data set
MedPix\textsuperscript{\textregistered}, mainly used by physicians, nurses, and
healthcare students for Continuing Medical Education purposes, a semi-automatic
pipeline was developed to extract visual and textual data followed by a manual
curing procedure in which noisy samples were removed, thus creating a MongoDB
database. Along with the data set, we developed a GUI aimed at navigating
efficiently the MongoDB instance and obtaining the raw data that can be easily
used for training and/or fine-tuning LMMs. To enforce this point, in this work,
we first recall DR-Minerva, a RAG-based LMM trained using MedPix 2.0.
DR-Minerva predicts the body part and the modality used to scan its input
image. We also propose the extension of DR-Minerva with a Knowledge Graph that
uses Llama 3.1 Instruct 8B, and leverages MedPix 2.0. The resulting
architecture can be queried in a end-to-end manner, as a medical decision
support system. MedPix 2.0 is available on GitHub.
\url{https://github.com/CHILab1/MedPix-2.0}
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recorder: Comprehensive Parallel I/O Tracing and Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Wang, Izzet Yildirim, Hariharan Devarajan, Kathryn Mohror, Marc Snir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Recorder, a parallel I/O tracing tool designed to capture
comprehensive I/O information on HPC applications. Recorder traces I/O calls
across various I/O layers, storing all function parameters for each captured
call. The volume of stored information scales linearly the application's
execution scale. To address this, we present a sophisticated
pattern-recognition-based compression algorithm. This algorithm identifies and
compresses recurring I/O patterns both within individual processes and across
multiple processes, significantly reducing space and time overheads. We
evaluate the proposed compression algorithm using I/O benchmarks and real-world
applications, demonstrating that Recorder can store more information while
requiring approximately 12x less storage space compared to its predecessor.
Notably, for applications with typical parallel I/O patterns, Recorder achieves
a constant trace size regardless of execution scale. Additionally, a comparison
with the profiling tool Darshan shows that Recorder captures detailed I/O
information without incurring substantial overhead. The richer data collected
by Recorder enables new insights and facilitates more in-depth I/O studies,
offering valuable contributions to the I/O research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages. Under Review. Submitted to the Journal of Supercomputing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Microservice Deployment in Space Computing Power Networks via Robust
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyong Yu, Yuning Jiang, Xin Liu, Yuanming Shi, Chunxiao Jiang, Linling Kuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing demand for Earth observation, it is important to provide
reliable real-time remote sensing inference services to meet the low-latency
requirements. The Space Computing Power Network (Space-CPN) offers a promising
solution by providing onboard computing and extensive coverage capabilities for
real-time inference. This paper presents a remote sensing artificial
intelligence applications deployment framework designed for Low Earth Orbit
satellite constellations to achieve real-time inference performance. The
framework employs the microservice architecture, decomposing monolithic
inference tasks into reusable, independent modules to address high latency and
resource heterogeneity. This distributed approach enables optimized
microservice deployment, minimizing resource utilization while meeting quality
of service and functional requirements. We introduce Robust Optimization to the
deployment problem to address data uncertainty. Additionally, we model the
Robust Optimization problem as a Partially Observable Markov Decision Process
and propose a robust reinforcement learning algorithm to handle the
semi-infinite Quality of Service constraints. Our approach yields sub-optimal
solutions that minimize accuracy loss while maintaining acceptable
computational costs. Simulation results demonstrate the effectiveness of our
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semantic Partitioning Method for Large-Scale Training of Knowledge
  Graph Embeddings <span class="chip">WWW '23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhe Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, knowledge graph embeddings have achieved great success. Many
methods have been proposed and achieved state-of-the-art results in various
tasks. However, most of the current methods present one or more of the
following problems: (i) They only consider fact triplets, while ignoring the
ontology information of knowledge graphs. (ii) The obtained embeddings do not
contain much semantic information. Therefore, using these embeddings for
semantic tasks is problematic. (iii) They do not enable large-scale training.
In this paper, we propose a new algorithm that incorporates the ontology of
knowledge graphs and partitions the knowledge graph based on classes to include
more semantic information for parallel training of large-scale knowledge graph
embeddings. Our preliminary results show that our algorithm performs well on
several popular benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WWW '23 Companion: Companion Proceedings of the ACM Web
  Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intelligent Task Offloading: Advanced MEC Task Offloading and Resource
  Management in 5G Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Ebrahimi, Fatemeh Afghah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  5G technology enhances industries with high-speed, reliable, low-latency
communication, revolutionizing mobile broadband and supporting massive IoT
connectivity. With the increasing complexity of applications on User Equipment
(UE), offloading resource-intensive tasks to robust servers is essential for
improving latency and speed. The 3GPP's Multi-access Edge Computing (MEC)
framework addresses this challenge by processing tasks closer to the user,
highlighting the need for an intelligent controller to optimize task offloading
and resource allocation. This paper introduces a novel methodology to
efficiently allocate both communication and computational resources among
individual UEs. Our approach integrates two critical 5G service imperatives:
Ultra-Reliable Low Latency Communication (URLLC) and Massive Machine Type
Communication (mMTC), embedding them into the decision-making framework.
Central to this approach is the utilization of Proximal Policy Optimization,
providing a robust and efficient solution to the challenges posed by the
evolving landscape of 5G technology. The proposed model is evaluated in a
simulated 5G MEC environment. The model significantly reduces processing time
by 4% for URLLC users under strict latency constraints and decreases power
consumption by 26% for mMTC users, compared to existing baseline models based
on the reported simulation results. These improvements showcase the model's
adaptability and superior performance in meeting diverse QoS requirements in 5G
networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Data Notarization Leveraging Hybrid DLTs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domenico Tortola, Claudio Felicioli, Andrea Canciani, Fabio Severino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Notarization is a procedure that enhance data management by ensuring the
authentication of data during audits, thereby increasing trust in the audited
data. Blockchain is frequently used as a secure, immutable, and transparent
storage, contributing to make data notarization procedures more effective and
trustable. Several blockchain-based data notarization protocols have been
proposed in literature and commercial solutions. However, these
implementations, whether on public or private blockchains, face inherent
challenges: high fees on public blockchains and trust issues on private
platforms, limiting the adoption of blockchains for data notarization or
forcing several trade-offs. In this paper, we explore the use of hybrid
blockchain architectures for data notarization, with a focus on scalability
issues. Through the analysis of a real-world use case, the data notarization of
product passports in supply chains, we propose a novel approach utilizing a
data structure designed to efficiently manage the trade-offs in terms of
storage occupation and costs involved in notarizing a large collection of data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>presented at WETICE 2024, pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effective Two-Stage Double Auction for Dynamic Resource Trading in Edge
  Networks via Overbooking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicheng Wu, Minghui Liwang, Deqing Wang, Xianbin Wang, Chao Wu, Junyi Tang, Li Li, Zhenzhen Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To facilitate responsive and cost-effective computing resource scheduling and
service delivery over edge-assisted mobile networks, this paper investigates a
novel two-stage double auction methodology via utilizing an interesting idea of
resource overbooking to overcome dynamic and uncertain nature from edge servers
(sellers) and demand from mobile devices (as buyers). The proposed auction
integrates multiple essential factors such as social welfare maximization and
decision-making latency (e.g., the time for determining winning seller-buyer
pairs) reduction, by introducing a stagewise strategy: an overbooking-driven
pre-double auction (OPDAuction) for determining long-term cooperations between
sellers and buyers before practical resource transactions as Stage I, and a
real-time backup double auction (RBDAuction) for handling residual resource
demands during actual transactions. In particular, by applying a proper
overbooking rate, OPDAuction helps with facilitating trading contracts between
appropriate sellers and buyers as guidance for future transactions, by allowing
the booked resources to exceed supply. Then, since pre-auctions may cause
risks, our RBDAuction adjusts to real-time market changes, further enhancing
the overall social welfare. More importantly, we offer an interesting view to
show that our proposed two-stage auction can support significant design
properties such as truthfulness, individual rationality, and budget balance.
Through extensive experiments, we demonstrate good performance in social
welfare, time efficiency, and computational scalability, outstripping
conventional methods in dynamic edge computing settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Developing a Modular Compiler for a Subset of a C-like Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debasish Dutta, Neeharika Sonowal, Irani Hazarika
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper introduces the development of a modular compiler for a subset of a
C-like language, which addresses the challenges in constructing a compiler for
high-level languages. This modular approach will allow developers to modify a
language by adding or removing subsets as required, resulting in a minimal and
memory-efficient compiler. The development process is divided into small,
incremental steps, where each step yields a fully functioning compiler for an
expanding subset of the language. The paper outlines the iterative
developmental phase of the compiler, emphasizing progressive enhancements in
capabilities and functionality. Adherence to industry best practices of modular
design, code reusability, and documentation has enabled the resulting
compiler's functional efficiency, maintainability, and extensibility. The
compiler proved to be effective not only in managing the language structure but
also in developing optimized code, which demonstrates its practical usability.
This was also further assessed using the compiler on a tiny memory-deficient
single-board computer, again showing the compiler's efficiency and suitability
for resource-constrained devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Inference Acceleration with Non-Penetrative Tensor
  Partitioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibang Liu, Chaonong Xu, Zhenjie Lv, Zhizhuo Liu, Suyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inference of large-sized images on Internet of Things (IoT) devices is
commonly hindered by limited resources, while there are often stringent latency
requirements for Deep Neural Network (DNN) inference. Currently, this problem
is generally addressed by collaborative inference, where the large-sized image
is partitioned into multiple tiles, and each tile is assigned to an IoT device
for processing. However, since significant latency will be incurred due to the
communication overhead caused by tile sharing, the existing collaborative
inference strategy is inefficient for convolutional computation, which is
indispensable for any DNN. To reduce it, we propose Non-Penetrative Tensor
Partitioning (NPTP), a fine-grained tensor partitioning method that reduces the
communication latency by minimizing the communication load of tiles shared,
thereby reducing inference latency. We evaluate NPTP with four widely-adopted
DNN models. Experimental results demonstrate that NPTP achieves a 1.44-1.68x
inference speedup relative to CoEdge, a state-of-the-art (SOTA) collaborative
inference algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 2025 IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demystification and Near-perfect Estimation of Minimum Gas Limit and Gas
  Used for Ethereum Smart Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danilo Rafael de Lima Cabral, Pedro Antonino, Augusto Sampaio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Ethereum blockchain has a \emph{gas system} that associates operations
with a cost in gas units. Two central concepts of this system are the \emph{gas
limit} assigned by the issuer of a transaction and the \emph{gas used} by a
transaction. The former is a budget that must not be exhausted before the
completion of the transaction execution; otherwise, the execution fails.
Therefore, it seems rather essential to determine the \emph{minimum gas limit}
that ensures the execution of a transaction will not abort due to the lack of
gas. Despite its practical relevance, this concept has not been properly
addressed. In the literature, gas used and minimum gas limit are conflated.
This paper proposes a precise notion of minimum gas limit and how it can differ
from gas used by a transaction; this is also demonstrated with a quantitative
study on real transactions of the Ethereum blockchain. Another significant
contribution is the proposition of a fairly precise estimator for each of the
two metrics. Again, the confusion between these concepts has led to the
creation of estimators only for the gas used by a transaction. We demonstrate
that the minimum gas limit for the state of the Ethereum blockchain (after the
block) $t$ can serve as a near-perfect estimation for the execution of the
transaction at block $t + \Delta$, where $\Delta \leq 11$; the same holds for
estimating gas used. These precise estimators can be very valuable in helping
the users predict the gas budget of transactions and developers in optimising
their smart contracts; over and underestimating gas used and minimum gas limit
can lead to a number of practical issues. Overall, this paper serves as an
important reference for blockchain developers and users as to how the gas
system really works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting LocalSGD and SCAFFOLD: Improved Rates and Missing Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichen Luo, Sebastian U Stich, Samuel Horváth, Martin Takáč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LocalSGD and SCAFFOLD are widely used methods in distributed stochastic
optimization, with numerous applications in machine learning, large-scale data
processing, and federated learning. However, rigorously establishing their
theoretical advantages over simpler methods, such as minibatch SGD (MbSGD), has
proven challenging, as existing analyses often rely on strong assumptions,
unrealistic premises, or overly restrictive scenarios.
  In this work, we revisit the convergence properties of LocalSGD and SCAFFOLD
under a variety of existing or weaker conditions, including gradient
similarity, Hessian similarity, weak convexity, and Lipschitz continuity of the
Hessian. Our analysis shows that (i) LocalSGD achieves faster convergence
compared to MbSGD for weakly convex functions without requiring stronger
gradient similarity assumptions; (ii) LocalSGD benefits significantly from
higher-order similarity and smoothness; and (iii) SCAFFOLD demonstrates faster
convergence than MbSGD for a broader class of non-quadratic functions. These
theoretical insights provide a clearer understanding of the conditions under
which LocalSGD and SCAFFOLD outperform MbSGD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoDFL: A Scalable and Automated Reputation-Aware Decentralized
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meryem Malak Dif, Mouhamed Amine Bouchiha, Mourad Rabah, Yacine Ghamri-Doudane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blockchained federated learning (BFL) combines the concepts of federated
learning and blockchain technology to enhance privacy, security, and
transparency in collaborative machine learning models. However, implementing
BFL frameworks poses challenges in terms of scalability and cost-effectiveness.
Reputation-aware BFL poses even more challenges, as blockchain validators are
tasked with processing federated learning transactions along with the
transactions that evaluate FL tasks and aggregate reputations. This leads to
faster blockchain congestion and performance degradation. To improve BFL
efficiency while increasing scalability and reducing on-chain reputation
management costs, this paper proposes AutoDFL, a scalable and automated
reputation-aware decentralized federated learning framework. AutoDFL leverages
zk-Rollups as a Layer-2 scaling solution to boost the performance while
maintaining the same level of security as the underlying Layer-1 blockchain.
Moreover, AutoDFL introduces an automated and fair reputation model designed to
incentivize federated learning actors. We develop a proof of concept for our
framework for an accurate evaluation. Tested with various custom workloads,
AutoDFL reaches an average throughput of over 3000 TPS with a gas reduction of
up to 20X.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at NOMS'2025 (pages 9, figures 5)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VerifBFL: Leveraging zk-SNARKs for A Verifiable Blockchained Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Ayoub Bellachia, Mouhamed Amine Bouchiha, Yacine Ghamri-Doudane, Mourad Rabah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blockchain-based Federated Learning (FL) is an emerging decentralized machine
learning paradigm that enables model training without relying on a central
server. Although some BFL frameworks are considered privacy-preserving, they
are still vulnerable to various attacks, including inference and model
poisoning. Additionally, most of these solutions employ strong trust
assumptions among all participating entities or introduce incentive mechanisms
to encourage collaboration, making them susceptible to multiple security flaws.
This work presents VerifBFL, a trustless, privacy-preserving, and verifiable
federated learning framework that integrates blockchain technology and
cryptographic protocols. By employing zero-knowledge Succinct Non-Interactive
Argument of Knowledge (zk-SNARKs) and incrementally verifiable computation
(IVC), VerifBFL ensures the verifiability of both local training and
aggregation processes. The proofs of training and aggregation are verified
on-chain, guaranteeing the integrity and auditability of each participant's
contributions. To protect training data from inference attacks, VerifBFL
leverages differential privacy. Finally, to demonstrate the efficiency of the
proposed protocols, we built a proof of concept using emerging tools. The
results show that generating proofs for local training and aggregation in
VerifBFL takes less than 81s and 2s, respectively, while verifying them
on-chain takes less than 0.6s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at NOMS'25 (9 pages, 6 Figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Large Language Model Training on Frontier with Low-Bandwidth
  Partitioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lang Xu, Quentin Anthony, Jacob Hatef, Aamir Shafi, Hari Subramoni, Dhabaleswar K.,  Panda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up Large Language Model(LLM) training involves fitting a tremendous
amount of training parameters across a limited number of workers. However,
methods like ZeRO-3 that drastically reduce GPU memory pressure often incur
heavy communication to ensure global synchronization and consistency.
Established efforts such as ZeRO++ use secondary partitions to avoid inter-node
communications, given that intra-node GPU-GPU transfer generally has more
bandwidth and lower latency than inter-node connections. However, as more
capable infrastructure like Frontier, equipped with AMD GPUs, emerged with
impressive computing capability, there is a need for investigations on the
hardware topology and to develop targeted strategies to improve training
efficiency. In this work, we propose a collection of communication and
optimization strategies for ZeRO++ to reduce communication costs and improve
memory utilization. In this paper, we propose a 3-level hierarchical
partitioning specifically for the current Top-1 supercomputing cluster,
Frontier, which aims at leveraging various bandwidths across layers of
communications (GCD-GCD, GPU-GPU, and inter-node) to reduce communication
overhead. For a 20B GPT model, we observe a 1.71x increase in TFLOPS per GPU
when compared with ZeRO++ up to 384 GCDs and a scaling efficiency of 0.94 for
up to 384 GCDs. To the best of our knowledge, our work is also the first effort
to efficiently optimize LLM workloads on Frontier AMD GPUs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiCoCS: High Concurrency Cross-Sharding on Permissioned Blockchains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingxiao Yang, Xuewen Dong, Zhiguo Wan, Di Lu, Yushu Zhang, Yulong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the foundation of the Web3 trust system, blockchain technology faces
increasing demands for scalability. Sharding emerges as a promising solution,
but it struggles to handle highly concurrent cross-shard transactions
(\textsf{CSTx}s), primarily due to simultaneous ledger operations on the same
account. Hyperledger Fabric, a permissioned blockchain, employs multi-version
concurrency control for parallel processing. Existing solutions use channels
and intermediaries to achieve cross-sharding in Hyperledger Fabric. However,
the conflict problem caused by highly concurrent \textsf{CSTx}s has not been
adequately resolved. To fill this gap, we propose HiCoCS, a high concurrency
cross-shard scheme for permissioned blockchains. HiCoCS creates a unique
virtual sub-broker for each \textsf{CSTx} by introducing a composite key
structure, enabling conflict-free concurrent transaction processing while
reducing resource overhead. The challenge lies in managing large numbers of
composite keys and mitigating intermediary privacy risks. HiCoCS utilizes
virtual sub-brokers to receive and process \textsf{CSTx}s concurrently while
maintaining a transaction pool. Batch processing is employed to merge multiple
\textsf{CSTx}s in the pool, improving efficiency. We explore composite key
reuse to reduce the number of virtual sub-brokers and lower system overhead.
Privacy preservation is enhanced using homomorphic encryption. Evaluations show
that HiCoCS improves cross-shard transaction throughput by 3.5-20.2 times
compared to the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Major Revised in IEEE TC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Publish on Ping: A Better Way to Publish Reservations in Memory
  Reclamation for Concurrent Data Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajay Singh, Trevor Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe memory reclamation techniques that utilize per read reservations, such
as hazard pointers, often cause significant overhead in traversals of linked
concurrent data structures. This is primarily due to the need to announce a
reservation, and fence to enforce appropriate ordering, before each read. In
read-intensive workloads, this overhead is amplified because, even if
relatively little memory reclamation actually occurs, the full overhead of
reserving records is still incurred while traversing data structures.
  In this paper, we propose a novel memory reclamation technique by combining
POSIX signals and delayed reclamation, introducing a publish-on-ping approach.
This method eliminates the need to make reservations globally visible before
use. Instead, threads privately track which records they are accessing, and
share this information on demand with threads that intend to reclaim memory.
The approach can serve as a drop-in replacement for hazard pointers and hazard
eras. Furthermore, the capability to retain reservations during traversals in
data structure operations and publish them on demand facilitates the
construction of a variant of hazard pointers (EpochPOP). This variant uses
epochs to approach the performance of epoch-based reclamation in the common
case where threads are not frequently delayed (while retaining the robustness
of hazard pointers).
  Our publish-on-ping implementations based on hazard pointers (HP) and hazard
eras, when applied to various data structures, exhibit significant performance
improvements. The improvements across various workloads and data structures
range from 1.2X to 4X over the original HP, up to 20% compared to a heavily
optimized HP implementation similar to the one in the Folly open-source
library, and up to 3X faster than hazard eras. EpochPOP delivers performance
similar to epoch-based reclamation while providing stronger guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of full paper accepted at PPoPP '25: The 30th ACM
  SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming
  Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Splicer$^{+}$: Secure Hub Placement and Deadlock-Free Routing for
  Payment Channel Network Scalability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingxiao Yang, Xuewen Dong, Wei Wang, Sheng Gao, Qiang Qu, Wensheng Tian, Yulong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Payment channel hub (PCH) is a promising approach for payment channel
networks (PCNs) to improve efficiency by deploying robust hubs to steadily
process off-chain transactions. However, existing PCHs, often preplaced without
considering payment request distribution across PCNs, can lead to load
imbalance. PCNs' reliance on source routing, which makes decisions based solely
on individual sender requests, can degrade performance by overlooking other
requests, thus further impairing scalability. In this paper, we introduce
Splicer$^{+}$, a highly scalable multi-PCH solution based on the trusted
execution environment (TEE). We study tradeoffs in communication overhead
between participants, transform the original NP-hard PCH placement problem by
mixed-integer linear programming, and propose optimal/approximate solutions
with load balancing for different PCN scales using supermodular techniques.
Considering global PCN states and local directly connected sender requests, we
design a deadlock-free routing protocol for PCHs. It dynamically adjusts the
payment processing rate across multiple channels and, combined with TEE,
ensures high-performance routing with confidential computation. We provide a
formal security proof for the Splicer$^{+}$ protocol in the UC-framework.
Extensive evaluations demonstrate the effectiveness of Splicer$^{+}$, with
transaction success ratio ($\uparrow$51.1%), throughput ($\uparrow$181.5%), and
latency outperforming state-of-the-art PCNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of ICDCS 2023 (arXiv:2305.19182)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Online Federated Learning with Correlated Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16542v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16542v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaojiao Zhang, Linglingzhi Zhu, Mikael Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel differentially private algorithm for online federated
learning that employs temporally correlated noise to enhance utility while
ensuring privacy of continuously released models. To address challenges posed
by DP noise and local updates with streaming non-iid data, we develop a
perturbed iterate analysis to control the impact of the DP noise on the
utility. Moreover, we demonstrate how the drift errors from local updates can
be effectively managed under a quasi-strong convexity condition. Subject to an
$(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the
entire time horizon, quantifying the impact of key parameters and the intensity
of changes in dynamic environments. Numerical experiments confirm the efficacy
of the proposed algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locally Differentially Private Online Federated Learning With Correlated
  Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaojiao Zhang, Linglingzhi Zhu, Dominik Fay, Mikael Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a locally differentially private (LDP) algorithm for online
federated learning that employs temporally correlated noise to improve utility
while preserving privacy. To address challenges posed by the correlated noise
and local updates with streaming non-IID data, we develop a perturbed iterate
analysis that controls the impact of the noise on the utility. Moreover, we
demonstrate how the drift errors from local updates can be effectively managed
for several classes of nonconvex loss functions. Subject to an
$(\epsilon,\delta)$-LDP budget, we establish a dynamic regret bound that
quantifies the impact of key parameters and the intensity of changes in the
dynamic environment on the learning performance. Numerical experiments confirm
the efficacy of the proposed algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2403.16542</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coded Distributed Computing with Pre-set Assignments of Data and Output
  Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.06300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.06300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Wang, Youlong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coded distributed computing can reduce the communication load for distributed
computing systems by introducing redundant computation and creating
multicasting opportunities. However, the existing schemes require delicate data
placement and output function assignment, which is not feasible when
distributed nodes fetch data without the orchestration of a master node. In
this paper, we consider the general systems where the data placement and output
function assignment are arbitrary but pre-set. We propose two coded computing
schemes, One-shot Coded Transmission (OSCT) and Few-shot Coded Transmission
(FSCT), to reduce the communication load. Both schemes first group the nodes
into clusters and divide the transmission of each cluster into multiple rounds,
and then design coded transmission in each round to maximize the multicast
gain. The key difference between OSCT and FSCT is that the former uses a
one-shot transmission where each encoded message can be decoded independently
by the intended nodes, while the latter allows each node to jointly decode
multiple received symbols to achieve potentially larger multicast gains.
Furthermore, based on the lower bound proposed by Yu et al., we derive
sufficient conditions for the optimality of OSCT and FSCT, respectively. This
not only recovers the existing optimality results but also includes some cases
where our schemes are optimal while others are not.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Information Theory</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Byzantine Robustness in Federated Recommendation from Sparse
  Aggregation Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongjian Zhang, Mengmei Zhang, Xiao Wang, Lingjuan Lyu, Bo Yan, Junping Du, Chuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To preserve user privacy in recommender systems, federated recommendation
(FR) based on federated learning (FL) emerges, keeping the personal data on the
local client and updating a model collaboratively. Unlike FL, FR has a unique
sparse aggregation mechanism, where the embedding of each item is updated by
only partial clients, instead of full clients in a dense aggregation of general
FL. Recently, as an essential principle of FL, model security has received
increasing attention, especially for Byzantine attacks, where malicious clients
can send arbitrary updates. The problem of exploring the Byzantine robustness
of FR is particularly critical since in the domains applying FR, e.g.,
e-commerce, malicious clients can be injected easily by registering new
accounts. However, existing Byzantine works neglect the unique sparse
aggregation of FR, making them unsuitable for our problem. Thus, we make the
first effort to investigate Byzantine attacks on FR from the perspective of
sparse aggregation, which is non-trivial: it is not clear how to define
Byzantine robustness under sparse aggregations and design Byzantine attacks
under limited knowledge/capability. In this paper, we reformulate the Byzantine
robustness under sparse aggregation by defining the aggregation for a single
item as the smallest execution unit. Then we propose a family of effective
attack strategies, named Spattack, which exploit the vulnerability in sparse
aggregation and are categorized along the adversary's knowledge and capability.
Extensive experimental results demonstrate that Spattack can effectively
prevent convergence and even break down defenses under a few malicious clients,
raising alarms for securing FR systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shattering the Ephemeral Storage Cost Barrier for Data-Intensive
  Serverless Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitrii Ustiugov, Shyam Jesalpura, Mert Bora Alper, Michal Baczun, Rustem Feyzkhanov, Edouard Bugnion, Boris Grot, Marios Kogias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serverless computing is a popular cloud deployment paradigm where developers
implement applications as workflows of functions that invoke each other. Cloud
providers automatically scale function instances on demand and forward workflow
requests to appropriate instances. However, current serverless clouds lack
efficient cross-function data transfer, limiting the execution of
data-intensive applications. Functions often rely on third-party services like
AWS S3, AWS ElastiCache, or multi-tier solutions for intermediate data
transfers, which introduces inefficiencies.
  We demonstrate that such through-storage transfers make data-intensive
deployments economically impractical, with storage costs comprising more than
24-99% of the total serverless bill. To address this, we introduce Zipline, a
fast, API-preserving data communication method for serverless platforms.
Zipline enables direct function-to-function transfers, where the sender
function buffers payloads in memory and sends a reference to the receiver. The
receiver retrieves the data directly from the sender's memory, guided by the
load balancer and autoscaler. Zipline integrates seamlessly with existing
autoscaling, maintains invocation semantics, and eliminates the costs and
overheads of intermediate services. We prototype Zipline in vHive/Knative on
AWS EC2 nodes, demonstrating significant improvements. Zipline reduces costs
and enhances latency and bandwidth compared to AWS S3 (the lowest-cost
solution) and ElastiCache (the highest-performance solution). On real-world
applications, Zipline lowers costs by 2-5x and reduces execution times by
1.3-3.4x versus S3. Compared to ElastiCache, Zipline achieves 17-772x cost
reductions while improving performance by 2-5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>added cost reduction details</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-07T00:00:00Z">2025-01-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEAP: LLM-powered End-to-end Automatic Library for Processing Social
  Science Queries on Unstructured Data <span class="chip">VLDB 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuxuan Hu, Austin Peters, Daniel Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social scientists are increasingly interested in analyzing the semantic
information (e.g., emotion) of unstructured data (e.g., Tweets), where the
semantic information is not natively present. Performing this analysis in a
cost-efficient manner requires using machine learning (ML) models to extract
the semantic information and subsequently analyze the now structured data.
However, this process remains challenging for domain experts.
  To demonstrate the challenges in social science analytics, we collect a
dataset, QUIET-ML, of 120 real-world social science queries in natural language
and their ground truth answers. Existing systems struggle with these queries
since (1) they require selecting and applying ML models, and (2) more than a
quarter of these queries are vague, making standard tools like natural language
to SQL systems unsuited. To address these issues, we develop LEAP, an
end-to-end library that answers social science queries in natural language with
ML. LEAP filters vague queries to ensure that the answers are deterministic and
selects from internally supported and user-defined ML functions to extend the
unstructured data to structured tables with necessary annotations. LEAP further
generates and executes code to respond to these natural language queries. LEAP
achieves a 100% pass @ 3 and 92% pass @ 1 on QUIET-ML, with a \$1.06 average
end-to-end cost, of which code generation costs \$0.02.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to VLDB 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partitioning Strategies for Parallel Computation of Flexible Skylines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emilio De Lorenzis, Davide Martinenghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While classical skyline queries identify interesting data within large
datasets, flexible skylines introduce preferences through constraints on
attribute weights, and further reduce the data returned. However, computing
these queries can be time-consuming for large datasets. We propose and
implement a parallel computation scheme consisting of a parallel phase followed
by a sequential phase, and apply it to flexible skylines. We assess the
additional effect of an initial filtering phase to reduce dataset size before
parallel processing, and the elimination of the sequential part (the most
time-consuming) altogether. All our experiments are executed in the PySpark
framework for a number of different datasets of varying sizes and dimensions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Datacubes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mickaël Martin Nevot, Sébastien Nedjar, Lotfi Lakhal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many approaches have been proposed to pre-compute data cubes in order to
efficiently respond to OLAP queries in data warehouses. However, few have
proposed solutions integrating all of the possible outcomes, and it is this
idea that leads the integration of hierarchical dimensions into these
responses. To meet this need, we propose, in this paper, a complete
redefinition of the framework and the formal definition of traditional database
analysis through the prism of hierarchical dimensions. After characterizing the
hierarchical data cube lattice, we introduce the hierarchical data cube and its
most concise reduced representation, the closed hierarchical data cube. It
offers compact replication so as to optimize storage space by removing
redundancies of strongly correlated data. Such data are typical of data
warehouses, and in particular in video games, our field of study and
experimentation, where hierarchical dimension attributes are widely
represented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A case study on the transformative potential of AI in software
  engineering on LeetCode and ChatGPT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Merkel, Jens Dörpinghaus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent surge in the field of generative artificial intelligence (GenAI)
has the potential to bring about transformative changes across a range of
sectors, including software engineering and education. As GenAI tools, such as
OpenAI's ChatGPT, are increasingly utilised in software engineering, it becomes
imperative to understand the impact of these technologies on the software
product. This study employs a methodological approach, comprising web scraping
and data mining from LeetCode, with the objective of comparing the software
quality of Python programs produced by LeetCode users with that generated by
GPT-4o. In order to gain insight into these matters, this study addresses the
question whether GPT-4o produces software of superior quality to that produced
by humans.
  The findings indicate that GPT-4o does not present a considerable impediment
to code quality, understandability, or runtime when generating code on a
limited scale. Indeed, the generated code even exhibits significantly lower
values across all three metrics in comparison to the user-written code.
However, no significantly superior values were observed for the generated code
in terms of memory usage in comparison to the user code, which contravened the
expectations. Furthermore, it will be demonstrated that GPT-4o encountered
challenges in generalising to problems that were not included in the training
data set.
  This contribution presents a first large-scale study comparing generated code
with human-written code based on LeetCode platform based on multiple measures
including code quality, code understandability, time behaviour and resource
utilisation. All data is publicly available for further research.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unraveling Responsiveness of Chained BFT Consensus with Network Delay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yining Tang, Qihang Luo, Runchao Han, Jianyu Niu, Chen Feng, Yinqian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of blockchain technology, chained Byzantine Fault
Tolerant (BFT) protocols have been increasingly adopted in practical systems,
making their performance a crucial aspect of the study. In this paper, we
introduce a unified framework utilizing Markov Decision Processes (MDP) to
model and assess the performance of three prominent chained BFT protocols. Our
framework effectively captures complex adversarial behaviors, focusing on two
key performance metrics: chain growth and commitment rate. We implement the
optimal attack strategies obtained from MDP analysis on an existing evaluation
platform for chained BFT protocols and conduct extensive experiments under
various settings to validate our theoretical results. Through rigorous
theoretical analysis and thorough practical experiments, we provide an in-depth
evaluation of chained BFT protocols under diverse attack scenarios, uncovering
optimal attack strategies. Contrary to conventional belief, our findings reveal
that while responsiveness can enhance performance, it is not universally
beneficial across all scenarios. This work not only deepens our understanding
of chained BFT protocols, but also offers valuable insights and analytical
tools that can inform the design of more robust and efficient protocols.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Locality of Hall's Theorem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Brandt, Yannic Maus, Ananth Narayanan, Florian Schager, Jara Uitto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The last five years of research on distributed graph algorithms have seen
huge leaps of progress, both regarding algorithmic improvements and
impossibility results: new strong lower bounds have emerged for many central
problems and exponential improvements over the state of the art have been
achieved for the runtimes of many algorithms. Nevertheless, there are still
large gaps between the best known upper and lower bounds for many important
problems. The current lower bound techniques for deterministic algorithms are
often tailored to obtaining a logarithmic bound and essentially cannot be used
to prove lower bounds beyond $\Omega(\log n)$. In contrast, the best
deterministic upper bounds are often polylogarithmic, raising the fundamental
question of how to resolve the gap between logarithmic lower and
polylogarithmic upper bounds and finally obtain tight bounds. We develop a
novel algorithm design technique aimed at closing this gap. In essence, each
node finds a carefully chosen local solution in $O(\log n)$ rounds and we
guarantee that this solution is consistent with the other nodes' solutions
without coordination. The local solutions are based on a distributed version of
Hall's theorem that may be of independent interest and motivates the title of
this work. We showcase our framework by improving on the state of the art for
the following fundamental problems: edge coloring, bipartite saturating
matchings and hypergraph sinkless orientation. In particular, we obtain an
asymptotically optimal $O(\log n)$-round algorithm for $3\Delta/2$-edge
coloring in bounded degree graphs. The previously best bound for the problem
was $O(\log^4 n)$ rounds, obtained by plugging in the state-of-the-art maximal
independent set algorithm from arXiv:2303.16043 into the $3\Delta/2$-edge
coloring algorithm from arXiv:1711.05469 .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at SODA'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homomorphic Encryption in Healthcare Industry Applications for
  Protecting Data Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. S. Rauthan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Focussing on two different use cases-Quality Control methods in industrial
contexts and Neural Network algorithms for healthcare diagnostics-this research
investigates the inclusion of Fully Homomorphic Encryption into real-world
applications in the healthcare sector. We evaluate the performance, resource
requirements, and viability of deploying FHE in these settings through
extensive testing and analysis, highlighting the progress made in FHE tooling
and the obstacles still facing addressing the gap between conceptual research
and practical applications. We start our research by describing the specific
case study and trust model were working with. Choosing the two FHE frameworks
most appropriate for industry development, we assess the resources and
performance requirements for implementing each of the two FHE frameworks in the
first scenario, Quality Control algorithms. In conclusion, our findings
demonstrate the effectiveness and resource consumption of the two use
cases-complex NN models and simple QC algorithms-when implemented in an FHE
setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metric Criticality Identification for Cloud Microservices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akanksha Singal, Divya Pathak, Kaustabha Ray, Felix George, Mudit Verma, Pratibha Moogi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For Site Reliability Engineers, alerts are typically the first and often the
primary indications that a system may not be performing as expected. Once
alerts are triggered, Site Reliability Engineers delve into detailed data
across various modalities such as metrics, logs, and traces - to diagnose
system issues. However, defining an optimal set of alerts is increasingly
challenging due to the sheer volume of multi-modal observability data points in
large cloud-native systems. Typically, alerts are manually curated, primarily
defined on the metrics modality, and heavily reliant on subject matter experts
manually navigating through the large state-space of intricate relationships in
multi-modal observability data. Such a process renders defining alerts prone to
insufficient coverage, potentially missing critical events. Defining alerts is
even more challenging with the shift from traditional monolithic architectures
to microservice based architectures due to the intricate interplay between
microservices governed by the application topology in an ever stochastic
environment. To tackle this issue, we take a data driven approach wherein we
propose KIMetrix, a system that relies only on historical metric data and
lightweight microservice traces to identify microservice metric criticality.
KIMetrix significantly aids Subject Matter Experts by identifying a critical
set of metrics to define alerts, averting the necessity of weaving through the
vast multi-modal observability sphere. KIMetrix delves deep into the
metric-trace coupling and leverages information theoretic measures to recommend
microservice-metric mappings in a microservice topology-aware manner.
Experimental evaluation on state-of-the-art microservice based applications
demonstrates the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intelligent Router for LLM Workloads: Improving Performance Through
  Workload-Aware Load Balancing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Jain, Anjaly Parayil, Ankur Mallick, Esha Choukse, Xiaoting Qin, Jue Zhang, Íñigo Goiri, Rujia Wang, Chetan Bansal, Victor Rühle, Anoop Kulkarni, Steve Kofsky, Saravan Rajmohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) workloads have distinct prefill and decode phases
with different compute and memory requirements which should ideally be
accounted for when scheduling input queries across different LLM instances in a
cluster. However existing scheduling algorithms treat LLM workloads as
monolithic jobs without considering the distinct characteristics of the two
phases in each workload. This leads to sub-optimal scheduling and increased
response latency. In this work, we start by characterizing factors affecting
the response latency during LLM inference serving. We establish that better
load balancing of inference requests across the available LLM instances can
improve the end-to-end latency to a larger extent than merely focusing on
optimizing the instance-level scheduler. Motivated by our findings, we propose
a heuristic-guided reinforcement learning-based intelligent router for
data-driven and workload-aware scheduling. Our router schedules queries across
LLM instances by leveraging a trainable response-length predictor, and a novel
formulation for estimating the impact of mixing different workloads and
achieves over 11% lower end-to-end latency than existing approaches on a mix of
public datasets and 7.8% lower end-to-end latency on real workload data with
diverse input and output trends from Cloud Provider X. Additionally, the
proposed framework can also serve as a standard for benchmarking different LLM
inference schedulers since it provides the best latency for a given model,
hardware, and instance-level scheduler combination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Edge Graph Intelligence: Reciprocally Empowering Edge Networks with
  Graph Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liekang Zeng, Shengyuan Ye, Xu Chen, Xiaoxi Zhang, Ju Ren, Jian Tang, Yang Yang,  Xuemin,  Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a thriving growth of computing facilities
connected at the network edge, cultivating edge networks as a fundamental
infrastructure for supporting miscellaneous intelligent services.Meanwhile,
Artificial Intelligence (AI) frontiers have extrapolated to the graph domain
and promoted Graph Intelligence (GI). Given the inherent relation between
graphs and networks, the interdiscipline of graph learning and edge networks,
i.e., Edge GI or EGI, has revealed a novel interplay between them -- GI aids in
optimizing edge networks, while edge networks facilitate GI model deployment.
Driven by this delicate closed-loop, EGI is recognized as a promising solution
to fully unleash the potential of edge computing power and is garnering growing
attention. Nevertheless, research on EGI remains nascent, and there is a
soaring demand within both the communications and AI communities for a
dedicated venue to share recent advancements. To this end, this paper promotes
the concept of EGI, explores its scope and core principles, and conducts a
comprehensive survey concerning recent research efforts on this emerging field.
Specifically, this paper introduces and discusses: 1) fundamentals of edge
computing and graph learning,2) emerging techniques centering on the closed
loop between graph intelligence and edge networks, and 3) open challenges and
research opportunities of future EGI. By bridging the gap across communication,
networking, and graph learning areas, we believe that this survey can garner
increased attention, foster meaningful discussions, and inspire further
research ideas in EGI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Communications Surveys & Tutorials</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-01-15T05:28:47.834712127Z">
            2025-01-15 05:28:47 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
